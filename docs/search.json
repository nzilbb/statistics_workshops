[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NZILBB R, Stats, and Open Science Workshops",
    "section": "",
    "text": "Introduction\nThis online book contains material for NZILBB’s R, Stats, and Open Science workshops. The workshops began late in the first semester of 2024. As time goes on, this book will expand in both content and contributors.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sessions-in-semester-2-2025",
    "href": "index.html#sessions-in-semester-2-2025",
    "title": "NZILBB R, Stats, and Open Science Workshops",
    "section": "Sessions in Semester 2, 2025",
    "text": "Sessions in Semester 2, 2025\nIn Semester 2, 2025 we will work through the “Foundations” sections of this book with occasional one-off sessions interspersed throughout.\nAll sessions will be held in the Brain Box, Elsie Locke 203 on Thursdays at 10am.\nAnnouncements are made on the NZILBB Rocket Chat in the #r-stats-open-sci-workshops channel. If you don’t know how to access this, email me at joshua.black@canterbury.ac.nz.\n\nProgrammed Workshops\nWorkshop sessions consist of a short presentation and a chance for some hands-on practice. The series as a whole is for complete beginners, but the sessions are cumulative.\nIf you need to catch up, check out the material in the ‘Foundations’ section of this website and don’t hesitate to contact me for a one-on-one session to discuss any difficulties.\nThe first session will be on Thurs 20th Feb, at 11am.,\n\n\nDrop-in Sessions\nI will remain in the Brain Box for an hour after the NZILBB research seminar for anyone who wishes to discuss any coding or stats problems.\nThe first drop-in session will be on Thurs 27th Feb at 11am, with subsequent sessions fortnightly.\n\n\nWednesday Sessions\nOur theme for this semester will be variable selection in statistical models. Typically, there are many possible variables which we might include in a model. There are many (strong!) opinions about the variable selection process. We’ll look at the options, their pros and cons, and implement them with our own data. This will help you both with your own research and when reading (and peer-reviewing!) other people’s work.\nThe first Wednesday session will be on Wed 26th Feb at 1pm, with subsequent sessions at the same time fortnightly.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#one-on-one-help",
    "href": "index.html#one-on-one-help",
    "title": "NZILBB R, Stats, and Open Science Workshops",
    "section": "One-on-one help",
    "text": "One-on-one help\nFor UC students and staff, I am happy to talk over any issues you have with this material. Please get in touch with me at joshua.black@canterbury.ac.nz.\nIf you have found your way to this material by some other means, you can also email me!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-induction",
    "href": "index.html#sec-induction",
    "title": "NZILBB R, Stats, and Open Science Workshops",
    "section": "NZILBB induction",
    "text": "NZILBB induction\nThe following steps are a very quick guide to getting up and running for quantitative analysis with R, along with the wider Open Science infrastructure at NZILBB. Some of these steps are discussed at greater length elsewhere.\nIf you have any troubles with any of the steps, let me know!\n\nInstall R and R Studio, following the steps here: https://posit.co/download/rstudio-desktop/ or using the UC Software Centre if you are on a managed UC laptop.\nOpen RStudio, type install.packages('usethis') into the ‘Console’ (at the bottom left of RStudio) and press enter/return.\nInstall Git.\n\nFor Windows: download from https://git-scm.com/downloads/win. If you are given an option to allow ‘3rd-party software’ say yes.\nFor macOS: install developer tools by opening the Terminal app and typing xcode-select --install.\nFor Linux: odds are you have it already, but here’s a list of terminal commands to install it for different versions of Linux: https://git-scm.com/downloads/linux.\n\nCreate accounts at https://github.com and https://osf.io.\n\nOn GitHub, request to join the NZILBB group at https://github.com/nzilbb/.\n\nConnect R and GitHub.\n\nRestart RStudio.\nWrite usethis::use_git_config(user.name=\"YOUR NAME\", user.email=\"YOUR GITHUB ACCOUNT'S EMAIL ADDRESS\") in the console and press enter/return. Replace YOUR NAME and YOUR GITHUB ACCOUNT'S EMAIL ADDRESS with your name and the email you used to sign up to GitHub.\nWrite usethis::git_default_branch_configure() in the console and press enter/return.\nWrite usethis::create_github_token() in the console and press enter/return.\n\nThis will open a browser window where you will log in to your GitHub account. Accept the default options (click ‘Generate Token’) and copy the token which appears (don’t close the window until you’re finished the next step).\n\nWrite gitcreds::gitcreds_set() in the console and press enter/return.\n\nPaste the token in after Enter password or token:.\n\n\nUniversity of Canterbury staff and students: ensure you can log in to the UC GitLab at (use your UC log in and password): https://eng-git.canterbury.ac.nz/\nOptional: Install GitKracken: https://www.gitkraken.com/",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "NZILBB R, Stats, and Open Science Workshops",
    "section": "Other resources",
    "text": "Other resources\nYou may find the following links profitable:\n\nThe alternative to data analysis with a programming language is usually some kind of spreadsheet. Here are some spreadsheet horror stories: https://eusprig.org/research-info/horror-stories/.\nWhy do we have to learn how to program? Why is science ‘amateur software development’? This is a good lecture on the topic: https://www.youtube.com/watch?v=8qzVV7eEiaI.\n\nUsually these techniques aren’t explicitly taught. These workshops are our attempt to respond to this problem!\n\nWhy can’t you do data science with a spreadsheet? https://www.youtube.com/watch?v=cpbtcsGE0OA\nThese workshops have been heavily influenced by Winter (2019)\n\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/getting_started.html",
    "href": "chapters/getting_started.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "1.1 Install R and RStudio\nR is a programming language. RStudio is a piece of software for interacting with R.\nYou don’t have to use RStudio in order to use R, but we will assume you are using it in these workshops.1\nTo install R and RStudio on your own device follow the steps at https://posit.co/download/rstudio-desktop/.\nTo install R and RStudio on a University of Canterbury device:\nInstalling RStudio on a University of Canterbury device will also install R.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/getting_started.html#install-r-and-rstudio",
    "href": "chapters/getting_started.html#install-r-and-rstudio",
    "title": "1  Getting Started",
    "section": "",
    "text": "Windows: open the “Software Center”, search for RStudio, and press the install button.\nMac: open “UC Self Service”, search for RStudio, and press the install button.\n\n\n\n\n\n\n\n\nOptional: install tools for building R packages\n\n\n\n\n\nAt this stage, you may also want to install additional tools for building R packages from source (rather than downloading pre-built versions of packages). This is sometimes important for getting fast performance out of a package.\nFor Windows, install Rtools: https://cran.r-project.org/bin/windows/Rtools/rtools44/rtools.html\nRTools comes with RStudio when you install it on a University of Canterbury Windows device via the software centre.\nLinux should already have everything required.\nInstructions for macOS are here: https://mac.r-project.org/tools/\nIf this starts to melt your mind, stop!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/getting_started.html#open-rstudio",
    "href": "chapters/getting_started.html#open-rstudio",
    "title": "1  Getting Started",
    "section": "1.2 Open RStudio",
    "text": "1.2 Open RStudio\nIf you have installed RStudio, if should now appear in your start menu on Windows, and your Applications folder and launchpad if you are on macOS. Open it. You should see something like this:\n\n\nThe RStudio interface has four primary ‘panes’. Only three of these will be visible when you first open RStudio. The largest pane is the console pane. It is usually on the bottom left of the RStudio window, but currently takes up the entire left side. We also see the environment pane at the top right and the output pane at the bottom right.2\nThe console pane should have a message telling you what version of R you are using and the platform you are on (i.e. your hardware and operating system). This is what you would see if you opened R by itself rather than through RStudio.3\nThe environment pane should be empty. You will see multiple tabs across the top of this pane. The environment tab will allow us to see the data which we are working with at a given time. At this stage, you may see a tab labelled ‘Tutorial’. I’ll tell you how to use this later (Section 1.6).\nThe output pane will start by showing you a list of files on your computer. This is useful for finding and manipulating files (just like the file browser built in to your operating system). This is also where plots and help pages will appear.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/getting_started.html#interact-with-r-in-the-console",
    "href": "chapters/getting_started.html#interact-with-r-in-the-console",
    "title": "1  Getting Started",
    "section": "1.3 Interact with R in the Console",
    "text": "1.3 Interact with R in the Console\nWe will get started by interacting with R in the console pane. You should see a &gt; in the console pane. This is one of the places we can enter R code. If the code works, we will see the output immediately below (or perhaps in the output pane, depending on the code). If the code doesn’t work, an error message will appear.\n\n1.3.1 Basic arithmetic\nWe’ll start with some basic arithmetic. We add two numbers together by writing the first number, the + sign, and the second number. Enter the code in the box below after the &gt; in your console. The expected output appears below the box. You should see the same thing in your console after you press enter/return.\n\n1 + 1\n\n[1] 2\n\n\nThe other basic arithmetic operators work in the same way. So subtraction we use ‘-’.\n\n500 - 49\n\n[1] 451\n\n\nWe use * for multiplication.\n\n43 * 6.4\n\n[1] 275.2\n\n\nFor exponentiation we use ^ (usually, Shift + 6).\n\n924^5\n\n[1] 6.735345e+14\n\n\nThe output given here is in scientific notation. It is important to be able to read this notation when using R. It makes very very small and very very large numbers much easier to write and is often used in the output of R functions. To convert from scientific notation to regular digits, multiple the number which appears before the e by 10 to the power of the number after the e. In this case, we take the number \\(6.735345\\) and multiply it by \\(10^{14}\\) to get \\(673,534,500,000,000\\). That is, six hundred seventy-three trillion and a bit. According to Wikipedia, this is something like the total number of cells in six and a half adult humans and a bit fewer than the number of ants on Earth.\nThere are a few different operators associated with division. Usually, you will want to use /.\n\n43 / 7\n\n[1] 6.142857\n\n\nSometimes, it is useful to get the integer4 component on the answer or the remainder. If we want the integer, we use %/%:\n\n43 %/% 7\n\n[1] 6\n\n\nIf we want the remainder, we use:\n\n43 %% 7\n\n[1] 1\n\n\nThat is, if we divide \\(43\\) by \\(7\\), we get \\(6\\) groups of \\(7\\), with \\(1\\) left over.\n\n\n\n\n\n\nSpaces, punctuation, and style\n\n\n\nComputer programming requires attention to minor details of punctuation and spacing. Hours can be spent trying to discover why code is not working, only to discover a missing comma. This is especially true in the early stages of learning, where error messages can be very confusing.\nIt is worth knowing when you can add spaces and when you can’t. The spaces in the code above between the numbers and the arithmetic operators are not necessary. So, for instance, you could write:\n\n43/7\n\n[1] 6.142857\n\n\nIn fact, you can add however many spaces you like!\n\n34  /    2\n\n[1] 17\n\n\nThe only reason to prefer one over the other is readability. This raises the issue of code style, which we will discuss in future workshops. Note that, above, there wasn’t a space in 924^5—this is a style convention for ^ and some other (‘high precedence’) operators which we will encounter later.5\n\n\n\n\n1.3.2 Vectors\nIn our actual research, we work with large collections of experimental data or values derived from corpora. But the commands we’ve looked at above only deal with two numbers at a time. The simplest structure for dealing with more than one value is a vector.\nWe create vectors using the function c(). The c() function combines values in to a vector.\n\nc(1, 2, 3, 4)\n\n[1] 1 2 3 4\n\n\nThe [1] you see in the output is followed by the first element of the vector. If you print out a very long vector you will see numbers other than 1 inside the square brackets. For instance:\n\n60:124\n\n [1]  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n[20]  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n[39]  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116\n[58] 117 118 119 120 121 122 123 124\n\n\nThe : operator produces a vector from the first number to the second number (inclusive) in steps of one. The resulting output is long enough that it has to go across two lines. When the new line starts you will see another number in square brackets. This tells you how far through the vector you are at the line break. Exactly which number it is will vary according to the width of whatever you are using to view the output. For me, editing this text in RStudio, it is [38]. That is, the number which follows [38] is the 38th number in the vector. If you are viewing this online, then the number is likely [20].\nWe call the values in a vector the elements of the vector. The elements of a vector have to be the same type of thing. We’ll talk about types more later. For now, just note that a number is a different kind of thing from a string of characters. So, what happens if we try to mix numbers and strings in a vector?\n\nc(1, 2, 3, \"dog\")\n\n[1] \"1\"   \"2\"   \"3\"   \"dog\"\n\n\nR hasn’t explicitly complained, but it has done something in the background without telling you. The numbers we entered now have quotation marks around them. They have been turned in to strings. Keep an eye out for quotation marks — sometimes you might think you are dealing with numbers, but really you are dealing with strings. This is a common problem when loading data from external files (e.g., from a .csv file).\nWhy worry? Your code likely won’t work if you have strings rather than numbers. For instance, you can’t apply arithmetic operators to strings.\n\n\"1\" + \"2\"\n\nError in \"1\" + \"2\": non-numeric argument to binary operator\n\n\nThe above is the first error message you’ve seen in this course. You will see many more in your time working with R. The error message is telling you that what you are doing does not work with anything ‘non-numeric’ (i.e., anything that isn’t a number). The ‘binary operator’ in this case is + (binary because it adds together two values).\nTo enter a string, you can use either double quotes (\") or single quotes (').\nVectors can also be used in arithmetic. Under the hood, statistics is mostly arithmetic with collections of vectors. How are these arithmetic operations implemented in R?\nThe simplest case is when we use a vector and a single number, as follows\n\n2 * c(1, 2, 3, 4)\n\n[1] 2 4 6 8\n\n\nEach element of the vector has been multiplied by \\(2\\). The same is true of addition, division, and subtraction. These are ‘element-wise’ operations. That is, they are applied to each element individually.\n\n3 / c(1, 2, 3, 4)\n\n[1] 3.00 1.50 1.00 0.75\n\n\nThis also works with two vectors. For instance:\n\nc(1, 2, 3, 4) * c(1, 2, 3, 4)\n\n[1]  1  4  9 16\n\n\nHere we get the first elements multiplied together, then the second, then the third, and so on.\nIf one vector is shorter than the other, is will be ‘recycled’ to match the longer vector:\n\nc(1, 2) * c(1, 2, 3, 4)\n\n[1] 1 4 3 8\n\n\n\n\n1.3.3 Variables\nYou do not want to be entering the same vector over and over again. This is where variables come in. Variables allow us to associate names with values.\nTo ‘assign’ an object to a name, we use &lt;-. For instance:\n\nmy_cool_vector &lt;- c(6, 9, 4, 5, 2, 2)\n\nNow the name my_cool_vector is associated with the vector c(6, 9, 4, 5, 2, 2). If you look to the top right of the RStudio window you should now see this variable in your environment pane. The name will be on the left and the value on the right.\n\n\n\nOur cool vector in the RStudio environment pane. Your screen may look a little different.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn most cases you can also use = to assign an object to a name. This may seem more natural to you if you are coming from another programming languages. The convention in R is to use &lt;-. This is because = has multiple functions in R, but &lt;- is always assignment of an object to a name.\n\n\nWe can now apply operations using the name instead of manually entering the vector again. For instance:\n\n4 * my_cool_vector\n\n[1] 24 36 16 20  8  8\n\n\nThis has multiplied each of the elements of my_cool_vector by \\(4\\).\nTo see what object is associated with a name we can look in the environment pane or simply enter the name into the console.\n\nmy_cool_vector\n\n[1] 6 9 4 5 2 2\n\n\nWe can also look up specific elements using square brackets. If we wanted to look up the fourth element in my_cool_vector we would enter the following code.\n\nmy_cool_vector[4]\n\n[1] 5\n\n\nWe can even change specific elements by using the assignment opperator (&lt;-).\n\nmy_cool_vector[4] &lt;- 3\nmy_cool_vector\n\n[1] 6 9 4 3 2 2\n\n\nThe fourth entry in the vector is now 3 rather than 5.\nNaming variables is serious business. It is important to know what you could do and what you should do.\nR has the following rules for names:\n\nA name must consist of letters, digits, ., and _.\nIt cannot begin with digits or _.\nIt cannot come from a list of reserved words (e.g. TRUE — these names have important roles in R and can’t be overridden.)\n\n\n\n\n\n\n\nWhat’s a letter?\n\n\n\nWhat counts as a ‘letter’ varies by operating system and local settings (your ‘locale’). The recommendation from Hadley Wickham is that you only use ASCII letters (for instance, avoid use of any diacritics/accents).6\nOne local reason you might want to use non-ASCII characters is if you want to use te reo Māori with macrons for your variable names. This might be appropriate for a particular project (the question is always who you want to share your code with). Pretty much anyone using a modern operating system should be able to use your code. You may decide that the small risk of incompatibility is worth it in this case.\n\n\nI follow the convention of using _ rather than . in my variable names. So, e.g., I’d prefer my_cool_vector over my.cool.vector. This reduces ambiguity in some cases.\nWhile we are talking about naming, R will accept anything placed within backticks (‘`’) as a variable name. If you have a chaotic temperament, you might decide to use variable names like this:\n\n# Eldritch variable\n`t̸̡͚̳͓̜̘̪̙̟̣͛̋̈̐͜ḩ̷̛̗̬̪̔̾͋̌̂̓͑̔̚͝ë̵̮̟̟̼̲̦͙̠̟́͋̇̏̓ ̶̟̱̲̠͎̙̠̆̑̈́̉̆̏̋͠͠t̷̲͉͔̘̬̪͖̗́͌̏̉̏̄͊̍̽͋̈̈́̀͝͠o̵͚͙̮͙͉̱̱͕̗̘̻͋͋͋̀́̒͝ͅw̸͖͚̖̣̭̥͍̹͚̞͕̺͇͙͌͛͋̆̿̈́̎̆̋̑͌̏͘͠͝e̵͖̝̞̙͕̤̅̃̓r̴͍̼̱̜̹͚̎̌̂͆͗̏́ṡ̷͔͉͇͗̍̆̔̕ͅ ̷̪̱̞͈̰̈́͜ǫ̷̤͍̫̠̻̣̪̻͖̒̈́͐͂̿̆̑̄̂͘f̶̠͉̯̪̪̖̦͋͝ ̶̙̻̝͆̈͠C̴̳̪̪̻̫̬̳̜̅͑̇͌̆̕a̶̡͚̼͍̺͂̈́̄r̷̨̛̛̜̹͙̲̝̲̖͍̓̊͒̄̓̏͂͐͛͑̊͘c̸͇̲̲͈͕͉͍̗̐ơ̵̟̠̒̔͑͆s̶̨̢̱̱̲͇͉̪̻̖̠͊̈́̐͋́̈́͜a̸̗̩̯̳̝͈̰̅͒̂̏͛̽̓͑̈́̾ͅ ̷̢͎͎̳̖̤̥̜̀̑̈́̈́r̴̦͌͛͘o̴̩̩̯̤̝̊͗̿̉͗͂͂̆̈́͘s̶͔̼̞̱̻̭̻͑̔͛̔ḙ̸̢̀̎͗̓͊̈̊̉̚̚͝ ̸̠̰̞̬̐̆̽̅̀̈̂̌͠b̶̧̜̟͍͔̘̥͇̈́͒̃͒̈́͊̓̉́̉̐͘͘͝͝ę̵͚̀̈́̿̌̆̈́͘̕͠͝ͅh̸̛͎̱͚͕̹̘̥̠͕̟̼͝ͅî̶̞̹̺̰̎̿̊̽͒͑͑̽͝n̵̢̢̛̛̟͓̗̮̦̪̥̩͓̪̘͗͗̑̊̌̉̂͊͠͝d̵͎̭̤̲͋͌̃̎̊ ̷̧̧̛̤͇̫̝̗̻͚̐̊̈́̇̂͗̋t̵͓̻̦̻̗͇̜̼̻̫̼̭̄́͘̚h̵̨̅̉̄e̸̡̡̨̞̪̝̝̟͔̞̞͔̰̒̓͆̐͛̂̒͂̊̆̽̃̌͘ ̴̛̦̖̖̖̹̖̹̣̳̕m̶̡͉̦̣͉̳̪͖͕͍͙̪̟͌̍̏͆̐̄̂̚͘o̸̭̯̠̭͎̖͐͗̏̉͋̅͊̓̓̂̏̓̏̍͝ǫ̴͖͈̖̣̤͍̝̩̳̪̔͂̋̄̑̏̒̏̏̈́ñ̸̙̪͉͓̼̯̩͋̋̌̏̃͘̕͘.̵̙̮̾̐͠ͅ` &lt;- 10\n\n# Spooky variable\n`👻` &lt;- 5\n\nYou could even do some maths with these variables:\n\n`t̸̡͚̳͓̜̘̪̙̟̣͛̋̈̐͜ḩ̷̛̗̬̪̔̾͋̌̂̓͑̔̚͝ë̵̮̟̟̼̲̦͙̠̟́͋̇̏̓ ̶̟̱̲̠͎̙̠̆̑̈́̉̆̏̋͠͠t̷̲͉͔̘̬̪͖̗́͌̏̉̏̄͊̍̽͋̈̈́̀͝͠o̵͚͙̮͙͉̱̱͕̗̘̻͋͋͋̀́̒͝ͅw̸͖͚̖̣̭̥͍̹͚̞͕̺͇͙͌͛͋̆̿̈́̎̆̋̑͌̏͘͠͝e̵͖̝̞̙͕̤̅̃̓r̴͍̼̱̜̹͚̎̌̂͆͗̏́ṡ̷͔͉͇͗̍̆̔̕ͅ ̷̪̱̞͈̰̈́͜ǫ̷̤͍̫̠̻̣̪̻͖̒̈́͐͂̿̆̑̄̂͘f̶̠͉̯̪̪̖̦͋͝ ̶̙̻̝͆̈͠C̴̳̪̪̻̫̬̳̜̅͑̇͌̆̕a̶̡͚̼͍̺͂̈́̄r̷̨̛̛̜̹͙̲̝̲̖͍̓̊͒̄̓̏͂͐͛͑̊͘c̸͇̲̲͈͕͉͍̗̐ơ̵̟̠̒̔͑͆s̶̨̢̱̱̲͇͉̪̻̖̠͊̈́̐͋́̈́͜a̸̗̩̯̳̝͈̰̅͒̂̏͛̽̓͑̈́̾ͅ ̷̢͎͎̳̖̤̥̜̀̑̈́̈́r̴̦͌͛͘o̴̩̩̯̤̝̊͗̿̉͗͂͂̆̈́͘s̶͔̼̞̱̻̭̻͑̔͛̔ḙ̸̢̀̎͗̓͊̈̊̉̚̚͝ ̸̠̰̞̬̐̆̽̅̀̈̂̌͠b̶̧̜̟͍͔̘̥͇̈́͒̃͒̈́͊̓̉́̉̐͘͘͝͝ę̵͚̀̈́̿̌̆̈́͘̕͠͝ͅh̸̛͎̱͚͕̹̘̥̠͕̟̼͝ͅî̶̞̹̺̰̎̿̊̽͒͑͑̽͝n̵̢̢̛̛̟͓̗̮̦̪̥̩͓̪̘͗͗̑̊̌̉̂͊͠͝d̵͎̭̤̲͋͌̃̎̊ ̷̧̧̛̤͇̫̝̗̻͚̐̊̈́̇̂͗̋t̵͓̻̦̻̗͇̜̼̻̫̼̭̄́͘̚h̵̨̅̉̄e̸̡̡̨̞̪̝̝̟͔̞̞͔̰̒̓͆̐͛̂̒͂̊̆̽̃̌͘ ̴̛̦̖̖̖̹̖̹̣̳̕m̶̡͉̦̣͉̳̪͖͕͍͙̪̟͌̍̏͆̐̄̂̚͘o̸̭̯̠̭͎̖͐͗̏̉͋̅͊̓̓̂̏̓̏̍͝ǫ̴͖͈̖̣̤͍̝̩̳̪̔͂̋̄̑̏̒̏̏̈́ñ̸̙̪͉͓̼̯̩͋̋̌̏̃͘̕͘.̵̙̮̾̐͠ͅ` + `👻`\n\n[1] 15\n\n\nUnsurprisingly, if you try this without the backticks, you will get an error:\n\n👻 &lt;- 5\n\nError in parse(text = input): &lt;text&gt;:1:1: unexpected input\n1: 👻\n    ^\n\n\nDo not take advantage of backticks to use names like this.\nWhy am I even telling you about backticks? They often appear in practice as a result of importing data from a spreadsheet. Usually they appear because the column names in the spreadsheet have spaces in them (e.g. a column called ‘Response Time’ would be loaded into R with the name `Response Time`). One of the first things to do when tidying up data loading from a spreadsheet is to change the names.\n\n\n1.3.4 Exercises\n\nWhat is the output of 5:10?\n\n c(5, 10) [1]  5  6  7  8  9 10 [1]  5  6  7  8  9\n\nWhat is the output of 10 * c(1, 2)?\n\n [1] 10 20 [1]  30 Error in 10 * c(1, 2) : non-numeric argument to binary operator\n\nWhat is the output of c(3, 4, 6, 2)[2]?\n\n [1] 4 [1] 3 4 6 2\n\nLook at the variable names in the following list. Some of them are very bad names for stylistic reasons, but will they be accepted by R? I.e., are they syntactically valid?\n\nnz_vowels TRUEFALSE\n_nz_vowels TRUEFALSE\n🥝_vowels TRUEFALSE\n`🥝_vowels` TRUEFALSE\nTraditional languages should be taught in school TRUEFALSE\nTraditional.languages_should_be.taught.in_school TRUEFALSE\nin_school TRUEFALSE\n5_points_attitude TRUEFALSE\nattitude_5 TRUEFALSE\n::::: TRUEFALSE\nfunction TRUEFALSE",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/getting_started.html#start-an-r-script",
    "href": "chapters/getting_started.html#start-an-r-script",
    "title": "1  Getting Started",
    "section": "1.4 Start an R Script",
    "text": "1.4 Start an R Script\nIf we exclusively used R in the console, we would be in no better position than if we just used Excel or another spreadsheet programme. We need to be able to retrace our steps. R Scripts are one way for us to do this.\nIn order to start an R script go to File &gt; New File &gt; R Script or use the keyboard shortcut Cmd + Shift + N (macOS) or Ctrl + Shift + N (Windows).\nThis will open the source pane. We will now enter code in the source pane rather than the console.7 Unlike the console, each command is saved in the script and pressing return/enter will not run the code.\nUsually you will run code by selecting it and pressing Cmd + Return (macOS) or Ctrl + Enter (Windows).8 If you have no code selected, this command will run the line which your cursor is on. The alternative is to run the entire script all at once. This can be done by pressing Source at the top right of the source pane.9\nEnter the following into your new script then run it. You should see the output in the console pane.\n\nmy_cool_vector &lt;- c(6, 9, 4, 5, 2, 2)\nmy_cool_vector ^ 2\n\n[1] 36 81 16 25  4  4\n\n\n\n\n\nThe source pane with our script entered. Note the ‘Run’ and ‘Source’ buttons at the right.\n\n\n\n\n\n\n\n\nCopying vs. typing\n\n\n\nThere is some wisdom to the idea that you should get coding ‘into your fingers’ by typing out examples yourself. It’s up to you whether you follow this wisdom. I have enabled the ‘copy/paste’ button in all code blocks. You should see a clipboard icon when you have your cursor over a code block. Click the clipboard to copy the code.\n\n\nIt is important to leave comments, so that your code can be interpreted by other researchers (including yourself in the future). Anything which appears in a line after a # is a comment and will be ignored by R.\nWe could change our script as follows, and the result will be identical:\n\nmy_cool_vector &lt;- c(6, 9, 4, 5, 2, 2)\n# Square each element of my cool vector and output to console.\nmy_cool_vector ^ 2\n\n[1] 36 81 16 25  4  4\n\n\nIn actual data analysis projects, commenting is vital. We’ll see some more useful examples of commenting as we go on.\n\n1.4.1 Matrices and Dataframes\nIn data analysis we want to find associations between multiple variables. So single vectors aren’t going to cut it. We need collections of vectors.\nThe simplest version of this is a matrix. Matricies are like vectors in that they can only contain elements of the same type.\nAdd the following lines to your script and run them:\n\nmy_cool_matrix &lt;- matrix(my_cool_vector, nrow = 3)\nmy_cool_matrix\n\n     [,1] [,2]\n[1,]    6    5\n[2,]    9    2\n[3,]    4    2\n\n\nWe now have a \\(3\\times2\\) matrix of numbers.\nSquare brackets are again used to manipulate individual elements. But we now have to include both rows and columns inside the square brackets (separated by a comma). If we want the second column and third row we can use my_cool_matrix[3, 2]. If you want the entire second column you would enter my_cool_matrix[, 2] (don’t put a number before the comma). If you want the entire first row, you would enter my_cool_matrix[1, ] (don’t put a number after the comma). Try out these commands in either the script or the console window.\nHave a look in the environment pane. You should now see a separation between “data” and “values”. The “data” section contains structured objects, such as matrices. The one practical difference here is that if you click on something in the data section you can explore its structure. If you click on the name of a data frame you’ll see something that looks a lot like a spreadsheet application in a new tab in the source pane.\nOften our data will comes in many different types. For instance, it might include numbers indicating the age of a participant, or which of a series of options they chose. It might also include words (strings) indicating which experimental condition they are in, or a transcript of an interview. Matricies can’t handle having elements of different types, but data frames can.\nWe create data frames using the data.frame function. Enter the following into your script and run it:10\n\n# We repeat this line from above because subsequent code modified \n# `my_cool_vector`.\nmy_cool_vector &lt;- c(6, 9, 4, 5, 2, 2)\n\nmy_data_frame &lt;- data.frame(\n  \"numbers\" = my_cool_vector,\n  \"letters\" = c(\"N\", \"Z\", \"I\", \"L\", \"B\", \"B\")\n)\n\nmy_data_frame\n\n# A tibble: 6 × 2\n  numbers letters\n    &lt;dbl&gt; &lt;chr&gt;  \n1       6 N      \n2       9 Z      \n3       4 I      \n4       5 L      \n5       2 B      \n6       2 B      \n\n\nWe now have a data frame with a column of numbers and a column of corresponding letters. We have also given each of these columns a name ( ‘numbers’ for the column of numbers and ‘letters’ for the column of letters). Each row of the data frame is an observation, and each column is a variable.11 Perhaps you can figure out what the association between the two variables is.\nWhen we have names for columns, we can access the column using the name by means of the $ symbol:\n\nmy_data_frame$numbers\n\n[1] 6 9 4 5 2 2\n\n\n\n\n1.4.2 Functions and Help\nWe have now seen a few functions (matrix to create a matrix, and data.frame to create a data frame). Functions are what we use to perform data analysis tasks in R. To apply a function, we write its name and then enter a series of arguments inside brackets. The arguments are the information which we pass for the function in order for it to do its work. Usually functions produce an output, which we can either see in the console or save to a variable.\nRecall the matrix code from above:\n\nmy_cool_matrix &lt;- matrix(my_cool_vector, nrow = 3)\n\nHere the matrix function is given two arguments. The first is the vector my_cool_vector and the second is nrow = 3. The two arguments are separated by a comma. If we want to work out what the arguments to a function are, we can look at the help page for the function. To do that enter the following in either the script or the console:\n\n?matrix\n\nYou should now see the help screen in the output pane (bottom right). This help page tells you about three related functions. In the ‘usage’ section, you will see some code examples which use the functions. The section arguments tells you what you can include as an argument to the function. So, for instance, you see that the nrow argument expects you to tell it how many rows you want the matrix to have.\nThe functions come in the order they appear in the usage example. In this case, each of the possible arguments is named. So, for instance, the first argument is called data. We did not enter this explicitly when we used the argument. But we could have:\n\nmy_cool_matrix &lt;- matrix(data = my_cool_vector, nrow = 3)\n\nSometimes it makes your code more clear to include the names of arguments. This is a judgement call.\nThe help screen also shows the default values for these arguments. If there are default values, then you don’t need to manually the argument. If you are modifying only one argument, which appears latter in the list, then you will need to use the name.\nFor instance, if we wanted to say the matrix has two columns (rather than three rows), we would have to add ncol as a named argument when we call the function:12\n\nmy_cool_matrix &lt;- matrix(my_cool_vector, ncol = 3)\nmy_cool_matrix\n\n     [,1] [,2] [,3]\n[1,]    6    4    2\n[2,]    9    5    2\n\n\nIf we didn’t include the name of this argument, R would not know what argument we intended to modify. If you look in the ‘details’ section of the help page, you will see that if we only specify a number of rows or a number of columns, it will attempt to work out the other value.\n\n\n1.4.3 Install and Use a Package\nOne of the great advantages of R is that it has a large community of developers making packages to share their code. Packages allow us to cumulatively build on each others work and to do things quickly which might otherwise take a lot of time and statistical knowledge to achieve.\nWell start with a silly package: cowsay.13 This package produces text art animals who will ‘say’ whatever text you enter.\nTo install a package, enter the following to the console:\n\ninstall.packages('cowsay')\n\nThis means that the package cowsay is now installed on your computer. To use it in a script, you need to enter the following at the top of your script:\n\nlibrary(cowsay)\n\nBy convention, we add libraries at the start of a script. This lets other researchers see exactly what is needed to run the script at the start. In addition, packages sometimes conflict with one another, and it is important to see this before we carry out any data analysis.\nTo see what functions cowsay has, look at the documentation. If you want to see the names of functions, you can enter cowsay:: and RStudio will suggest the names of functions. You can add a ? in front of any of these function names to see the help file for the function. Often packages have one or more ‘vignettes’ which explain the package. To see what vignettes come with the cowsay package enter vignette(package=\"cowsay\"). In this case, there is only one, called “cowsay”. To open this in the output pane enter vignette(\"cowsay\", package=\"cowsay\").\nThere are two functions which come with cowsay: say and endless_horse. The function we will use is called say, so enter ?say in the console.\nNow enter the following into your script:\n\nsay(\n  what = \"\", # Write your own quote here (between the quotation marks)\n  by = \"\" # Enter a 'type of thing' from the list on the help page.\n)\n\nHere, I have used comments to indicate what you need to do to complete the code.\nHere’s one possible completion:\n\nsay(\n  what = \"It sure is lonely down here.\",\n  by = \"whale\"\n)\n\n\n ______________________________ \n&lt; It sure is lonely down here. &gt;\n ------------------------------ \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`\n\n\nSometimes packages contain data. This is one way to get data in to your R session.\nIn fact, there’s plenty of data built in to R. Often this is used to demonstrate different functions. To see these, enter data() in the console. You can load one of these datasets in to your script by entering the name of the dataset as an argument to the function data(). The following code block loads up one of these datasets.\n\ndata(warpbreaks)\n\n# To view the data in RStudio, use `View` instead of `head`.\nhead(warpbreaks)\n\n# A tibble: 6 × 3\n  breaks wool  tension\n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  \n1     26 A     L      \n2     30 A     L      \n3     54 A     L      \n4     25 A     L      \n5     70 A     L      \n6     52 A     L      \n\n\nThere datasets also have help pages (see ?warpbreaks). What does this data represent?\nAs a final bit of R code, and to show you another function in action, let’s plot the numeric information in warpbreaks:\n\nhist(warpbreaks$breaks)\n\n\n\n\n\n\n\n\nHere we see the distribution of the count of warp breaks while weaving for a fixed length of yarn.\n\n\n1.4.4 Exercises\n\nWhat is the output of # 2 + 2?\n\n [1] 4 2 + 2 Nothing.\n\nConsider the following code to create a matrix:\n\nmatrix(\n  data = 1:50,\n  ncol = 5\n)\n\nHow many columns does the matrix have? 105Not enough information.\nHave a look at the documentation for the function say(), are the following statements true or false:\n\nThe default argument value for by is \"cat\". TRUEFALSE\nKākapo is an option for the by argument. TRUEFALSE\nIf you enter what = \"catfact\" the animal will say \"catfact\". TRUEFALSE.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/getting_started.html#modify-rstudio-defaults",
    "href": "chapters/getting_started.html#modify-rstudio-defaults",
    "title": "1  Getting Started",
    "section": "1.5 Modify RStudio Defaults",
    "text": "1.5 Modify RStudio Defaults\nThere are many useful options which you might want to change to improve your RStudio experience. These can be found at Tools &gt; Global Options.\nI’m going to assert that you should change some settings in the ‘General’ window which will have appeared for you without properly explaining myself.14 Make it so that your settings match the following image:\n\n\n\nThe desired state of the General settings.\n\n\nThis means that nothing will be saved between times when you open R. This, in turn, means that your script has to contain everything that is important for your analysis and you will not accidentally rely on something being carried over between programming sessions. What do I mean? Well, you might run a piece of code and then accidentally delete it from the script. The results of running the code could hang around between sessions and you would not notice your mistake. When it comes to sharing your code, the research you share it with will not be able to run it successfully and it might take a long time to discover the problem.\n\n\n\n\n\n\nWarning\n\n\n\nIf you have been using R for a while already, you may be relying on R to keep the result of long computations between sessions. If so, leave the settings as they are for now and talk to me (Josh). There are a few ways to save computations so that you do not have to, say, refit a large model from scratch every time.\n\n\nYou can also modify the appearance of RStudio to your liking using the Appearance options.\n\n\n\nThe appearance pane in General Options\n\n\nAnother thing which it is worth doing now and then forgetting about is to set the ‘Line ending conversion’ setting to ‘Posix (LF)’ (Tools &gt; Global Options, Code in the menu on the left of the pop-up, then go to the Savind tab). You may be surprised to learn there is no such this as ‘plain text’. The ending of lines are represented differently in Windows and in other operating systems. This setting insists on the non-Windows line ending and will make your life easier when you work with others using GitHub.\n\n\n\nDesired state of the ‘Line ending conversion’ setting (highlighted red).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/getting_started.html#sec-gs-additional",
    "href": "chapters/getting_started.html#sec-gs-additional",
    "title": "1  Getting Started",
    "section": "1.6 Additional Resources",
    "text": "1.6 Additional Resources\n\nThere are many good R and RStudio tutorials out there. One, which you can use from within RStudio is learnr. Install the package by entering install.packages('learnr') and you should see the tutorials in the environment pane.\nThe material which you get in learnr is from the book R for Data Science, available here: https://r4ds.hadley.nz/\nSee the first chapter of Winter (2019)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/getting_started.html#sec-alternatives",
    "href": "chapters/getting_started.html#sec-alternatives",
    "title": "1  Getting Started",
    "section": "1.7 Alternatives to RStudio",
    "text": "1.7 Alternatives to RStudio\nYou can write R code in any text editor which you like. Popular options with more or less integration of R include:\n\nVisual Studio Code\nESS (i.e. Emacs Speaks Statistics).\n\nWe won’t discuss these alternatives in these workshops. The most likely reason for you to use one of them is that you are already a keen programmer with strong preferences concerning your tools.\nThe company that make RStudio (Posit) also have a new editor called Positron. This is built on top of Visual Studio Code. I have used it a little and think it is very good! It is not yet officially released, but pre-release versions are available at https://github.com/posit-dev/positron.\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/getting_started.html#footnotes",
    "href": "chapters/getting_started.html#footnotes",
    "title": "1  Getting Started",
    "section": "",
    "text": "For alternatives which you might explore see Section 1.7↩︎\nYou will find different terminology out there. I’m following the language in the official RStudio User Guide: https://docs.posit.co/ide/user/ide/guide/ui/ui-panes.html.↩︎\nTry this. You should find a shortcut to open R in the Start menu on Windows or the Launchpad in macOS. On Linux or macOS you can also open R by opening a terminal window, typing ‘r’, and pressing enter.↩︎\nThe integers are 0, the natural numbers (1, 2, 3, …) and the negatives of the natural numbers (-1, -2, -3, …). They do not have a decimal point. The ‘real’ numbers include integers and numbers which need a decimal point. When programming it is sometimes important to know whether a number is an integer or a real number.↩︎\nYou might want to look at this page: https://style.tidyverse.org/. I try to follow this style guide as much as possible.↩︎\nSee (https://adv-r.hadley.nz/names-values.html)↩︎\nAll steps required to repeat your analysis should be in a script or markdown (more about markdown later). However, sometimes it is useful to use the console to double check something about your data or load a help file etc.↩︎\n The same can be achieved by pressing the Run button at the top right. But since you are likely to be running code very frequently, it is best to learn the keyboard shortcut. ↩︎\n The keyboard shortcut for this is Cmd + Shift + S (macOS) or Ctrl + Shift + S (Windows). From now on you can look up keyboard shortcuts by using Option + Shift + K (macOS) or Alt + Shift + K (Windows).↩︎\nNote the use of a comment to explain why we are re-creating my_cool_vector. This is the kind of step in the middle of a script which is likely to cause confusion without a comment.↩︎\nNote that there are two meanings of ‘variable’. An R variable is a name and its object, a data variable is a column of a data frame.↩︎\nTo ‘call’ a function just means to use it.↩︎\nI first became aware of this package through a tutorial produced by Kevin Watson for LING316.↩︎\n For the rationale see: https://r4ds.hadley.nz/workflow-scripts.html.↩︎",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "chapters/data_processing.html",
    "href": "chapters/data_processing.html",
    "title": "2  Data Processing",
    "section": "",
    "text": "2.1 Processing Data in Base R.\nIn this chapter, we’ll look at some common things you will need to do with data. We will also begin to work with a series of packages called the tidyverse, which implement a series of data processing and visualising techniques on top of base R.\nI tend to use the tidyverse when I work with data, but there are many people who prefer “base R” (i.e., not the tidyverse). We will look at some base R approaches as these workshops progress, and especially in this chapter, but the majority of the code I put in front of you will use at least some tidyverse methods.\nThe easiest way to interact with this chapter is to run the code in the callout block above. This will set up a new RStudio project and provide all the data you need. However, you may wish to set up your own project and avoid GitHub. If so, you need to have this data file inside a folder called data inside your project. To download the file after following the link, click the download button at the right of the toolbar.\nNow either make your own new script and enter in the code from each of the blocks below, or open scripts/data_processing.R to follow along.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "chapters/data_processing.html#processing-data-in-base-r.",
    "href": "chapters/data_processing.html#processing-data-in-base-r.",
    "title": "2  Data Processing",
    "section": "",
    "text": "2.1.1 Loading in Data\nUsually, you will load data from an external file. It is best practice to treat this file as ‘read only’ for the purposes of your R project. That is, you shouldn’t be overwriting the data file in the course of your project.\nUsually this data will be either:\n\na csv file: CSV stands for comma separated values. It is a text file in which values a separated by commas and each new line is a new row. It is an example of the more general class of ‘delimited’ files, where there is a special character separating values (another one you might come across is ‘tab separated values’ (.tsv)— where the values are separated by a tab).\nan Excel file (.xlsx or .xls). There is no base R function to directly read Excel files, but the readxl package provides one. Be extra careful when loading Excel files. Excel may have modified the data in various ways. Famously, dates and times are difficult to manage.\n\nThere are other possibilities as well. Perhaps, for instance, you are collaborating with someone using SPSS or some other statistical software. There are too many cases to consider here. Usually a quick search online will tell you what you need to do.\nHere, we’ll load a csv of reading from vowels from the QuakeBox corpus using the base R read.csv function. The most simple approach is:\n\nvowels &lt;- read.csv('data/qb_vowels.csv')\n\nI often use the here package to manage file paths. These can be a little finicky, especially when shifting between devices and operating systems. In an R project, the function here() starts from the root directory of the project. We then add arguments to the function containing the names of the directories and files that we want. The here version of the previous line of code is:\n\nvowels &lt;- read.csv(here('data', 'qb_vowels.csv'))\n\nLook in your file browser to make sure you understand where the file you are loading lives and how the path you enter, either using relative paths within an R project and/or using the here package, relates to the file.\n\n\n\n\n\n\nWarning\n\n\n\nYou should (almost) never have a full file path in your R script. These cause a lot of problems for reproducibility as they refer to your specific computer.\nFor instance, the full path on my computer to the qb_vowels.csv file is ~/UC Enterprise Dropbox/Joshua Wilson Black/teaching/intro_workshops/statistics_workshops/data/qb_vowels. This is not the full path on your computer.\nThe here() package ensures that file paths always start from the project directory. This means file paths will work for anyone who has the project.\n\n\nWe should always check a few entries to make sure the data is being read in correctly. The head() function is very useful.\n\nhead(vowels)\n\n# A tibble: 6 × 14\n  speaker     vowel F1_50 F2_50 participant_age_category participant_gender\n  &lt;chr&gt;       &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;             \n1 QB_NZ_F_281 GOOSE   427  2050 46-55                    F                 \n2 QB_NZ_F_281 DRESS   440  2320 46-55                    F                 \n3 QB_NZ_F_281 NURSE   434  1795 46-55                    F                 \n4 QB_NZ_F_281 KIT     554  2050 46-55                    F                 \n5 QB_NZ_F_281 LOT     530  1130 46-55                    F                 \n6 QB_NZ_F_281 START   851  1810 46-55                    F                 \n# ℹ 8 more variables: participant_nz_ethnic &lt;chr&gt;, word_freq &lt;int&gt;, word &lt;chr&gt;,\n#   time &lt;dbl&gt;, vowel_duration &lt;dbl&gt;, articulation_rate &lt;dbl&gt;,\n#   following_segment_category &lt;chr&gt;, amplitude &lt;dbl&gt;\n\n\nWe see a mix of numerical values and characters.\nAnother useful function is summary which provides some nice descriptive statistics.\n\nsummary(vowels)\n\n   speaker             vowel               F1_50            F2_50     \n Length:26331       Length:26331       Min.   : 208.0   Min.   : 473  \n Class :character   Class :character   1st Qu.: 404.0   1st Qu.:1401  \n Mode  :character   Mode  :character   Median : 473.0   Median :1730  \n                                       Mean   : 506.1   Mean   :1733  \n                                       3rd Qu.: 586.0   3rd Qu.:2094  \n                                       Max.   :1054.0   Max.   :2837  \n                                                                      \n participant_age_category participant_gender participant_nz_ethnic\n Length:26331             Length:26331       Length:26331         \n Class :character         Class :character   Class :character     \n Mode  :character         Mode  :character   Mode  :character     \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n   word_freq          word                time         vowel_duration   \n Min.   :     0   Length:26331       Min.   :   0.54   Min.   :0.03000  \n 1st Qu.:   688   Class :character   1st Qu.: 150.30   1st Qu.:0.05000  \n Median :  2859   Mode  :character   Median : 313.10   Median :0.08000  \n Mean   :  8204                      Mean   : 472.27   Mean   :0.08746  \n 3rd Qu.:  7540                      3rd Qu.: 578.62   3rd Qu.:0.11000  \n Max.   :111471                      Max.   :3352.78   Max.   :1.81000  \n NA's   :7                                                              \n articulation_rate following_segment_category   amplitude    \n Min.   :0.7949    Length:26331               Min.   :35.53  \n 1st Qu.:4.3143    Class :character           1st Qu.:61.94  \n Median :4.8843    Mode  :character           Median :66.61  \n Mean   :4.9352                               Mean   :66.34  \n 3rd Qu.:5.4927                               3rd Qu.:70.80  \n Max.   :8.8729                               Max.   :91.95  \n                                              NA's   :31     \n\n\nLook at the values for amplitude. Reading down the column we see that the minimum value for amplitude is \\(35.53\\), the first quartile is \\(61.94\\), and so on, until we read the entry NA's, which tells us that \\(31\\) entries in the column are missing.\nMany of the entries in this summary just say Class: character. Sometimes, we can get more information by turning a column into a factor variable. A factor variable is like a character variable, except that it also stores the range of possible values for the column. So, for instance, there is a short list of possible vowels in this data set. We can use the factor() function to create a factor variable and look at the resulting summary.\n\n# The use of `$` here will be explained in a moment.\nsummary(factor(vowels$vowel))\n\n  DRESS  FLEECE    FOOT   GOOSE     KIT     LOT   NURSE   START   STRUT THOUGHT \n   4596    3379     741    1454    3639    2428    1137    1272    3162    2012 \n   TRAP \n   2511 \n\n\nNow we see how many instances of each vowel we have in the data, rather than just Class: character.\nOf course, properly interpreting any of these columns requires subject knowledge and proper documentation of data sets! We will leave this aside for the moment as we work on the mechanics of data processing in R.\n\nAs always, check the documentation for a function which is new to you. Enter ?read.csv in the console.\n\nWhat is the default seperator between values for read.csv? \nWhich argument would you change if your csv does not have column names? \n\n\n\n\n2.1.2 Accessing Values in a Data Frame\nHow do we see what values are in a data frame? In RStudio, we can always click on the name of the data frame in the environment pane (recall: the environment pane is at the top right of the RStudio window). This will open the data as a tab in the source pane. It should look like a familiar spreadsheet programme, with the exception that you can’t modify the values.\nYou will very frequently see code that looks like this some_name$some_other_name. This allows us to access the object with the name some_other_name from an object with the name some_name. We’ve just loaded a big data frame. This data frame has a name (vowels — which you can see in the environment pane) and it has columns which also have names (for instance, participant_age_category). We can get access to these columns using $:\n\nword_frequencies &lt;- vowels$word_freq\n\nLook in the environment pane in the ‘Values’ section and you will see a new name (word_frequencies), followed by its type (‘int’ for ‘integer’ — numbers that don’t need a decimal point), how many values it has (\\(26331\\)) and the first few values in the vector. So the $ has taken a single column from the data frame we loaded earlier and we have assigned this to the variable word_frequencies). Enter the name word_frequencies in your console, and you will see all of the values from the word_frequencies vector.\nThere are a few ways to access values from this vector. We can use square brackets to get a vector which contains a subset of the original vector. If we wanted the first hundred elements from the vector, we would use square brackets and a colon:\n\nword_frequencies[1:100]\n\n  [1]     38   1606     97   5476   5151    845    797  34640    726   5879\n [11]   2405  24552   1233    556    347   2038   2175     63  24552     24\n [21]   4376  34640      0   8249   3080    762    383   6555    521     24\n [31]   2858   3080  29391   2858   9525  99820   9525   4376    521   3291\n [41]   5046     55    642   4376   6555    420      0    420  14049  12551\n [51]   5476  22666  21873   1340   5411   1492 111471    969     98    203\n [61]   2075   1147   1237   3299   2812   1237   4546   4135      0   5428\n [71]    785   1492  15724  11914    644   3371    644  11943  11943   3123\n [81]   1385   3123   5891    590   1078  24552    456    989   1381     78\n [91]  34640   1487   1487    688   1330      0      0    284      0      0\n\n\nThe colon produces a sequence from the number on the left to the number on the right, and the square brackets produce a subset of word_frequencies. We can put a single number inside square brackets. The result is a vector with a single value in it:\n\nword_frequencies[7]\n\n[1] 797\n\n\nWe can also use negative numbers to exclude the numbered elements. Here we exclude the values from 1 to 26231.\n\nword_frequencies[-1:-26231]\n\n  [1]  1625   136   136   128  4628   661    10  1938   244  2134  6515   424\n [13] 34640 14049  7749   750   892   384  2178  7749  2178   101  4161 26215\n [25]   428 16068   520  4843  5236  4135 11344  5843  7749 33749  4052     8\n [37]    42  6515  1611 10720    97   202   194 33749   164   164    97 43071\n [49] 11914    99  5236 22697  8880    37   518   243    10    56   892   384\n [61]  7749    10  1938  7540  3080  4135     0    37   750  3645  4100  4100\n [73]    29    19  3099  1581  3121  5242  4342  3524    29  1508  9931   358\n [85]   146 10720    97   911  4455  1223   153    56  3056   244    56   313\n [97] 34640  1638    56   711\n\n\nThis is equivalent to:\n\nword_frequencies[26232:26331]\n\n  [1]  1625   136   136   128  4628   661    10  1938   244  2134  6515   424\n [13] 34640 14049  7749   750   892   384  2178  7749  2178   101  4161 26215\n [25]   428 16068   520  4843  5236  4135 11344  5843  7749 33749  4052     8\n [37]    42  6515  1611 10720    97   202   194 33749   164   164    97 43071\n [49] 11914    99  5236 22697  8880    37   518   243    10    56   892   384\n [61]  7749    10  1938  7540  3080  4135     0    37   750  3645  4100  4100\n [73]    29    19  3099  1581  3121  5242  4342  3524    29  1508  9931   358\n [85]   146 10720    97   911  4455  1223   153    56  3056   244    56   313\n [97] 34640  1638    56   711\n\n\nThe use of the colon (:) creates a vector whose elements make up a numerical sequence. The vector we put inside the square brackets doesn’t need to be a sequence though. If we wanted the 3rd, 6th, 10th, and 750th entries in the vector we would say:\n\nword_frequencies[c(3, 6, 10, 750)]\n\n[1]   97  845 5879  644\n\n\nWe are again using c() to create a vector.\nYou can’t mix negative and positive numbers here:\n\nword_frequencies[c(3, 6, -10, 750)]\n\nError in word_frequencies[c(3, 6, -10, 750)]: only 0's may be mixed with negative subscripts\n\n\nIn this case, the error message is reasonably understandable.\nIn addition to numeric vector, we can subset with logical vectors. These are vectors which contain the values TRUE and FALSE. This is particularly important for filtering data. Let’s look at a simple example. We’ll create a vector of imagined participant ages and then create a logical vector which represents whether the participants are over 18 or not.\n\nparticipant_ages &lt;- c(10, 19, 44, 33, 2, 90, 4)\nparticipant_ages &gt; 18\n\n[1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n\n\nThe first participant is not older than \\(18\\), so their value is FALSE.\nNow, say we want the actual ages of those participants who are older than 18 we can combine the two lines above\n\nparticipant_ages[participant_ages &gt; 18]\n\n[1] 19 44 33 90\n\n\nIf you want a single element from a vector, then you use double square brackets ([[]]). So, for instance, the following will not work because it attempts to get two elements using double square brackets:\n\nparticipant_ages[[c(2, 3)]]\n\nError in participant_ages[[c(2, 3)]]: attempt to select more than one element in vectorIndex\n\n\nWhile this may seem like a very small difference, it can be a source of errors in practice. Some functions care about the difference between, say, a single number and a list containing a single number.\nWe can use square brackets with data frames too. The only differences comes from the fact that data frames are two dimensional whereas vectors are one dimensional. If we want the entry in the second row and the third column of the vowels data, we do this:\n\nvowels[2, 3]\n\n[1] 440\n\n\nWe can again use sequences or vectors. For instance:\n\nvowels[1:3, 4:6]\n\n# A tibble: 3 × 3\n  F2_50 participant_age_category participant_gender\n  &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;             \n1  2050 46-55                    F                 \n2  2320 46-55                    F                 \n3  1795 46-55                    F                 \n\n\nHere we get the values for the first three rows of the data frame from the fourth, fifth, and sixth columns.\nIf we want to specify just rows, or just columns, we can leave a blank space inside the square brackets:\n\n# rows only:\nvowels[1:3, ]\n\n# A tibble: 3 × 14\n  speaker     vowel F1_50 F2_50 participant_age_category participant_gender\n  &lt;chr&gt;       &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;             \n1 QB_NZ_F_281 GOOSE   427  2050 46-55                    F                 \n2 QB_NZ_F_281 DRESS   440  2320 46-55                    F                 \n3 QB_NZ_F_281 NURSE   434  1795 46-55                    F                 \n# ℹ 8 more variables: participant_nz_ethnic &lt;chr&gt;, word_freq &lt;int&gt;, word &lt;chr&gt;,\n#   time &lt;dbl&gt;, vowel_duration &lt;dbl&gt;, articulation_rate &lt;dbl&gt;,\n#   following_segment_category &lt;chr&gt;, amplitude &lt;dbl&gt;\n\n\n\n# columns only:\nvowels[, 1:3]\n\n# A tibble: 26,331 × 3\n   speaker     vowel F1_50\n   &lt;chr&gt;       &lt;chr&gt; &lt;int&gt;\n 1 QB_NZ_F_281 GOOSE   427\n 2 QB_NZ_F_281 DRESS   440\n 3 QB_NZ_F_281 NURSE   434\n 4 QB_NZ_F_281 KIT     554\n 5 QB_NZ_F_281 LOT     530\n 6 QB_NZ_F_281 START   851\n 7 QB_NZ_F_281 DRESS   415\n 8 QB_NZ_F_281 STRUT   805\n 9 QB_NZ_F_281 START   857\n10 QB_NZ_F_281 TRAP    624\n# ℹ 26,321 more rows\n\n\nThe filtering code we looked at above is now a little more useful. What if we want to explore just the data from the female participants? We can use a logical vector again and the names of the columns.\n\nvowels_f &lt;- vowels[vowels$participant_gender == \"F\", ]\n\nWe have just filtered the entire data frame using the values of a single column. Look at the environment pain and you should now see two data frames: vowels and vowels_f. You should also see that one has many fewer rows than the other.\nNB: Don’t confuse == and =! The double == is used to test whether the left side and the right side are equal. The single = behaves like &lt;-. Here’s the kind of error you will see if you confused them:\n\n# This code is incorrect\nvowels_f &lt;- vowels[vowels$participant_gender = \"F\", ]\n\nError in parse(text = input): &lt;text&gt;:2:46: unexpected '='\n1: # This code is incorrect\n2: vowels_f &lt;- vowels[vowels$participant_gender =\n                                                ^\n\n\nIt says unexpected '='. Usually this is a good sign that you should use ==.\nWe can also use names to filter. We know already that columns have names. We can give their names inside the square brackets. For instance:\n\nvowels_formants &lt;- vowels[, c(\"F1_50\", \"F2_50\")]\n\nThe above creates a data frame called vowels_formants containing all rows of the vowel data but only the columns with the names “F1_50” and “F2_50”.\nWith data frames, you can also just enter the names without the comma.\n\nvowels_formants &lt;- vowels[c(\"F1_50\", \"F2_50\")]\n\nThis code has the same effect as the previous code.\nTo get access to a single column, you can use a name with double square brackets. e.g.:\n\nword_frequencies &lt;- vowels[[\"word_freq\"]]\n\nIn fact, some_name$some_other_name is a shorthand form of some_name[[\"some_other_name\"]]. A single element of a list or vector returned by $ and [[]], whereas [] returns a list or vector which may contain multiple elements.1\n\nIf data_frame is a data frame, what will data_frame[5:6, ] return?\n\n This code will produce an error. The fifth and sixth row of the data frame The fifth and sixth columns of the data frame\n\nIf data_frame is a data frame, what will data_frame[, c(3, 6, 9)] return?\n\n This code will produce an error. The third, sixth, and ninth row of the data frame The third, sixth, and ninth column of the data frame.\n\nImagine you save a vector to the variable vector_name, as follows:\n\nvector_name &lt;- c(5, 2, 7, 3, 2)\n\nWhat would be the output of vector_name &gt; 2?\n\n [1]  TRUE FALSE  TRUE  TRUE FALSE TRUE FALSE [1] TRUE TRUE TRUE TRUE TRUE\n\nWhat would be the output of vector_name[3, 5]?\n\n [1] 7 2 TRUE Error in vector_name[3, 5] : incorrect number of dimensions\n\n\n\n\n2.1.3 Modifying Data\nWe’ve learned how to get access to data. But how to we change it? Usually this is as simple as an application of the &lt;-, which we have already seen.\nThe following block of code creates a new column in vowels which contains a log transformation of the word frequency column. Often this is a sensible thing to do with word frequency data.\n\nvowels$word_freq_ln &lt;- log(vowels$word_freq + 1)\n\nThis statement uses a few things we have already seen. Reading from the inside out, we see that the column with the name word_freq is being referenced from the vowels data frame (vowels$word_freq), and every element in the column is being increased by \\(1\\). Why? Well the logarithm of \\(0\\) is not defined and there are some \\(0\\)’s in this column. We take the log using the log() function (have a look at the documentation to see what the default behaviour of the function is). Finally, we use the &lt;- to put this data into a new column in the vowels data frame which we call word_freq_ln.\nWe can do this to individual elements as well. For instance, if we want to change the third entry in the participant_ages vector to \\(65\\), we would write participant_ages[3] &lt;- 65.\nIf we want to change the age of all participants with ages below 18 to \\(0\\), for whatever reason, we could say:\n\nparticipant_ages[participant_ages &lt; 18] &lt;- 0\n\nWe can also overwrite existing data in a data frame, including whole columns, this way.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "chapters/data_processing.html#processing-data-in-the-tidyverse",
    "href": "chapters/data_processing.html#processing-data-in-the-tidyverse",
    "title": "2  Data Processing",
    "section": "2.2 Processing Data in the tidyverse",
    "text": "2.2 Processing Data in the tidyverse\nThere are (at least) two interacting dialects of R: the style associated with the tidyverse and the ‘base R’ approach. The tidyverse is a collection of packages which work well together and share a design philosophy. The most famous of these packages is ggplot, which implements a flexible approach to producing plots. This package will be the subject of a future workshop. We will focus instead on dplyr, a package which implements a set of ‘verbs’ for use with data frames. For more information see https://www.tidyverse.org/.\nMany scripts and markdowns will start with library(tidyverse), which loads all of the core tidyverse packages. To emphasise the modular nature of the tidyverse, this chapter only loads dplyr, tidyr, and readr.\nFor example, one verb is rename(). This function renames existing columns of a data frame. Another is mutate(). The mutate() function changes the data frame, typically by modifying existing columns or adding new ones. Moreover, these functions can be strung together in step-by-step chains of data manipulation using ‘pipes’. Here is an example of data processing in the tidyverse style using these functions:\n\nvowels &lt;- vowels %&gt;%\n  mutate(\n    word_freq_ln = log(word_freq + 1)\n  ) %&gt;%\n  rename(\n    F1_midpoint = F1_50,\n    F2_midpoint = F2_50\n  )\n\nThe pipe works by taking the object on the left of the pipe and feeding it to the function on the right side of the pipe as the first argument. So, e.g. 2 %&gt;% log() is the same as log(2). In this case, the data frame vowels becomes the first argument to mutate(), the following arguments then modify the data frame. Notice that we only need to say word_freq to refer to the column (rather than vowels$word_freq). This is because mutate() knows the names of the columns in the data frame it receives. Once the ‘mutation’ has happened, the modified data frame is pased to the rename function, which renames the columns F1_50 and F2_50 to F1_midpoint and F2_midpoint respectively. To see that this has happened, double click on vowels in the environment pane.\nThere are ongoing interactions between base R and the tidyverse. One particularly prominent instance is the inclusion of a ‘pipe’ operator in base R (|&gt;) which behaves in a very similar way to the tidyverse pipe (%&gt;%). I now prefer to use the base R pipe (|&gt;). In practice, I always use the shortcut ctrl + shift + M/command + shift + M to insert the pipe. RStudio has an option to choose whether the result of this is the tidyverse pipe or the base R pipe. I prefer to use the base R pipe now.\n\n\n\nSetting to use the base R pipe (|&gt;) or the magrittr pipe (%&gt;%). These options can be set globally with Tools &gt; Global Options or for a specific project with Tools &gt; Project Options.\n\n\nNB: you don’t need to entirely adopt either style! \n\n2.2.1 Reading in Data (again!)\nBefore we look in more detail at the data manipulation techniques which come with the dplyr package, we should look again at reading data. The readr package comes with modifications to the base R methods for reading in csv and similar files. readr includes the function read_csv (note the use of an underscore rather than dots, as in the base R function).\n\nvowels &lt;- read_csv(here('data', 'qb_vowels.csv'))\n\nRows: 26331 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): speaker, vowel, participant_age_category, participant_gender, parti...\ndbl (7): F1_50, F2_50, word_freq, time, vowel_duration, articulation_rate, a...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne great advantage of read_csv is that the output tells us something about the way R has interpreted each column of the data and the names which have been given to each column. We see how many rows and columns, what delimited the values (as we expected, \",\"), we also see what type of data R thinks these columns contain. So, chr means text strings. These are mostly participant metadata, such as their gender or ethnic background. We also see dbl columns. These contain numerical data.2\nAnother change between read.csv() and read_csv() is that the data is now a ‘tibble’ rather than a base R data frame. For current purposes, you don’t need to worry about this distinction. All of the base R methods we introduced above work with tibbles and with data frames.\nFor more information on importing data see the relevant chapter of R for Data Science.\n\nIn the output of read_csv() above, F1_50 is listed as a chrdbl column\nTherefore it contains numerical datastringsbooleans (i.e. TRUE or FALSE)\n\n\n\n2.2.2 Some dplyr Verbs\nWhat are the main dplyr verbs?\nIf we want to filter a data frame, we use filter(). The first argument to filter() is a data frame. This argument is usually filled by the use of a pipe function (i.e., the filter() function usually appears within a pipe). The remaining arguments are a series of logical statements which say which rows you want to keep and which you want to remove. By ‘logical’ recall that we mean TRUE or FALSE. Rows which produce TRUE will be kept, those which don’t will not.\nWe can put multiple filtering statements inside the filter() function. Look at this code:\n\nvowels_filtered &lt;- vowels |&gt; \n  filter(\n    following_segment_category == \"other\",\n    !is.na(amplitude),\n    between(F1_50, 300, 1000),\n    vowel_duration &gt; 0.01 | word_freq &lt; 1000\n  )\n\nThere are four statements being used to filter. Each is on a new line, but this is a stylistic choice, rather than one required for the code to work. You don’t need to use $ with column names inside tidyverse verbs.\n\nThe first statement uses ==. This says that we only want rows in the data frame where the following_segment column has the value other.\nThe second statement uses the function is.na() to test whether the amplitude column is empty or not. If a row has no value for amplitude then is.na() produces TRUE. So is.na(amplitude) will select the rows which have no value for amplitude. We want all the values which are not empty. So we add a ! to the start of the statement. This reverses the values so that TRUE becomes FALSE and vice versa. This statement now removes all the rows without amplitude information.\nThe third statement uses a helpful function from dplyr called between() which allows us to test whether a column’s value is between a two numbers. In this case, we want our values for the F1_50 column to be between \\(300\\) and \\(1000\\). This is inclusive. That is, it includes \\(300\\) and \\(1000\\). I forget this kind of thing all the time. To check, enter ?between in the console.\nThe fourth statement is made up of two smaller ones, combined with a bar (|). The bar means ‘or’. This will be TRUE when either of the statements is TRUE. So, in this case, it selects rows which have a vowel_duration value greater than \\(0.01\\) and rows which have a word_freq value less than \\(1000\\).3\n\nWe have seen mutate() and rename() already. mutate() is used to create new columns and to modify existing columns. The statements in a mutate() function are all of the form column_name =, with an R statement defining the values which the column will take. These can be either a single value, if you want every row of the column to have the same value (e.g. version = 1 would create a column called version which always has the value \\(1\\)), or a vector with a value for each row of the data frame. If you get this wrong it will look like this:\n\nvowels |&gt; \n  mutate(\n    bad_column = c(1, 2, 3, 4)\n  )\n\nError in `mutate()`:\nℹ In argument: `bad_column = c(1, 2, 3, 4)`.\nCaused by error:\n! `bad_column` must be size 26331 or 1, not 4.\n\n\nIf you want a subset of the columns, you can use select(). Here is an example:\n\nparticipant_metadata &lt;- vowels_filtered |&gt; \n  select(\n    speaker,\n    contains('participant_')\n  ) |&gt; \n  unique()\n\nThe select() verb here has two arguments. The first is just the name speaker. This, unsurprisingly, selects the column speaker. The second, contains('participant_') uses the function contains() (also from dplyr) to pick out columns containing the string 'participant_' in their names.4 After selecting these columns, there are many duplicate rows, so we pass the result into the base R function unique() with a pipe. The result is a data frame with the meta data for each participant in the data. Look in the environment pane to see how many rows and columns there are in the data frame participant_metadata.\nThe relocate() function is sometimes useful with the nzilbb.vowels package and other packages where the order of the columns is important. It relocates columns within a data frame. See for instance,\nWe have just covered:\n\nmutate(): the change a data frame.\nrename(): to change the names of existing columns.\nfilter(): to filter the rows of a data frame.\nselect(): to select a subset of columns of the data.\nrelocate(): to move selected columns within a data frame.\n\n\nConsider the following code. Assume that vowels has columns called F1_50 andF2_50.\nvowels &lt;- vowels |&gt; \n  rename(\n    first_formant = F1_50,\n    second_formant = F2_50\n  )\n\nAfter running the code, qb_vowels will have a column called first_formant: TRUEFALSE.\nAfter running the code, qb_vowels will have a column called F2 _50: TRUEFALSE.\n\n\n\n\n2.2.3 Grouped Data\nAnother advantage of dplyr is that the same functions work for grouped data and ungrouped data. What is grouped data? It is data in which a group structure is defined by one or more of the columns.\nThis is most clear by example. In the data frame we have been looking at, we have a series of readings for a collection of speakers. There are only 77 speakers in the data frame, from which we get more than 20,000 rows. What if we want to get the mean values of these observations for each speaker? That is, we treat the data frame as one which is grouped by speaker.\nIn order to group data we use the group_by() function. In order to remove groups we use the ungroup() function. Let’s see this in action:\n\nvowels_filtered &lt;- vowels_filtered |&gt; \n  group_by(speaker) |&gt; \n  mutate(\n    mean_F1 = mean(F1_50),\n    mean_amplitude = mean(amplitude)\n  ) |&gt; \n  ungroup()\n\nThe above code groups the data, then uses mutate(), in the same way as we did above, to create two new columns. These use the mean() function, from base R, to calculate the mean value for each speaker. Have a look at the data, using one of the methods for accessing data we discussed above, to convince yourself that each speaker is given a different mean value. The same points apply to amplitude. Note, by the way, that the mean() function returns NA if there are any NA values in the column. This is a common issue. If you see NA when you expect a sensible mean, you can add na.rm = TRUE to the arguments of mean() or filter out any rows with missing information before you apply mean().\nSometimes we want summary information for each group. In this case, it is useful to have a data frame with a single row for each group. To do this, we use summarise rather than mutate. We can combine the output of the code block we have just looked at with the participant metadata as follows:\n\nvowels_summary &lt;- vowels_filtered |&gt; \n  group_by(speaker) |&gt; \n  summarise(\n    n = n(),\n    mean_F1 = mean(F1_50),\n    mean_amplitude = mean(amplitude),\n    gender = first(participant_gender),\n    age_category = first(participant_age_category)\n  )\n\nvowels_summary |&gt; \n  head()\n\n# A tibble: 6 × 6\n  speaker         n mean_F1 mean_amplitude gender age_category\n  &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       \n1 QB_NZ_F_138    86    572.           61.0 F      18-25       \n2 QB_NZ_F_161   273    433.           76.9 F      18-25       \n3 QB_NZ_F_169   100    461.           69.5 F      66-75       \n4 QB_NZ_F_195   138    558.           65.9 F      26-35       \n5 QB_NZ_F_200   259    521.           63.7 F      56-65       \n6 QB_NZ_F_213   218    513.           69.4 F      66-75       \n\n\nThe code above has two statements. We create a data frame called vowels_summary, which uses summarise() instead of mutate(). The second statement outputs the first six rows of vowels_summary using the head() function. Each row is for a different speaker and each column is defined by the grouping structure and summarise(). There is a column for each group (in this case just speaker), and then one for each argument to summarise(). The first, n, uses the n() function to count how many rows there are for each speaker. The second and third columns contain the mean value for the speaker for two variables. We then use the function first() to pull out the first value for the speaker for a given column. This is very useful in cases when every row for the speaker should have the same value. In this case, the speaker’s age category, for instance, does not change within a single recording so we can safely just take the first value for participant_age_category for each speaker.\n\nConsider the following code:\nvowels_summary &lt;- vowels |&gt; \n  group_by(participant_gender, vowel) |&gt; \n  summarise(\n    n = n(),\n    F1_50 = mean(F1_50),\n    any_loud = any(amplitude &gt; 85, na.rm = TRUE),\n    all_loud = all(amplitude &gt; 85, na.rm = TRUE)\n  )\n\nThere are 11 vowels and 2 genders in vowels. How many rows are there in vowel_summary? \n\nWhat is the function of the function n() in this code?\n\n To return the number of rows in each group. To return the number of rows in the entire data frame. To ensure the script is reproducible.\n\n\nExperiment with the mean() functiona. What does the function output if it gets NA values (e.g. mean(c(1, 2, 3, NA, 5)))? NA7.25\n\n\n\n\n2.2.4 Pivoting data\nThis is a quick note on pivot_longer() and pivot_wider() from the tidyr package. These enable you to change the repesentation of the data in your data frame so that more information is carried in rows (i.e., you have more rows, therefore a ‘longer’ data frame), or columns (i.e., you have more columns, therefore a ‘wider’ data frame).\nPivoting is tied up with the idea of a ‘tidy’ data set, which I haven’t written much about in this chapter so far. The basic idea is that we want data where each row is an observation and each column is a variable. Pivoting is useful for getting to this point (and that’s why the pivot function are in a package called tidyr).\nFor our purposes, it’s worth noticing that sometimes we need to shift between different ideas of what our observations are. Sometimes they might be individual formant readings, sometimes they might be individual vowel tokens, and sometimes they might be speakers. Pivoting is useful for shifting between these representations of the data.\nHere’s an example of a shift to a ‘longer’ formant.\n\n## Split F1_midpoint and F2_midpoint into distinct rows (now a 'formant reading'\n## is our observation rather than an individual vowel token.) That is, make the\n## data 'longer'.\nvowels_long &lt;- vowels_filtered |&gt; \n  pivot_longer(\n    # Which columns do we want to 'lengthen'?\n    cols = c(F1_50, F2_50),\n    # What do we want to name the column which identifies the kind of formant \n    # reading?\n    names_to = \"formant_type\",\n    # What do we want to name the column containing the formant values?\n    values_to = \"formant_value\"\n  )\n\nhead(vowels_long |&gt; select(speaker, vowel, formant_type, formant_value))\n\n# A tibble: 6 × 4\n  speaker     vowel formant_type formant_value\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;\n1 QB_NZ_F_281 DRESS F1_50                  440\n2 QB_NZ_F_281 DRESS F2_50                 2320\n3 QB_NZ_F_281 NURSE F1_50                  434\n4 QB_NZ_F_281 NURSE F2_50                 1795\n5 QB_NZ_F_281 START F1_50                  851\n6 QB_NZ_F_281 START F2_50                 1810\n\n\nThe above code moves us from a data frame with 17240 to one with 34480 rows. The ‘observations’ are now individual formant readings.\nNow look at an example where we pivot ‘wider’:\n\n# What is we want a single row for each speaker, with their mean value for each\n# vowel as columns.\nvowels_wide &lt;- vowels_filtered |&gt;\n  # Select only relevant data\n  select(\n    speaker, vowel, F1_50, F2_50,\n    contains(match = \"participant_\")\n  ) |&gt; \n  pivot_wider(\n    # Column names come from 'vowel' column\n    names_from = vowel,\n    # Values come from both midpoint columns\n    values_from = c(F1_50, F2_50),\n    # We aggregate using the `mean` function if there is more than\n    # one value for each cell of the table. i.e., the `F1_midpoint_DRESS`\n    # column will end up with the mean F1 value across *all* the speakers tokens\n    # of DRESS.\n    values_fn = mean\n  )\n\nhead(vowels_wide |&gt; select(speaker, matches(\"F1|F2\")))\n\n# A tibble: 6 × 23\n  speaker     F1_50_DRESS F1_50_NURSE F1_50_START F1_50_STRUT F1_50_GOOSE\n  &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 QB_NZ_F_281        439.        417         858.        606.        389.\n2 QB_NZ_F_377        396.        372.        771.        646.        388 \n3 QB_NZ_F_432        439.        443.        886.        653.        394.\n4 QB_NZ_F_449        439         443.        847.        692.        467.\n5 QB_NZ_F_470        495.        524         926.        816         380 \n6 QB_NZ_F_478        408.        403.        804.        639.        381.\n# ℹ 17 more variables: F1_50_FLEECE &lt;dbl&gt;, F1_50_TRAP &lt;dbl&gt;, F1_50_LOT &lt;dbl&gt;,\n#   F1_50_FOOT &lt;dbl&gt;, F1_50_KIT &lt;dbl&gt;, F1_50_THOUGHT &lt;dbl&gt;, F2_50_DRESS &lt;dbl&gt;,\n#   F2_50_NURSE &lt;dbl&gt;, F2_50_START &lt;dbl&gt;, F2_50_STRUT &lt;dbl&gt;, F2_50_GOOSE &lt;dbl&gt;,\n#   F2_50_FLEECE &lt;dbl&gt;, F2_50_TRAP &lt;dbl&gt;, F2_50_LOT &lt;dbl&gt;, F2_50_FOOT &lt;dbl&gt;,\n#   F2_50_KIT &lt;dbl&gt;, F2_50_THOUGHT &lt;dbl&gt;\n\n\nHere we have switched from a representation of the data in terms of vowel tokens to one in terms of speakers. We have aggregated the data so that each cell contains the mean value for each speaker of each vowel and formant reading. For instance, there is now a column called F1_50_DRESS, which contains the mean midpoint first formant reading for each speaker’s dress vowel.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "chapters/data_processing.html#further-resources",
    "href": "chapters/data_processing.html#further-resources",
    "title": "2  Data Processing",
    "section": "2.3 Further Resources",
    "text": "2.3 Further Resources\n\nFor a fuller introduction to data transformation in the tidyverse see R for Data Science\nFor a discussion more focused on linguistics see the second chapter of Winter (2019).\nFor a discussion of the differences between base R and dplyr approaches to data processing see this vignette from dplyr.\n\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "chapters/data_processing.html#footnotes",
    "href": "chapters/data_processing.html#footnotes",
    "title": "2  Data Processing",
    "section": "",
    "text": "There are some slight differences. If you are interested see the relevant section of Advanced R↩︎\nA dbl is a double length floating point number… Just think a number which can have a decimal point. Computers are, obviously, finite systems. Numbers are not (well… you could become a strict finitist I suppose). The technicalities of representing numbers on computers are very interesting, but we will avoid them where we can!↩︎\n This juggling of ‘true’ and ‘false’ is a bit of formal logic and make take a while to get your head around if you haven’t come across formal logic before.↩︎\n The documentation for select() covers a bunch of other helper functions like contains(). As usual, just type ?select in the console.↩︎",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "chapters/exploratory_data_vis.html",
    "href": "chapters/exploratory_data_vis.html",
    "title": "3  Exploratory Data Visualisation",
    "section": "",
    "text": "3.1 Overview\nSo far, the approach taken in these workshops has been detached from the actual work of data analysis in linguistics and allied fields.\nWe have learnt enough now that we can start to approach more ‘real world’ data analysis examples. From here on out, new material will be illustrated with actual problems.\nThe primary aim of this session is what it says on the tin, namely, to introduce exploratory data visualisation. This is one of the most important skills for data analysis. We will particularly focus on the ggplot2 package from the tidyverse.\nThe secondary aim of this session is to introduce some basic R and open science infrastructure, and a particular research area, which will follow us through this and future sessions. We will be exploring the effect on word duration of ‘word usage factors’, such as word frequency, where in a sentence the word typically appears, how likely the word is in a context.\nBefore we turn to ggplot2, we will need to orient ourselves with respect to this secondary aim.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Visualisation</span>"
    ]
  },
  {
    "objectID": "chapters/exploratory_data_vis.html#start-a-new-rstudio-project",
    "href": "chapters/exploratory_data_vis.html#start-a-new-rstudio-project",
    "title": "3  Exploratory Data Visualisation",
    "section": "3.2 Start a New RStudio Project",
    "text": "3.2 Start a New RStudio Project\nRStudio projects are one of the primary ways to organise a data analysis project in the R ecosystem. They enable the integration of data files, R scripts, and supplementary materials. Ideally, you can share the entire project online along with any publications.1\nTo create a new RStudio project in RStudio:\n\nGo to File &gt; New Project\nSelect ‘New Directory’ and then ‘New Project’\nUse the Browse... button to choose a directory for the project (you might have to create a new one). For instance, Documents/research_projects/ then enter a name for the project directory in the Directory name box (Figure 3.1).\nPress Create Project.2\n\n\n\n\n\n\n\nFigure 3.1: New project wizard.\n\n\n\nWe will look at the use of renv in the near future.\nFor now, we need to populate our project with a sensible collection of folders/directories. You will have your own ideas about the best way to do this. For our current purposes, I’ll insist on three: data, plots, and markdown.\nHow do you create these? You can do it using the file browser which comes with your operating system or from inside the RStudio window in the output pane (recall: at the bottom right of your RStudio window). There is a New Folder bottom at the top of the Files tab.\n\n\n\n\n\n\nFigure 3.2: Files tab of Output pane with ‘New Folder’ button highlighted.\n\n\n\n\n3.2.1 Create a Quarto Markdown Document\nWe want a sharable record of our data analysis journey. The ideal format for this is one which enables us to both set out the code we used and explain the project to our fellow researchers. Markdown is an ideal format for this.\nMarkdown is a markup language, like HTML or LaTeX, which aims at human readability. Quarto is a publishing platform for converting markdown documents into html files (which can be viewed in a web browser and/or hosted online for anyone to view), pdf files (via LaTeX), or even Word files. The materials you are reading now were produced in RStudio using Quarto Markdown.\nYou are likely to head about RMarkdown. Quarto is a successor to RMarkdown which is less tied to R. It can be used just as easily with Python or Julia (other programming languages used for data analysis). With very few exceptions, RMarkdown files work with Quarto. One difference you are likely to notice is that the names of RMarkdown files end in .Rmd and the names of Quarto files end in .qmd.\nCreate a new Quarto Document using File &gt; New File &gt; Quarto Document.... This will open another box where you can enter the name of the file and your author details.\n\n\n\n\n\n\nFigure 3.3: The New Quarto Document window.\n\n\n\nYou should enter some variant of “Data Exploration” into the Title: field and, if you like, add your name in Author:. Select HTML from the options available and leave everything else as it is. Press Create.3\nThe default Quarto document will appear in front of you in the source pane. Save this file (Ctrl + S or Cmd + S), placing it in the markdown directory. Give it a sensible name like data_exploration.qmd.\nQuarto documents have two main sections. At the top, you get the YAML header. This sets global options for the document. The YAML header for the default quarto document is very small:\n---\ntitle: \"data_exploration\"\nformat: html\n---\nQuarto is installed with RStudio now. Check that your Quarto installation is working properly by pressing Render on the tool bar at the top of the source pane (or Ctrl/Cmd + Shift + k).\nIf successful, your web browser should open with a fairly boring looking document.\nHere are the YAML header settings I tend to use (assuming you already have title and author specified):\ndate: today\nformat: \n  html:\n    theme: flatly\n    toc: true\n    toc-depth: 6\n    toc-expand: true\n    toc-location: right\n    code-summary: \"To view code click here\"\n    anchor-sections: true\n    number-sections: true\n    cap-location: margin\n    title-block-banner: true\n    fig-responsive: true\n    lang: 'en-GB'\n    execute:\n      warning: false\n    embed-resources: false\nThe actual content appears in the body of the Quarto document. In Markdown we use # signs for section headings. Usually, the sections in an html document will start with ##. If you want a subsection (i.e., a section within a section) use ###, if you want a subsubsection use #### etc etc. You will also see code blocks. These start and end with three backticks (```). The language used inside the block is given inside curly brackets (e.g. {r}).\nOptions to control the block appear as special comments (e.g. #| echo: false) in the second code block, which means the block will be hidden in the rendered document (but will run in the background). It is often useful to use #| eval: false, which prevents a block from being run when the document is rendered. This is helpful in situations where there is a code block which takes a long time to run.\nThere are many good tutorials on Quarto and Markdown available online. If you like videos, I recommend:\n\nhttps://www.youtube.com/watch?v=_f3latmOhew (this one introduces the visual editor.)\nhttps://www.youtube.com/watch?v=yvi5uXQMvu4\n\nFor a text tutorial, you can’t go past https://r4ds.hadley.nz/quarto.\n\nHave a look at the reference page for HTML documents in Quarto. This tells you all the options that can go in the YAML header. It’s worth bookmarking this page.\nAnswer the following questions by looking at the reference document and experimenting by changing options and re-rendering.\nThe code options control some of the behaviour of code blocks.\n\nWhat option would you use to ‘fold’ all code blocks (i.e., create a document in which you have to click to show any code blocks)? \n\nThe figures options help to control the output of plots. These are often useful.\n\nI want all figures to be aligned in the centre of the page, how should I finish the following in my YAML header: fig-align: .\n\nFrustratingly, the default unit for figures generated by code blocks is inches. The defaults for each format are here\n\nSay I want the width of all my figures to be 8 inches rather than 7, complete the following entry in my YAML header: widthfig-widthfig-formatfig-asp : \nWhat option should I use with df-print to produce paged tables (i.e., tables which allow you divide table output into multiple pages)? defaultpagedkabletibble.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Visualisation</span>"
    ]
  },
  {
    "objectID": "chapters/exploratory_data_vis.html#word-duration-and-usage-factors",
    "href": "chapters/exploratory_data_vis.html#word-duration-and-usage-factors",
    "title": "3  Exploratory Data Visualisation",
    "section": "3.3 Word Duration and Usage Factors",
    "text": "3.3 Word Duration and Usage Factors\nWe need some data to explore. There is no shortage of data from previous studies for us to look at thanks to the increased takeup of open science practices.\nWe’re going to explore data from Sóskuthy and Hay (2017). Sóskuthy and Hay investigate variation in the duration of words. That is, the time, in seconds, that it takes to pronounce a word. They replicate a series of effects already reported in the literature in which ‘usage factors’, such as how frequent the word is, how often it is at the end of an utterance, and how predictable it is from the surrounding words, affect word duration. For instance, the more predictable a word is from its context, the shorter it is likely to be.\nSóskuthy and Hay then go on to find evidence that changes in these word usage factors affect word duration over the course of the 100+ years represented in the Origins of New Zealand English (ONZE) corpus. The explanation of this requires some kind of feedback between the production of words and the perception of words and this has significant consequences for how changes in language over time are understood.\nIn this chapter, we will use ggplot2, and some of the skills learned in previous chapters, to explore the data used in this paper. We will go on, in later chapters, to pre-register an analysis on the basis of this paper but applied to a different data set.\nWe need the data from Sóskuthy and Hay (2017). We can get it from the osf.io repository for the paper, here: https://osf.io/q5wgh/.\n\nHave a look around the osf.io page. There are four files in the “Files” section of the page. What are they?\n\nDownload the big_dia.csv file and put it in the data directory in your R project. The download button is a little hard to spot. The big download icon on the right is to download the file’s metadata. Instead, you should select the small ‘hamburger menu’ button and then Download file (Figure 3.4).\n\n\n\n\n\n\nFigure 3.4: Hamburger menu and download button on osf.io.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Visualisation</span>"
    ]
  },
  {
    "objectID": "chapters/exploratory_data_vis.html#getting-started-with-ggplot2",
    "href": "chapters/exploratory_data_vis.html#getting-started-with-ggplot2",
    "title": "3  Exploratory Data Visualisation",
    "section": "3.4 Getting Started with ggplot2",
    "text": "3.4 Getting Started with ggplot2\n\n3.4.1 Load packages and data\nAt the top of your Quarto Document, add a code block which loads the required packages. A convenient way to do this is to use the keyboard shortcut (Windows and Linux: Ctrl + Alt + I, macOS: Cmd + Option + I).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/jbl91/UC Enterprise Dropbox/Joshua Wilson Black/teaching/statistics_workshops\n\n\nHave a look at the output of this code block. It tells you which packages are included in the ‘core tidyverse packages’ and which version you are loading. Note that ggplot2 is included among them. A set of ‘conflicts’ follow: these are all of the form x masks y. The first of these is ✖ dplyr::filter() masks stats::filter(). This means that the name filter() previously referred to a function in the stats package called filter(), but now refers to the function of the same name from dplyr. Keep an eye out for clashes between packages in terms of function names.\nThe package here then tells you where ‘here’ is. This should be the directory in which your R project is stored.\nThen load the data from your data directory.\n\nbig_dia &lt;- read_csv(here('data', 'big_dia.csv'))\n\nNew names:\nRows: 271764 Columns: 30\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(7): TargetOrthography, foll_wf, prev_wf, Speaker, Corpus, TargetPhonem... dbl\n(20): ...1, YOB, WordDuration, dur.context, dur.context.avg, prev_pred_w... lgl\n(3): repeated.20, initial, final\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nThe output of this function tells us how read_csv() has interpreted the columns in the data file, how many rows there are, and any names which it has created for the columns: chr means the column contains text strings, dbl means it carries numbers, and lgl means it carries TRUE or FALSE values.4\nIt is a good idea to have a look in this data by using the RStudio viewer (Recall: View(big_dia) or double click big_dia in the environment pane.)\n\nRun spec(big_dia) in your console and look at the output. Identify the types of the following columns:\n\nwclass: characternumericallogical\nseg.no: characternumericallogical\nfoll_pred_wf_log: characternumericallogical\nrepeated.20: characternumericallogical\nTargetPhonemes: characternumericallogical\n\n\n\n\n3.4.2 The ‘grammar of graphics’\nWe’re almost at some actual code. The package ggplot2 is the kind of package where it is helpful to know something about the underlying design principles.\nThe gg in ggplot2 stands for “Grammar of Graphics”. It is an implementation, by Hadley Wickham, of the philosophy of data visualisation developed in (Wilkinson 1999).\nWhat is the ‘grammar’?\n\n…a statistical graphic is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system{.red}. (Wickham 2016, 4, emphasis mine)\n\nIn order, a plot is made up of:\n\nData\n\neach observation has a row,\neach variable has a column.\n\n‘Aesthetic mappings’\n\nFrom columns to a visual property.\n\ne.g. age category will be represented by colour,\ne.g. height will be represented by position on the x axis.\n\n\nLayers\n\nThe elements of the plot you actually see.\nGeometrical objects (geoms) display the data.\nStatistical transformations (stats), provide a visual summary.\n\nScales and coordinates\n\nScales specify the aesthetic mapping.\n\ne.g. use this shade of blue to represent the age category “25–34”.\n\nCoordinates determine the axes of the plot.\n\nFacets\n\nFacets allow us to divide the data into groups and plot them separately.\n\ne.g. we plot participants from Christchurch and from Otago in different ‘facets’.\n\n\nTheme\n\nThe theme determines additional visual details of the plot, e.g., font size and background colour.\n\n\nThe code for a ggplot2 plot builds up a graphic, layer by layer, ideally following the above order.\nLet’s illustrate this with some actual examples.\n\n\n3.4.3 One-Dimensional Plots\nOften, we want to understand the distribution of a particular variable. Often errors in the data are picked up when we notice something strange in the distribution of values that a variable takes. Usually, these plots will not appear in a paper, but are important for ensuring that our data is what we think it is.\nSo, let’s look at some ways to visualise this. Starting with continuous variables. This is what a histogram looks like in ggplot2:\n\nbig_dia |&gt; \n  ggplot(\n    mapping = aes(\n      x = WordDuration\n    )\n  ) +\n  geom_histogram()\n\n\n\n\n\n\n\nFigure 3.5: Basic histogram.\n\n\n\n\n\nA histogram divides a continuous variable up into a series of ‘bins’, then counts the number of rows where the value of the variable falls within the bin.\nWe can change the bin very simply by adding the bin argument to geom_histogram(). The default number of bins is 30. The appropriate number of bins depends on the specifics of the data. Play around a bit.\n\nbig_dia |&gt; \n  ggplot(\n    mapping = aes(\n      x = WordDuration\n    )\n  ) +\n  geom_histogram(bins = 200)\n\n\n\n\n\n\n\nFigure 3.6: Basic histogram (200 bins).\n\n\n\n\n\nIf you increase the number of bins, you can start to see influence of rounding (the gaps between values here).\nLet’s zoom out and think a bit more about the ‘grammar’ of graphics introduced in the previous section. The code starts with the data frame. This is then ‘piped’ in to the ggplot() function. The mapping argument to ggplot() is used to specify the aesthetic mapping between the data and any geometrical objects in the plot. In this case, the only mapping we specify is that position on the \\(x\\)-axis is mapped to the value of the variable named WordDuration. Finally, we add a ‘geom’, that is, a geometrical object, which will appear on the plot. There are many geoms available, in this case we use geom_histogram().5 The other features of the ‘grammar’ are dealt with by sensible defaults.\n\n\n\n\n\n\nWarning\n\n\n\nFunctions in ggplot2 are connected with the + sign rather than a pipe (|&gt; or %&gt;%). This is a common source of errors.\n\n\n\nPlot some histograms of other continuous variables in the data.\n\nWhat other ‘aesthetic features’ do these histograms have? Two are worth mentioning here: colour (color if you prefer) and fill. colour controls the colour of the lines on the plot, whereas fill determines the colour which fills the bars. We add aesthetic mappings within the aes() function. Colours are often useful for adding a factor variable to a plot. In this case, we could add fill = Corpus as an argument to the aes() function. This adds a mapping between the Corpus variable in the data (one of three corpora which this data is taken from) and the fill of the bars. To make this more visible, we will reduce the number of bins.\n\nbig_dia |&gt; \n  ggplot(\n    mapping = aes(\n      x = WordDuration,\n      fill = Corpus\n    )\n  ) +\n  geom_histogram(bins = 30)\n\n\n\n\n\n\n\nFigure 3.7: Histogram with 30 bins and fill for corpus.\n\n\n\n\n\nThe aes() function establishes the mapping between the colour of the bars in this histogram, but it does not specify which colours are mapped to which values in the data frame. For this we need a scale. Here, we will use scale_fill_manual() and some HTML colour codes.6\n\nbig_dia |&gt; \n  ggplot(\n    mapping = aes(\n      x = WordDuration,\n      fill = Corpus\n    )\n  ) +\n  geom_histogram(bins = 30) +\n  scale_fill_manual(\n    values = c(\"MU\" = \"#b120cf\", \"IA\" = \"#cfb120\", \"CC\" = \"#20cfb1\")\n  )\n\n\n\n\n\n\n\nFigure 3.8: Histogram with 30 bins and fill for corpus. Colour has been specified with a scale function.\n\n\n\n\n\nWe can use the same code, but swap out the geom. What if we want to know the density rather than the raw counts. That is, what if we don’t care about exactly how many rows appear between the durations 0.1 and 0.2 (for instance), but instead care what proportion of the rows appear in that area. In this case, we have corpora with different raw sizes. Their difference in height in the histogram might not be particularly interesting to us. In this case, the geom_density() function is useful.\n\nbig_dia |&gt; \n  ggplot(\n    mapping = aes(\n      x = WordDuration,\n      fill = Corpus,\n      colour = Corpus\n    )\n  ) +\n  geom_density(alpha = 0.2) +\n  scale_fill_manual(\n    values = c(\"MU\" = \"#b120cf\", \"IA\" = \"#cfb120\", \"CC\" = \"#20cfb1\")\n  ) +\n  scale_colour_manual(\n    values = c(\"MU\" = \"#b120cf\", \"IA\" = \"#cfb120\", \"CC\" = \"#20cfb1\")\n  )\n\n\n\n\n\n\n\nFigure 3.9: Density plot with colour and fill for corpus. Both fill and colour specified with a scale function. Transparency increased.\n\n\n\n\n\nNow the difference in size between the (sub)corpora is gone. We can see that the modal duration value for the CC corpus (which happens to be the most recent one), is lower than for the older corpora. Visually, it looks like the reduction in mode probably something like 0.05 lower for the CC corpus.\nAt the ggplot2 code level, note that inclusion of a new aesthetic mapping: the Corpus variable is now also mapped to colour. Note that this requires the addition of an additional scale_... function. Finally, note that I control an aesthetic feature of the density geom without mapping it to some feature of the data. Sometimes it is useful to control the aesthetic features of a plot without mapping them to data. In this case, I add alpha = 0.2 to the geom_density() function. The alpha argument controls how transparent a geom is. Finally, note that I’ve stopped explicitly writing mapping = before aes().\nWhat if we want to look at the distribution of a factor variable? Here, a close relative of geom_histogram() is useful: geom_bar().\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = wclass\n    )\n  ) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 3.10: Simple bar plot.\n\n\n\n\n\n\nAdd an aesthetic mapping using colour or fill to the previous plot. This should work is exactly the same way as worked for geom_histogram() above.\n\n\n\n3.4.4 Two-Dimensional Plots\nWe have already started adding multiple variables in the plots above (by using colour in addition to position on the \\(x\\)-axis). Now, we go into the two dimensional situation in more detail. Here we want to see relationships between two variables.\nThe most obvious example is the scatter plot. Let’s look at change in word duration given word frequency by mapping one variable to the \\(x\\)-axis and another to the \\(y\\)-axis. I’ll use a few other tricks which we have already discussed as well.\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = unigram.google.gb,\n      y = WordDuration\n    )\n  ) +\n  geom_point(alpha = 0.01)\n\n\n\n\n\n\n\nFigure 3.11: Scatter plot of word frequency and length.\n\n\n\n\n\nYikes, this looks like a bit of a sad slug. This is often the case in the social sciences, where effects sizes are quite small. Note also the vertical lines, these are likely all of the instances of a given word in the data frame (i.e., a series of readings for which the unigram frequency is the same but, because they are different instances of the same word, the duration is different).\nOften R functions are demonstrated using an inbuilt function with measurements from various cars called mtcars. Here is a plot of the fuel economy of a set of cars (on the \\(y\\)-axis) with their horsepower() along the \\(x\\)-axis). The more power a car has, the less likely it is to be very fuel efficient.\n\nmtcars |&gt; \n  ggplot(\n    aes(\n      x = hp,\n      y = mpg\n    )\n  ) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 3.12: Scatter plot of horsepower and miles per gallon from built-in mtcars data frame.\n\n\n\n\n\nWith a handful of plots and a clear physical relationship, things look a bit more understandable. I mention this just to note that not every scatter plot is a hairy slug.\nWith scatter plots, it is often useful to plot a trend line of some sort. This can be done with geom_smooth().\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = unigram.google.gb,\n      y = WordDuration\n    )\n  ) +\n  geom_point(alpha = 0.01) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nFigure 3.13: Scatter plot of word frequency (x-axis) and word duration (y-axis) with GAM smooth.\n\n\n\n\n\nThe geom_smooth() function, by default, fits a GAM model or a LOESS smooth (depending on the number of data points). A message is output with the plot which tells you what the smooth is. You can insist on a straight trend line by using the argument method = \"lm\".\nIt looks like, as predicted by previous literature, increased word frequency comes along with reduced word duration. Actually demonstrating this requires modelling which controls for other sources in variation. We might also think a bit about where this big thick line around \\(-10\\) on the \\(x\\)-axis comes from! It creates a ‘notch’ in the smooth line.\nThe final plot type we will look at is the violin plot (with geom_violin()). This is useful when we have a continuous variable and a factor variable. The following plot uses geom_violin() to plot the word duration of words at the end of an utterance vs. not at the end of an utterance.\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = final,\n      y = WordDuration\n    )\n  ) +\n  geom_violin(\n    draw_quantiles = c(0.05, 0.5, 0.95),\n    alpha = 0.3\n  )\n\n\n\n\n\n\n\nFigure 3.14: Violin plot of word duration divided by utterance finality. Bars indicate the 0.05, 0.5, and 0.95 quantiles of the word duration distribution.\n\n\n\n\n\nThe \\(x\\)-axis is now divided between TRUE and FALSE, i.e., whether a word is at the end of an utterance or not. We see that this distribution of word duration has a higher centre when the word is at the end of an utterance. This is brought our more strongly by adding lines at the 0.5 quantile (that is, the median, the middle value), and the identifying the extreme tails of the distribution by drawing a line at the 0.05 quantile (i.e. the value which divides the bottom 5% of the distribution from the rest) and the 0.95 quantile. The pattern here is also one which we would expect given the literature on word duration.\n\n\n3.4.5 Themes and Saving Plots\nThe current plots are not ‘production ready’. This is not a big deal as we are primarily interested in plots for our own purposes during data analysis. However, it is worth talking a bit about themes and labelling.\nLook at the following code:\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = final,\n      y = WordDuration\n    )\n  ) +\n  geom_violin(\n    draw_quantiles = c(0.05, 0.5, 0.95),\n    alpha = 0.3\n  ) +\n  labs(\n    title = \"Word duration by utterance finality\",\n    x = \"Word is utterance final\",\n    y = \"Word duration (seconds)\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 3.15: Violin plot of word duration divided by utterance finality. Bars indicate the 0.05, 0.5, and 0.95 quantiles of the word duration distribution. Labels and theme modified.\n\n\n\n\n\nHere I have changed the theme using a theme_... function.7 Many visual details can be controlled by theming functions and these are highly customisable. Often little features like, for instance, rotating labels on the axes, are dealt with by theming functions.\nI have also added a series of more informative axis labels using the labs() function. In addition to setting a title (with title =), I use the names of the aesthetic features used inside the aes function to control the labels.\nThe theme_set() function can be used to change the theme of all ggplot2 plots in an R sessions. I almost always add theme_set(theme_bw()) to the first block of my markdown files (along with the loading of libraries and data). Try this, and see what happens when you run the markdown from the start. All plots, apart from those which have had the theme manually specified, should have changed.\n\nLook up the prismatic library and its tools for controlling colour. There is a particularly useful function called check_color_blindness() which helps to determine if a collection of colours you have chosen works for different forms of colour blindness.\n\nHow do we get plots out of our Markdown? The best approach is to use ggsave(). ggsave() lets us specify a location to save our plot and various details of format and image quality which may be specified by a journal or other publisher. Using ggsave() is easiest if we save our plot to a variable. The following code saves a plot to a variable, outputs it to our plots directory.\n\nviolin_plot &lt;- big_dia |&gt; \n  ggplot(\n    aes(\n      x = final,\n      y = WordDuration\n    )\n  ) +\n  geom_violin(\n    draw_quantiles = c(0.05, 0.5, 0.95),\n    alpha = 0.3\n  ) +\n  labs(\n    title = \"Word duration by utterance finality\",\n    x = \"Word is utterance final\",\n    y = \"Word duration (seconds)\"\n  ) +\n  theme_bw()\n\nggsave(\n  filename = here(\"plots\", \"violin_plot.png\"), # format is controlled by the file name.\n  plot = violin_plot,\n  units = \"mm\", # unit determines what `width` and `height` mean\n  width = 80,\n  height = 100,\n  dpi = 300 # 'dots per inch', i.e., resolution.\n)\n\nIf you want the plot to also appear in the markdown document, as the previous plots have, then simply write the variable name as the last line in the code block.\n\n\n3.4.6 Plots in Markdown Documents\nYou will notice in the above that there are figure numbers and figure captions under the plots. These are created with code block options. In Quarto, these looks like the following, and appear as the first lines of a code block:\n#| label: fig-violin-plot\n#| fig.cap: An informative figure caption.\nYou can make figure captions appear across multiple lines in your code as follows:\n#| label: fig-violin-plot\n#| fig.cap: |\n#|    An even longer and more informative figure caption which\n#|    has to appear across multiple lines.\nThe label is a label used to refer to the code block and any output. You can use these labels to generate references to plots in the text of your document using the @ sign. So, for instance, the scatter plot above has the label fig-scatter-smooth. If I write @fig-scatter-smooth, the following will appear in the markdown text: Figure 3.13. Figure labels must start with fig- for this to work.\nTo see other options which affect the output of code blocks, go here: https://quarto.org/docs/computations/execution-options.html#figure-options.\nIn RMarkdown, these options appear in the curly brackets, separated by commas. That is, they look like this: {r fig-violin-plot, fig.cap = \"my caption...\"}.\n\n\n3.4.7 Final Thoughts on Exploratory Data Visualisation\nAlways plot your data. You have an incredible capacity to see problems in data when it is appropriately plotted. Visualisation can help you to see all sorts of errors which might arise.\nIn particular, if you know enough to formulate an interesting research question, you know enough to identify if your data is way outside of the range in which it should be.\nThis is what ‘exploratory data visualisation’ is all about.\nOne potential trap is worth mentioning: it is important to avoid loading in an unfamiliar data set, hunting for an interesting looking relationship between the variables in the data, and then proceeding as though this relationship was what you were looking for in the first place! If you found the relationship by poking around with some visualisation methods, then you are engaged in exploratory rather than confirmatory research.\n\n\n3.4.8 Useful Resources\n\nSee the ggplot2 cheat sheet. This can be useful to print and put somewhere visible while you write plot code: https://rstudio.github.io/cheatsheets/html/data-visualization.html\nThe ggplot2 book may be useful if you want to get more deeply into the package: https://ggplot2-book.org/\n\nThe book includes some great material on putting multiple plots together and on more complex combinations of geoms.\n\nI wrote a tutorial on plotting complex model outputs in vowel space plots here: https://lingmethodshub.github.io/content/R/animated_vowel_plots_tutorial/\nHere is a tutorial by Joey Stanley on vowel space plots: https://lingmethodshub.github.io/content/R/vowel-plots-tutorial/\n\n\n\n\n\nSóskuthy, Márton, and Jennifer Hay. 2017. “Changing Word Usage Predicts Changing Word Durations in New Zealand English.” Cognition 166 (September): 298–313. https://doi.org/10.1016/j.cognition.2017.05.032.\n\n\nWickham, H. 2016. Ggplot2: Elegant Graphics for Data Analysis. Use R! Springer International Publishing. https://books.google.co.nz/books?id=RTMFswEACAAJ.\n\n\nWilkinson, Leland. 1999. The Grammar of Graphics. Statistics and Computing. New York: Springer. http://digitool.hbz-nrw.de:1801/webclient/DeliveryManager?pid=2966363.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Visualisation</span>"
    ]
  },
  {
    "objectID": "chapters/exploratory_data_vis.html#footnotes",
    "href": "chapters/exploratory_data_vis.html#footnotes",
    "title": "3  Exploratory Data Visualisation",
    "section": "",
    "text": "Often, in practice, one shares a tidied version of a project without a full history. Perhaps, for instance, your ‘in-house’ data analysis uses the names of participants whereas your public release of data and code has to be anonyimised. There are advantages to getting to the ‘fit for public consumption’ phase as soon as possible and sharing as much of the history of the project as is feasible. I have, so far, failed to live up to this advice myself.↩︎\nIf you want to keep your current work in RStudio open, then tick the small box in the bottom left (Open in new session). This will open the project in a new RStudio window.↩︎\nYou may want to experiment with the ‘visual markdown editor’. I always use the source editor, but you may find the visual editor more user friendly. You can always switch between the two later.↩︎\nThe strings chr, dbl, and lgl will be more clear in the output if you run this code in RStudio.↩︎\nFor a full list of geoms see the reference to ggplot2: https://ggplot2.tidyverse.org/reference. The ggplot2 cheat sheet is also worth looking at: https://rstudio.github.io/cheatsheets/html/data-visualization.html↩︎\nThere are plenty of tools for generating HTML colours, for instance: https://htmlcolorcodes.com/color-picker/.↩︎\nSee details of predefined themes here: https://ggplot2.tidyverse.org/reference/ggtheme.html↩︎",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Visualisation</span>"
    ]
  },
  {
    "objectID": "chapters/linear_models.html",
    "href": "chapters/linear_models.html",
    "title": "4  Linear Models",
    "section": "",
    "text": "4.1 Overview\nIn this chapter we continue to explore the duration and word frequency data from Sóskuthy and Hay (2017). In the previous chapter we learned how to set up an R project in RStudio and plotted the data in various ways using ggplot2. In this chapter, we turn to modelling the data. In particular, we look at a very useful class of models: linear models. We’re going to draw some straight lines.\nWhat is a model? It’s a simplified representation of something. The phenomena we’re interested in in the human sciences, including linguistics, are incredibly complex. We have various strategies for reducing this complexity. Statistical models are an important (but not the only!) tool for this. Such models usually find patterns of association between variables in a data set. These patterns might then be used to predict future data, explain one variable in terms of others, or explore looking for surprising features of the data which might motivate future research.\nThis chapter will quickly run through some of the key workflow steps in fitting a linear model. We’ll start with a ‘simple linear regression model’ (Section 4.2). We’ll use a straight line to represent the relationship between word duration and word frequency. We’ll follow this workflow:\nThere is no need to do things in exactly this order all the time. But this is a nice pattern. Certainly, whatever pattern or workflow you do use, you should spend some time thinking about it.\nWe’ll then add additional variables to the model (Section 4.3), and consider how to fit interactions between variables (Section 4.4).\nYou will notice, in Section 4.6, that the material covered in this chapter corresponds to eight chapters in Winter (2019). We’re moving fast here. As you get in to your own projects, with your own data, you will need to explore each of these topics in more detail.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/linear_models.html#overview",
    "href": "chapters/linear_models.html#overview",
    "title": "4  Linear Models",
    "section": "",
    "text": "Plot data.\nFit a linear model using lm().\nCheck residual plots.\nLook at the model summary (summary()).\nPlot predictions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/linear_models.html#sec-simple",
    "href": "chapters/linear_models.html#sec-simple",
    "title": "4  Linear Models",
    "section": "4.2 Simple Linear Regression",
    "text": "4.2 Simple Linear Regression\n\n4.2.1 Straight Lines\nAt simplest, a linear model is just a straight line fit through data. If we have a two-dimensional space with a horizontal \\(x\\)-axis and a vertical \\(y\\)-axis, then we can specify any straight line using two numbers: the intercept and the slope. The intercept tells you where the the line intercepts the \\(y\\)-axis and the slope tells you how steep the line is.\n\n\nTo view the code click here\nset.seed(26)\n\nlines &lt;- tibble(\n  line_id = glue(\"line_{rep(1:5, each = 11)}\"),\n  x = rep(-5:5, times = 5),\n  intercept = rep(runif(5, min=-4, max = 4), each = 11),\n  slope = rep(runif(5, min=-3, max=3), each = 11),\n  y = intercept + x * slope\n)\n\nintercepts &lt;- lines |&gt; \n  filter(x == 0)\n\nlabels &lt;- lines |&gt; \n  filter(x != 0) |&gt; \n  group_by(line_id) |&gt; \n  slice_sample(n=1) |&gt; \n  mutate(label = glue(\"{round(slope, digits = 2)}\"))\n\nlines |&gt; \n  ggplot(\n    aes(\n      x = x,\n      y = y,\n      colour = line_id\n    )\n  ) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_path(linewidth = 2, show.legend = FALSE) +\n  geom_point(data = intercepts, size = 7, show.legend = FALSE) +\n  geom_label_repel(aes(label=label), data = labels, show.legend = FALSE)\n\n\n\n\n\n\n\n\nFigure 4.1: A collection of lines with the intercept indicated by a point. Text annotation indicate the slope term.\n\n\n\n\n\nIn Figure 4.1, the intercept is indicated with a large point. We see five distinct intercepts. The slope is indicated by the numbers in labels. Two of these are negative numbers, and the corresponding lines are going down as you look from left to right. The positive slopes correspond to lines which go up as you view from left to right. The larger the magnitude of the slope (i.e., the size of the number, ignoring whether it is positive or negative), the steeper the line.\nMathematically, a line in two-dimensional space is represented by the equation \\(y = a + bx\\), where \\(a\\) is the intercept term and \\(b\\) is the slope term. If you increase the intercept term, then \\(y\\) increases across the board. That is, changing the intercept moves the line up and down. The term \\(b\\) represents a relationship between the variables \\(y\\) and \\(x\\). If \\(b\\) is positive, then as \\(x\\) increases, \\(y\\) will also increase. If it is negative, \\(y\\) will decrease as \\(x\\) increases.\n\n\nWhich line has the slope closest to zero? RedGreenBluePurpleBrown\nWhich line has the intercept closest to zero? RedGreenBluePurpleBrown\nWhat slope value does a horizontal line have? \n\n\n\n\n4.2.2 Word Frequency and Word Duration\nIn the data set we looked at in the last chapter, we considered explanations of word duration and word frequency. We can fit a straight line through our data concerning word duration and word frequency. If we can do this, then the slope term should tell us something about the relationship between word duration and frequency. That is, we are modelling for the purpose of explanation.\nWe start our workflow by plotting the data. We already started this work in the previous chapter. Let’s look at a scatter plot again (Figure 4.2).\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = unigram.google.gb,\n      y = WordDuration\n    )\n  ) +\n  geom_point(alpha = 0.01) +\n  # Let's add some labels\n  labs(\n    x = \"Log word frequency (via. Google)\",\n    y = \"Word Duration (s)\"\n  )\n\n\n\n\n\n\n\nFigure 4.2: Scatter plot of word frequency and length.\n\n\n\n\n\nFigure 4.2 is characteristic of the messy data we find in work with large corpora of linguistic data. The key thing to note here is whether there is a shortage of data anywhere (here, we would notice, for instance, that the lower frequency areas to the left of the plot are much more sparce than the higher frequency areas), whether there are any data points which are totally implausible (and perhaps represent measurement errors). Check any such points. You may need to filter them out (using functions we have already looked at in the previous chapter).\nThe next step is to fit the model using lm(). Linear regression typically fits a straight line by ‘least squares’. It finds the line which minimises the sum of the squared differences between the line and the actual data points in the \\(y\\) dimension. Put simply, it finds the slope and intercept which are ‘closest’ to the actual data.\nWe specify a model using the formula argument. We can use variable names from our data set. If we do this, we have to specify the data set using the data argument. The formula for a simple linear regression just has the name of the variable to be explained (the ‘response’ variable) on the left of a ~ sign and the variable which we will use to explain it (the ‘explanatory’ variable) on the right.1 The following code block fits a simple model and associates it with the name simple_fit.\n\nsimple_fit &lt;- lm(\n  formula = WordDuration ~ unigram.google.gb,\n  data = big_dia\n)\n\nThe next step in our workflow is to check residual plots. The main ways in which a linear model can fail are present in ‘residual plots’. There are a few things to look for (see Winter 2019, 6.1). A ‘residual’ is the difference between a particular data point and the line. The residuals are the variation which is not captured by the model. We assume that this is normally distributed random noise. If there are clear patterns in the residuals, then we have good reason to think our model is missing something important.\nIn R, we can use the plot() function to generate ‘diagnostic plots’ of our residuals. When plotting the result of a linear model, we can use the which argument to specify which of the four available diagnostic plots we want.2 The first plot is a plot of the ‘fitted’ values and the residuals (Figure 4.3.\n\nplot(simple_fit, which = 1)\n\n\n\n\n\n\n\nFigure 4.3: Scatter plot of fitted values and residuals from our simple linear regression model.\n\n\n\n\n\nFigure 4.3 should look like a big slug. If we look along the \\(x\\) axis, we see ‘fitted’ values. So, for instance, if we look up in the straight vertical line from at \\(0.30\\) on the \\(x\\) axis, we see the range of residuals whenever the model says that \\(y\\) is \\(0.30\\). We don’t want any kind of ‘fan’ pattern here, where the distribution of residuals is much wider at one side of the plot than the other.\nWith this plot, and others like it, we are looking for large violations of the assumptions, rather than ensuring the the assumptions are perfectly satisfied.\nThe most worrying thing about this plot is the red line, which shows the mean values of the residuals departs from \\(0\\) as the fitted values increase. This suggests there might be some kind of non-linearity in the data.\nI also find ‘QQ plots’ useful. We can access this with which = 2.\n\nplot(simple_fit, which = 2)\n\n\n\n\n\n\n\nFigure 4.4: Quantile-quantile plot from our simple linear regression model.\n\n\n\n\n\nFigure 4.4 should show a series of points in a straight line. This is more or less true in the middle of the curve, but is violated heavily at both ends of the curve. This shows that we are missing something important in our simple model.\nWhat does this kind of plot show? We assume out residuals are normally distributed. The ‘theoretical quantiles` indicate the value we would expect from normally distributed residuals, the ’standardised residuals’ are the values we actually observe, but appropriately scaled. The fact that the curve is above the straight line indicates that there are many words which have much longer duration that we would assume given only information about the word frequency, and the assumptions of the linear regression model itself (including, importantly, that the relationship can be captured by a straight line.)\nSo, we have reason to be pretty sceptical that our model is capturing the patterns at the edges of the data. We should keep this in mind as we move to the next phase of our workflow: look at the model summary. Violation of model assumptions reduces the confidence we should have when looking at its results.3\nIt can be frustrating for someone to tell you things like ‘be a bit sceptical’, or, ‘this is basically fine, but not perfect’ when looking at this kind of diagnostic plot. You want clear answers to your questions. Unfortunately clear answers don’t always exist. But, in lieu of a perfect criterion for residual plot badness, the following code block produces data which, when modelled by a linear model, produce a very bad residual plot and a dubious QQ plot (Figure 4.5).\n\n\nTo view the code click here\nset.seed(100)\nwiggly_data &lt;- \n  tibble(\n    explanatory = runif(n = 1000, min = -4, max = 4)\n  ) |&gt; \n  mutate(\n    response = sin(explanatory * 10) + rt(n = 1000, 1.9)/10 \n  )\n \ndata_and_fit &lt;- wiggly_data |&gt; \n  ggplot(\n    aes(\n      x = explanatory,\n      y = response\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n  labs(\n    title = \"Simulated data and linear fit\"\n  )\n\nlm_fit &lt;- lm(response ~ explanatory, data = wiggly_data)\n\nfit_data &lt;-\n  tibble(\n    \"residuals\" = residuals(lm_fit),\n    \"predictions\" = predict(lm_fit)\n  ) \n\nresid_plot &lt;- fit_data |&gt; \n  ggplot(\n    aes(\n      x = predictions,\n      y = residuals\n    )\n  ) +\n  geom_point() +\n  labs(title = \"Residual plot\")\n\nqq &lt;- fit_data |&gt; \n  ggplot(\n    aes(\n      sample = residuals\n    )\n  ) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"QQ-plot\",\n    x = \"Theoretical quantiles\",\n    y = \"Residuals\"\n  )\n  \n\ndata_and_fit / (resid_plot + qq) + \n  plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nFigure 4.5: Simulated data with wave pattern. Panel A shows raw data and linear fit, panel B shows the residual plot, with a clear sin wave pattern rather than the (expected) random cluster. Panel C shows a QQ plot indicating residuals with more extreme values than expected from a linear model.\n\n\n\n\n\nThe summary() function outputs key features of the model.4 Let’s look at the output:\n\nsummary(simple_fit)\n\n\nCall:\nlm(formula = WordDuration ~ unigram.google.gb, data = big_dia)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41159 -0.08406 -0.01667  0.06675  0.94423 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.042965   0.001441   29.82   &lt;2e-16 ***\nunigram.google.gb -0.027695   0.000168 -164.85   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1178 on 271762 degrees of freedom\nMultiple R-squared:  0.09091,   Adjusted R-squared:  0.09091 \nF-statistic: 2.718e+04 on 1 and 271762 DF,  p-value: &lt; 2.2e-16\n\n\nWe start with a Call which says what the R statement which generated the model was (including the formula). We then see information about the distribution of the residuals, before the Coefficients: section. This is where we will find the intercept and slope terms of our model. The (Intercept) term tells us what the model predicts when \\(x\\) is 0. i.e., when the log word frequency is 0. Is this a useful thing to know? Have a look at the scatter plot and see what this would correspond to in terms of our data (Figure 4.2). We will consider some alternatives later. We look at the Estimate column to see the estimated value of the coefficients. In this case, we estimate a word duration of 0.04 when the log word frequency is 0.\nWe then look at the unigram.google.gb row. This will tell us the slope of the line. The best straight line through the data has a slope of -0.028.\nWhat else is in the table? Information which quantifies our uncertainty about the estimates and tests of how consistent the data is with the absence of an effect. That is, we’ve got a series of tests of null hypotheses. For the intercept, the null hypothesis is that the intercept is zero. For the slope, the null hypothesis is that the slope is zero. The t value column contains ‘test statistics’ which summarise features of the data set with respect to the hypothesis that there is no effect. The Pr(&gt;|t|) columns gives ‘p values’, which indicate the probability of observing the test statistic, or a more extreme value, if there is no actual effect. Low values, traditionally values less that 0.05, lead us to ‘reject the null hypothesis’. In this case, then, we reject the null hypothesis for both the intercept term and the slope term. The effect of word frequency on word duration is ‘statistically significant’ in this model.\nThe final stage of our workflow is to plot the predictions. We get access to the predictions using the predict() function.5 We use the interval argument to specify that we want confidence intervals around the predictions.\n\nmodel_preds &lt;- predict(\n  simple_fit, \n  interval = \"confidence\",\n  level = 0.95\n)\n\nsimple_preds &lt;- bind_cols(big_dia, model_preds)\n\n# Here's 10 random rows.\nsimple_preds |&gt; \n  select(WordDuration, unigram.google.gb, fit, lwr, upr) |&gt; \n  slice_sample(n=10)\n\n# A tibble: 10 × 5\n   WordDuration unigram.google.gb   fit   lwr   upr\n          &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1        0.310             -7.49 0.250 0.250 0.251\n 2        0.15              -8.52 0.279 0.279 0.279\n 3        0.190             -8.22 0.271 0.270 0.271\n 4        0.360             -8.67 0.283 0.283 0.283\n 5        0.570             -9.07 0.294 0.294 0.295\n 6        0.460             -7.73 0.257 0.257 0.258\n 7        0.465             -8.98 0.292 0.291 0.292\n 8        0.380             -9.99 0.320 0.319 0.320\n 9        0.240             -7.80 0.259 0.258 0.259\n10        0.220             -8.71 0.284 0.284 0.285\n\n\nThe output above shows the actual word duration scores, word frequencies, model predictions (fit) and lower (lwr) and upper (upr) bounds on the confidence intervals for 10 random rows of the data.\nWe can plot this without the original data as follows:\n\nsimple_preds |&gt; \n  ggplot(\n    aes(\n      x = unigram.google.gb,\n      y = fit,\n      ymin = lwr,\n      ymax = upr\n    )\n  ) +\n  geom_ribbon(alpha = 0.5) +\n  geom_path(colour = \"red\") +\n  labs(\n    x = \"Log word frequency (via. Google)\",\n    y = \"Predicted Word Duration (s)\"\n  )\n\n\n\n\n\n\n\nFigure 4.6: Model predictions with 95% confidence intervals.\n\n\n\n\n\nFigure 4.6 has an incredibly small confidence interval. This is because of the very large size of the data set!\nAnd with data:\n\nsimple_preds |&gt; \n  ggplot(\n    aes(\n      x = unigram.google.gb,\n      y = WordDuration\n    )\n  ) +\n  geom_point(alpha = 0.01) +\n  # Let's add some labels\n  geom_ribbon(\n    aes(\n      y = fit,\n      ymin = lwr,\n      ymax = upr\n    ),\n    alpha = 0.5\n  ) +\n  geom_path(\n    aes(\n      y = fit\n    ),\n    colour = \"red\") +\n  labs(\n    x = \"Log word frequency (via. Google)\",\n    y = \"Word Duration (s)\"\n  )\n\n\n\n\n\n\n\nFigure 4.7: Model predictions with 95% confidence intervals and original data.\n\n\n\n\n\nSometimes it is valuable to plot the model alongside the raw data. It can make it more visually clear just how much variation there is from the model. However, real patterns in large corpus data sets may be very small relative to the range of raw data values. This can make plotting both model and data in a single plot impractical.\n\n\n\n\n\n\nExplanation and Correlation\n\n\n\n\n\nWhy are we trying to explain word frequency using word duration rather than the other way around? There is no mathematical difference between the two options.\nThe simple answer is that we can think of plausible cognitive mechanisms by which word frequency might influence word duration. At simplest, this is something like the view that frequent signals are easier to process and this enables them to be ‘reduced’ in various ways, including in duration.\nThe alternative direction of explanation would propose that we have a tendency to prefer shorter words, so that words that are on average shorter somehow become more frequent. This doesn’t make sense for a number of reasons. For one thing, we are looking at word duration by token. If there were a push towards shorter and shorter words at this level, you would expect it to lead to faster speech across the board.\nThe point is that we always interpret statistical models through previous research and more general intuitions about plausibility. These might end up being wrong, but we have to assume something to get an experiment or study going!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/linear_models.html#sec-multiple",
    "href": "chapters/linear_models.html#sec-multiple",
    "title": "4  Linear Models",
    "section": "4.3 Multiple Linear Regression",
    "text": "4.3 Multiple Linear Regression\nIn simple linear regression, we have a single explanatory variable. In multiple linear regression, we have more than one. There are a few different considerations when we do this. We’ll separately look at adding categorical variables (columns which contain a finite number of ‘categories’: e.g. social categories like class or linguistic categories like what kind of consonant follows a vowel) and continuous variables (roughly, columns which contain numbers).\nOften, many variables are involved in the phenomena we explore or the hypotheses we test. Sometimes we add variables which we know have an independent effect on the response variable, but which we aren’t directly interested in, in order to control for the effect. This allows us to say things like: “holding the speed at which someone speaks constant, the height of this vowel increases as a person ages”. We thus avoid attributing variation due to the ‘control’ variable to the other variables we are investigating. This will make more sense with some examples.\nDo not just add all the variables you have as ‘control’ variables. Sometimes a ‘control’ variable can create effects which are not real. Think about each variable you add to a model.6\n\n4.3.1 Adding a Categorical Variable\nThere are a few categorical phenomena which affect word duration. For instance, whether or not a word appears at the end of an utterance affects its duration. We can incorporate this information into our model by adding a variable to the model which tracks whether observed word occurs at the end of an utterance or not.\nWhy do we want this information? We might be worried that our sample of low frequency words just happens, by random change, to contain a lot of tokens from the end of an utterance. We might then see an effect of word frequency which is actually due to where the words in our sample appear in an utterance. This is very unlikely in the dataset we are currently using, due to its sheer size (271764 tokens). But it could very well be a factor in a smaller data set.\nMore generally, we hope that our model is the best depiction of the features of the phenomena we are interested in which we can manage with the data at hand. If we suspect some variable has an independent effect of the response variable, and there is no other downside to including it, it is good to include it in the model.7\nLet’s have a quick visual look at the variable called final in the big_dia dataset. We’ll use a box plot.\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = final, \n      y = WordDuration\n    )\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFigure 4.8: Box plot indicating word duration by position in utterance (utterance final vs. non-final).\n\n\n\n\n\nOn the \\(x\\) axis, we distinguish whether an observed word token appears at the end of an utterance or not. On the \\(y\\) axis we see the word duration. The line in the middle of a box plot indicates the median value within the relevant category. So, for instance, the line in the middle of the left box indicates the median word duration in seconds for word tokens which do not appear at the end of an utterance.\nFigure 4.8 suggests that words which appear at the end of an utterance as slightly longer than those which appear at the start of an utterance.\nIn order to include this variation in a linear model, we simply add the variable to the formula using the + sign, as follows:\n\nmultiple_fit &lt;- lm(\n  WordDuration ~ unigram.google.gb + final,\n  data = big_dia\n)\n\n\nCarry out the graphical model checking steps we introduced for simple linear regression above with the multiple_fit model.\n\nWhat does this look like in the model summary? We will look at the coefficients only. To get at the coefficients table in a convinent way, we can use the tidy() function from the tidymodels package.\n\ntidy(multiple_fit)\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         0.0428  0.00144       29.8 2.34e-194\n2 unigram.google.gb  -0.0276  0.000168    -164.  0        \n3 finalTRUE           0.0295  0.00115       25.7 5.75e-146\n\n\nThe most obvious difference is that we now have a row in our coefficients table called finalTRUE. But to understand how final is included in the model, we need to look at the intercept value again. Categorical variables are represented as having a ‘base’ level.8 In this case, FALSE is the base level. This means that the intercept value for this model is the intercept value when the word is not at the end of an utterance. The estimate of the coefficient finalTRUE tells us how much longer a word is when it appears at the end of an utterance. In this case, the estimate is \\(0.03\\). So, on the whole, words at the end of utterances are estimated to be \\(0.3\\) seconds longer than words which are not at the end.\nAs in the previous case, we are given \\(p\\)-values for each of these estimates. The very low \\(p\\)-values in this case indicate that the data at hand is not compatible with these estimates being zero. That is, there is a ‘statistically significant’ associated with each of these coefficients.\nWe now want to plot the model predictions. The following code extracts model predictions for some specified points, rather that for every data point in the data frame. We use crossing() from the tidyr package (loaded as part of the tidyverse) to say we want every combination of the values we list. We then feed the result of this to the predict() function using the newdata argument.\n\n# specify the values we want predictions for.\nto_predict &lt;- crossing(\n  \"unigram.google.gb\" = c(-15, -10, -8, -7),\n  \"final\" = c(TRUE, FALSE)\n)\n\n# get predictions using `predict()`.\nmultiple_preds &lt;- predict(\n  multiple_fit, \n  newdata = to_predict,\n  interval = \"confidence\" # The interval argument says we want a confidence\n  # interval for each prediction too.\n)\n\n# combine predictions with the values we specified.\nmultiple_preds &lt;- bind_cols(to_predict, multiple_preds)\n\n\nExperiment with some ways to plot these model predictions. These should be in a variable called multiple_preds.\n\n\n\n4.3.2 Adding a Continuous Variable\nWe don’t have any sense yet of how long we would expect a given word to be. The variable dur.context gives a sense of the length expected given the context of the word in synthesised speech. Here’s what the variable looks like in a scatter plot with word duration:\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = dur.context,\n      y = WordDuration\n    )\n  ) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nFigure 4.9: Plot of word duration given expected duration (dur.context).\n\n\n\n\n\nIf you want to see what a simple linear regression looks like on a scatter plot, it can be convenient to use geom_smooth(method = \"lm\"), as follows:\n\nbig_dia |&gt; \n  ggplot(\n    aes(\n      x = dur.context,\n      y = WordDuration\n    )\n  ) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nFigure 4.10: Plot of word duration given expected duration (dur.context) with simple linear regression.\n\n\n\n\n\nThe measure of expected duration is positively related to the actual measures of duration. This is an example of a simple visual check that we are on the right track.\nLet’s add this measure to our model. Again, we just use a +. We will reuse the variable name multiple_fit.\n\nmultiple_fit &lt;- lm(\n  # I will stop explicitly saying 'formula =' now.\n  WordDuration ~ unigram.google.gb + dur.context + final,\n  data = big_dia\n)\n\nAre the diagnostic plots any better?\n\nplot(multiple_fit, which = 2)\n\n\n\n\n\n\n\n\nThis is better than before, but could still be improved. The addition of a variable capturing contextual factors in word duration has helped.\n\nsummary(multiple_fit)\n\n\nCall:\nlm(formula = WordDuration ~ unigram.google.gb + dur.context + \n    final, data = big_dia)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66112 -0.07115 -0.01392  0.05709  0.98263 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -9.610e-03  1.272e-03  -7.555  4.2e-14 ***\nunigram.google.gb -1.022e-02  1.587e-04 -64.406  &lt; 2e-16 ***\ndur.context        6.191e-04  2.146e-06 288.461  &lt; 2e-16 ***\nfinalTRUE         -7.348e-02  1.064e-03 -69.087  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.103 on 271760 degrees of freedom\nMultiple R-squared:  0.3057,    Adjusted R-squared:  0.3057 \nF-statistic: 3.989e+04 on 3 and 271760 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/linear_models.html#sec-interactions",
    "href": "chapters/linear_models.html#sec-interactions",
    "title": "4  Linear Models",
    "section": "4.4 Interactions",
    "text": "4.4 Interactions\n\ninteraction_fit &lt;- lm(\n  # I will stop explicitly saying 'formula' now.\n  WordDuration ~ unigram.google.gb * final + dur.context,\n  data = big_dia\n)\n\nsummary(interaction_fit)\n\n\nCall:\nlm(formula = WordDuration ~ unigram.google.gb * final + dur.context, \n    data = big_dia)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66253 -0.07102 -0.01401  0.05705  0.99390 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 -1.225e-02  1.304e-03  -9.394  &lt; 2e-16 ***\nunigram.google.gb           -1.051e-02  1.619e-04 -64.949  &lt; 2e-16 ***\nfinalTRUE                   -2.032e-02  5.925e-03  -3.429 0.000606 ***\ndur.context                  6.196e-04  2.147e-06 288.647  &lt; 2e-16 ***\nunigram.google.gb:finalTRUE  6.158e-03  6.751e-04   9.122  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.103 on 271759 degrees of freedom\nMultiple R-squared:  0.3059,    Adjusted R-squared:  0.3059 \nF-statistic: 2.994e+04 on 4 and 271759 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n4.4.1 Something something scaling",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/linear_models.html#whats-missing",
    "href": "chapters/linear_models.html#whats-missing",
    "title": "4  Linear Models",
    "section": "4.5 What’s Missing?",
    "text": "4.5 What’s Missing?\n\nThere are a series of assumptions that are made when building a mathematical model. These are the assumptions which come along with the mathematical machinery used to back up the models. These are always violated in the real world. Some statistics texts become obsessed with checking assumptions. This can become excessive. In cases with multiple continuous variables, it is worth checking for colinearity because it can lead to very unstable coefficients. In so far as these coefficients tell us how big an effect is, unstable coefficients have a significant impact on the scientific conclusions we draw from a model. But the primary thing to check is for normality of the residuals, as we did in the diagnostic plots above.\nThe primary assumption of linear regression which we have considered above is normality of the residuals. We want the variation which is left behind by our model to have no obvious structure. But these is another very important assumption of linear regression models: that each data point is independent. A simple way to think of this, is that every data point provides just as much information to the model as any other. But this is not true in many cases.\nWhen using corpora or carrying out experiments, we take multiple measurements from individual listeners or speakers. Each of these listeners or speakers has idiosyncratic features which influence the measurements we take from them. There is a dependence between measurements taken from the same individual.\nThere are mathematical tools to handle this problem: mixed effects models. We will look at these in the next chapter.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/linear_models.html#sec-further",
    "href": "chapters/linear_models.html#sec-further",
    "title": "4  Linear Models",
    "section": "4.6 Futher Resources",
    "text": "4.6 Futher Resources\nFor linear regression with linguistic data, see chapters 4-8 in (Winter 2019). For interpretation of p-values and confidence intervals, etc, see chapters 9-11.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSóskuthy, Márton, and Jennifer Hay. 2017. “Changing Word Usage Predicts Changing Word Durations in New Zealand English.” Cognition 166 (September): 298–313. https://doi.org/10.1016/j.cognition.2017.05.032.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/linear_models.html#footnotes",
    "href": "chapters/linear_models.html#footnotes",
    "title": "4  Linear Models",
    "section": "",
    "text": "There are many different terms used for the variables used in regression modelling. These reflect different motivations for fitting a model and differences between disciplines. For instance, in some cases you might say ‘predictor’ instead of ‘explanatory variable’. You will pick up the terminology over time.↩︎\nIn R, there are a series of functions called ‘generics’, which behave differently for different input. To see the documentation for plot() when applied to a linear model, use ?plot.lm in the console. Note that the result of this is different from entering ?plot.↩︎\nWe will continue to improve our model as we go on in these workshops.↩︎\nThe summary() function is also a generic function (just like plot()). Have a look at ?summary.lm().↩︎\nYes, you guessed it, this is also a generic function.↩︎\n We will also have to discuss what happens when multiple variables are correlated with one another (collinearity) in a later session. Adding multiple variables which track pretty much the same information (e.g., multiple closely related measures of how fast someone is speaking or or their reaction time in a psychological experiment) can lead to instability in the model. This is not a problem for predicting the response variable, but it is a problem when we try to explain the variation in the response variable.↩︎\nThis is a simplification and does not yet tell you much about what kind of variables to avoid. This will come later.↩︎\nThis can be changed, but we won’t look at this in these sessions.↩︎",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/mixed_effects.html",
    "href": "chapters/mixed_effects.html",
    "title": "5  Mixed Effects",
    "section": "",
    "text": "5.1 Exercises\nI assume you are using the project from the previous chapters. If you don’t have this, start from the ‘Exploratory Data Visualisation’ chapter.\nStart a new script or markdown file and load the following libraries. Install them if they are not available to you using install.packages().\nlibrary(tidyverse)\n\nlibrary(here)\n\n# To fit mixed effects models\nlibrary(lme4)\nlibrary(lattice) # required for some plots in lme4.\n\n# This package contains functions to simulate Simpson's paradox data.\nlibrary(bayestestR)\n\n# set the ggplot theme. You might try some alternatives here.\n# The documentation which appears if you type ?theme_bw will also show the\n# other possible prebuilt themes.\ntheme_set(theme_bw())\nAs before, load the data with:\nbig_dia &lt;- read_csv(here('data', 'big_dia.csv'))",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed Effects</span>"
    ]
  },
  {
    "objectID": "chapters/mixed_effects.html#exercises",
    "href": "chapters/mixed_effects.html#exercises",
    "title": "5  Mixed Effects",
    "section": "",
    "text": "5.1.1 Simpson’s Paradox\nWe looked at ‘Simpson’s paradox’ in the workshop presentation. This paradox can be used to motivate both control variables and random effects. We’re looking from the perspective of random effects.\nFirst simulate some data. You can change n or groups as you like.\n\nsimpson_data &lt;- simulate_simpson(n = 100, groups = 8)\n\nHere is a simple plot of the data with a simple linear regression using geom_smooth().\n\nsimpson_data |&gt; \n  ggplot(\n    aes(\n      x = V1,\n      y = V2\n    )\n  ) +\n  geom_point() + \n  labs(\n    V1 = \"explanatory variable\",\n    V2 = \"response\"\n  ) +\n  geom_smooth(method=\"lm\", se=FALSE, linewidth=3)\n\n\n\n\n\n\n\n\n\nModify the plot so each group is coloured differently and has its own linear trend line.\n\nThe plot you have made, if you succeeded in the exercise, shows multiple independent regressions (one for each group). This is not the same as mixed effects, which can pool information together across the different groups.\n\n\nFit a simple linear regression model predicting V2 by V1 using lm(). Plot predictions for this model using the techniques introduced in the previous session. What is wrong with the simple linear regression?\nComplete the following code block to fit a model of the Simpson’s paradox data with random intercepts for each group.\n\n```{r}\nsimpson_fit &lt;- lmer(\n  formula = V2 ~ V1 + (1|???),\n  data = ???\n)\n```\nRun the code.\n\n\n\nClick here for the answer to question 2.\nsimpson_fit &lt;- lmer(\n  formula = V2 ~ V1 + (1|Group),\n  data = simpson_data\n)\n\n\nWe can generate predictions from this model for each line in the data.\n\nComplete the following code block to generate predictions for each group using the predict() function\n```{r}\nsimpson_data &lt;- simpson_data |&gt; \n  mutate(\n    re_predictions = ???\n  )\n```\nRun the code.\n\n\n\nClick here for the answer\nsimpson_data &lt;- simpson_data |&gt; \n  mutate(\n    re_predictions = predict(simpson_fit)\n  )\n\n\nWe now plot these predictions:\n\nsimpson_data |&gt; \n  ggplot(\n    aes(\n      x = V1, \n      y = V2,\n      colour = Group\n    )\n  ) +\n  geom_point() +\n  # override part of the aesthetic mapping to get predictions into `geom_path`\n  geom_path(\n    aes(y = re_predictions)\n  )\n\n\n\n\n\n\n\n\nLetting the model know about the grouping structure, via random intercepts, is enough to fix the problem with the simple linear regression. If you are interested, compare this to a multiple linear regression with Group as fixed effect.\n\n\n5.1.2 Word duration and frequency\nComplex mixed effects models benefit from scaling and/or centring variables. This is true both for humans and for the computer. For us, it can help with interpretation of models. For the machine, it can help with the mathematics of fitting the model.\nBoth use the scale() function.\n\nLook at the documentation for scale().\n\nWork out how to scale only, centre only, and do both using the function. What is the default behaviour?\nCreate two new variables in big_dia:\n\ncentred_frequency, which centres unigram.google.gb.\nscaled_duration, which scales and centres WordDuration\n\nFit a model with scaled_duration as the response, centred_frequency and final as fixed effects, and random intercepts for Speaker and TargetOrthography (i.e. the word).\nLook at the summary and plot the overall model prediction. Note that you can generate predictions for each Speaker and ignoring Speaker by using the re.form argument to predict(). Say re.form = NA to ignore groups.\n\n\n\n\nClick here for the answer to (2)\nbig_dia &lt;- big_dia |&gt; \n  mutate(\n    centred_frequency = scale(\n      unigram.google.gb, \n      scale=FALSE\n    ),\n  scaled_duration = scale(WordDuration)\n  )\n\n\n\n\nClick here for the answer to (3)\nintercept_fit &lt;- lmer(\n  scaled_duration ~ centred_frequency + final +\n    (1|Speaker) + (1|TargetOrthography),\n  data = big_dia\n)\n\n\n\nWe can also add random slopes. Recall that (1 + variable|group) fits a random intercept and slope for each group and estimates the correlation of the intercepts and slopes.\n\nExtend the model you fit in (3) to include a random slope on centred_frequency. Explore this model using the tools we have considered already.\n\n\n\n\nClick here for the answer to (5)\nslope_fit &lt;- lmer(\n  scaled_duration ~ centred_frequency + final +\n    (1 + centred_frequency|Speaker) + (1|TargetOrthography),\n  data = big_dia\n)\n\n\n\nHow do we test the significance of centred frequency? We discussed using the anova() function for model comparison.\n\nFit a model which matched the model in (5), but doesn’t include centred_frequency anywhere. Use the argument REML = FALSE in lmer(). Compare this model with the model in (5) using the anova() function, with argument test = 'Chisq'. Does this method indicate a statistically significant effect of centred_frequency? What is the \\(p\\)-value?\n\nWe can extracting random effects terms from our model using the ranef() function.\n\nre_slope &lt;- ranef(slope_fit)\n\n\nLook at the structure of this object in the RStudio Environment pane (top right)\nMake a histogram of the random intercepts of TargetOrthography. Does this look normally distributed to you?\nCreate a scatter plot of speaker intercepts and slopes for Speaker. Are these values correlated? What might this suggest about the effect of frequency on duration?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed Effects</span>"
    ]
  },
  {
    "objectID": "chapters/rstudio_server.html",
    "href": "chapters/rstudio_server.html",
    "title": "6  RStudio Server",
    "section": "",
    "text": "Academics at the University of Canterbury can use the RStudio Server.\nAs of this writing writing… [details]\nCurrent details",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>RStudio Server</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html",
    "href": "chapters/qualtrics.html",
    "title": "7  Loading Data from Qualtrics",
    "section": "",
    "text": "7.1 Set up an R Project\nTo interact with data from Qualtrics, we will use the package qualtRics.\nIf you haven’t installed the package, run install.packages('qualtRics') in the R console.\nTo set up a new R project:\nWe will look at how to load data from .csv files generated by Qualtrics.\nPlace the .csv file you want to use in the data directory. For this exercise we will use this csv.\nCreate an R script and save it in the scripts directory.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#set-up-an-r-project",
    "href": "chapters/qualtrics.html#set-up-an-r-project",
    "title": "7  Loading Data from Qualtrics",
    "section": "",
    "text": "Go to File &gt; New Project\nSelect ‘New Directory’ and then ‘New Project’\nUse the browse choose a directory for the project (you might have to create a new one). For instance, Documents/linguistics_projects/ then enter a name for the project directory in the Directory name box.\nPress Create Project.\nCreate two directories in the project directory:\n\ndata\nscripts",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#load-useful-libraries",
    "href": "chapters/qualtrics.html#load-useful-libraries",
    "title": "7  Loading Data from Qualtrics",
    "section": "7.2 Load Useful Libraries",
    "text": "7.2 Load Useful Libraries\nStart the script with the following lines:\n\nlibrary(tidyverse)\nlibrary(here)\n\nlibrary(ggcorrplot)\n\n# The following code changes the ggplot theme. You may like to explore\n# alternative themes.\ntheme_set(theme_bw())\n\nIf R says that a package is not found, install it using install.packages(). Simply enter the name of the package inside quotation marks inside the brackets. For instant, if you don’t have the ggcorrplot package, enter the following in the R console (the bottom pane in RStudio): install.packages('ggcorrplot')",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#load-data-exported-from-qualtrics-with-read_survey",
    "href": "chapters/qualtrics.html#load-data-exported-from-qualtrics-with-read_survey",
    "title": "7  Loading Data from Qualtrics",
    "section": "7.3 Load Data Exported from Qualtrics with read_survey()",
    "text": "7.3 Load Data Exported from Qualtrics with read_survey()\nWe can now use the read_survey function from the qualtRics package to read in the data.\n\n# load the qualtRics package\nlibrary(qualtRics)\n\n# Read in the data\nsurvey &lt;- read_survey(here('data', 'Ling310-Week1-2022.csv')) \n\nRunning the above line of code will both read in the data (giving it the name survey in R) and provide output which tells us how the columns have been interpreted. The output shows that the default is to assume that each column has text in it rather than numbers (see .default = col_character() in the output). We also see that the Progress, Duration (in seconds), and latitude and longitude columns contain numbers (see col_double() in the output — ‘double’ refers to a way in which computer represent numbers which can contain a decimal point). Finally, there are a set of columns with the type col_logical(). These columns are ‘logical’ in the sense that they contain only TRUE or FALSE. Any of these columns can have NA, which indicates that data is missing or unavailable.\nYou may be confused now! Why would a column called RecipientLastName, for instance, only have the values TRUE or FALSE? Shouldn’t it have… the last names of recipients? There must be something wrong here. Let’s figure it out.\nIt is worth having a look at some rows from the columns we are worried about. There are only 17 rows in the data frame so we can easily look at all of the values in the confusing columns.1 Here is some tidyverse style code to do that:\n\nsurvey |&gt;\n  # Select the 'logical' columns which are confusing.\n  select(\n    RecipientLastName, \n    RecipientFirstName, \n    RecipientEmail, \n    ExternalReference\n  )\n\n# A tibble: 17 × 4\n   RecipientLastName RecipientFirstName RecipientEmail ExternalReference\n   &lt;lgl&gt;             &lt;lgl&gt;              &lt;lgl&gt;          &lt;lgl&gt;            \n 1 NA                NA                 NA             NA               \n 2 NA                NA                 NA             NA               \n 3 NA                NA                 NA             NA               \n 4 NA                NA                 NA             NA               \n 5 NA                NA                 NA             NA               \n 6 NA                NA                 NA             NA               \n 7 NA                NA                 NA             NA               \n 8 NA                NA                 NA             NA               \n 9 NA                NA                 NA             NA               \n10 NA                NA                 NA             NA               \n11 NA                NA                 NA             NA               \n12 NA                NA                 NA             NA               \n13 NA                NA                 NA             NA               \n14 NA                NA                 NA             NA               \n15 NA                NA                 NA             NA               \n16 NA                NA                 NA             NA               \n17 NA                NA                 NA             NA               \n\n\nAll 17 rows have NA values. That is, there is no data in these columns. We could just ignore these columns or we can get rid of them. Either option is fine. Here’s one way to get rid of them, using select() again, but this time with a minus sign (-) to indicate that we don’t want the named columns.\n\nsurvey &lt;- survey |&gt;\n  select(\n    -RecipientLastName, \n    -RecipientFirstName, \n    -RecipientEmail, \n    -ExternalReference\n  )\n\nWhat is actually in this data? Let’s look at the first few entries. If you simply enter survey we see the following:\n\nsurvey\n\n# A tibble: 17 × 36\n   StartDate       EndDate Progress Duration (in seconds…¹ Finished RecordedDate\n   &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;                  &lt;dbl&gt; &lt;lgl&gt;    &lt;chr&gt;       \n 1 18/07/2022 1:27 18/07/…      100                    339 TRUE     18/07/2022 …\n 2 19/07/2022 0:13 19/07/…      100                    570 TRUE     19/07/2022 …\n 3 19/07/2022 0:35 19/07/…      100                    251 TRUE     19/07/2022 …\n 4 19/07/2022 20:… 19/07/…      100                    277 TRUE     19/07/2022 …\n 5 19/07/2022 21:… 19/07/…      100                    265 TRUE     19/07/2022 …\n 6 20/07/2022 20:… 20/07/…      100                    343 TRUE     20/07/2022 …\n 7 20/07/2022 21:… 20/07/…      100                    390 TRUE     20/07/2022 …\n 8 20/07/2022 21:… 20/07/…      100                    328 TRUE     20/07/2022 …\n 9 21/07/2022 1:58 21/07/…      100                    213 TRUE     21/07/2022 …\n10 21/07/2022 17:… 21/07/…      100                    177 TRUE     21/07/2022 …\n11 21/07/2022 19:… 21/07/…      100                    497 TRUE     21/07/2022 …\n12 21/07/2022 19:… 21/07/…      100                    213 TRUE     21/07/2022 …\n13 21/07/2022 20:… 21/07/…      100                    203 TRUE     21/07/2022 …\n14 22/07/2022 22:… 22/07/…      100                    280 TRUE     22/07/2022 …\n15 24/07/2022 21:… 24/07/…      100                    639 TRUE     24/07/2022 …\n16 25/07/2022 19:… 25/07/…      100                     91 TRUE     25/07/2022 …\n17 28/07/2022 20:… 28/07/…      100                    159 TRUE     28/07/2022 …\n# ℹ abbreviated name: ¹​`Duration (in seconds)`\n# ℹ 30 more variables: ResponseId &lt;chr&gt;, LocationLatitude &lt;dbl&gt;,\n#   LocationLongitude &lt;dbl&gt;, DistributionChannel &lt;chr&gt;, UserLanguage &lt;chr&gt;,\n#   Q3 &lt;chr&gt;, Q4 &lt;chr&gt;, Q5 &lt;chr&gt;, Q6 &lt;chr&gt;, Q8 &lt;chr&gt;, Q9 &lt;chr&gt;, Q10 &lt;chr&gt;,\n#   Q11 &lt;chr&gt;, Q12 &lt;chr&gt;, Q13 &lt;chr&gt;, Q14 &lt;chr&gt;, Q15 &lt;chr&gt;, Q16 &lt;chr&gt;,\n#   Q17 &lt;chr&gt;, Q18 &lt;chr&gt;, Q19 &lt;chr&gt;, Q20 &lt;chr&gt;, Q21 &lt;chr&gt;, Q22 &lt;chr&gt;,\n#   Q23 &lt;chr&gt;, Q24 &lt;chr&gt;, Q25 &lt;chr&gt;, Q26 &lt;chr&gt;, Q27 &lt;chr&gt;, Q28 &lt;chr&gt;\n\n\nThis doesn’t tell us much, because there are so many columns. How many? Look at the top left of the output: there are 17 rows and 40 columns (17 x 40). We see when the participant started and ended the survey, how far through they got (Progress), how long it took (`Duration (in seconds)`), and whether the participant finished (Finished).\nHave a look at the full data set in RStudio’s viewer by either clicking on the name survey in the environment pane (top right) or entering View(survey) in the console. There are a series of columns, one for each question. When we view in the RStudio viewer we see the text of survey question as ‘labels’ underneath the variable names (e.g. the variable named Q23 has the label ‘Does “pool” rhyme with “food”?’).\nThe other way to get at these labels is to use the function sjlabelled::get_label(). This will output the text label for each variable. The bit of code [15:20] tells R just to print the 30th label through to the 35th. You can change these numbers to see more labels or remove them entirely to see all of the labels.\n\nsjlabelled::get_label(survey)[30:35]\n\n                                                                                      Q22 \n                                                        \"Does 'pear' rhyme with 'share'?\" \n                                                                                      Q23 \n                                                     \"Does \\\"pool\\\" rhyme with \\\"food\\\"?\" \n                                                                                      Q24 \n                                             \"Are 'doll' and 'dole' pronounced the same?\" \n                                                                                      Q25 \n                \"Do you think Aucklanders sound different from people from Christchurch?\" \n                                                                                      Q26 \n\"Do you think you could guess whether someone is Māori or Pākeha from listening to them?\" \n                                                                                      Q27 \n              \"Would it be easier to guess someone's age or job from the way they sound?\" \n\n\nYou may need to scroll right to see the full question and the variable names in the output.\nThe bit of code sjlabelled:: means that we are looking for a name which exists within the package sjlabelled. You can avoid having to enter this in by loading the library sjlabelled at the start of your script. This is entirely up to you.\n\nWhat is the label of the variable named Q25?\n\n Does 'pear' rhyme with 'share'? Do you think Aucklanders sound different from people from Christchurch? It doesn't have a label.\n\n\nLet’s have a look at the actual answers for Q24:\n\nsurvey$Q24\n\n [1] \"No\"  \"Yes\" \"No\"  \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"No\" \n[13] \"No\"  \"Yes\" \"No\"  \"Yes\" \"Yes\"\nattr(,\"label\")\n                                         Q24 \n\"Are 'doll' and 'dole' pronounced the same?\" \n\n\nThe responses are stored as character strings. Most respondents seem to think that ‘doll’ and ‘dole’ are pronounced the same.\nWe don’t have to count these manually. Use the following code to see the counts of each answer:\n\nsummary(factor(survey$Q24))\n\n No Yes \n  7  10 \n\n\nThere are ten respondents who think ‘Aucklanders and people from Christchurch sound the same and four who don’t ’doll’ and ‘dole’ are pronounced the same.\n\nWhat do you think the function factor is doing here? See what happens if you remove it and have a look here to understand what is going on in more detail: https://r4ds.hadley.nz/factors.html",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#a-bar-chart",
    "href": "chapters/qualtrics.html#a-bar-chart",
    "title": "7  Loading Data from Qualtrics",
    "section": "7.4 A Bar Chart",
    "text": "7.4 A Bar Chart\nLet’s create some plots in ggplot using the data we have loaded from Qualtrics.\n\nsurvey |&gt; \n  ggplot(\n    aes(\n      x = Q28\n    )\n  ) +\n  geom_bar() +\n  labs(\n    title = \"Q28\",\n    subtitle = \"What does the word 'worry' rhyme with?\"\n  )",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#factor-associations",
    "href": "chapters/qualtrics.html#factor-associations",
    "title": "7  Loading Data from Qualtrics",
    "section": "7.5 Factor Associations",
    "text": "7.5 Factor Associations\nWe are often interested in associations between answers to distinct questions. The table function can be very useful as an initial was of seeing differences. Table is not a tidyverse function, and so we have to use the $ to indicate column names.\n\ntable(\n  survey$Q8,\n  survey$Q10\n)\n\n                \n                 No Yes\n  Definitely      5   2\n  Definitely not  0   3\n  Maybe?          3   4\n\n\n\nFigure out what the questions associated with the columns Q8 and Q10 are. Enough information has been given above to work this out.\n\nAre we surprised by the absence of ‘no’ answers in the ‘definitely not’ group? We can do a simple test to check this using a Chi-square test.\n\nsurvey_test &lt;- chisq.test(\n  survey$Q8,\n  survey$Q10\n)\n  \nsurvey_test\n\n\n    Pearson's Chi-squared test\n\ndata:  survey$Q8 and survey$Q10\nX-squared = 4.3849, df = 2, p-value = 0.1116\n\n\nHere, a value is calculated which compares the counts in the table above that we would expect given no association between the answers to the two questions, and what we actually observe. This value is called the \\(\\chi^2\\) value (or ‘Chi-square’). The consistency of our data with the assumption that there is no relationship is captured by the \\(\\chi^2\\) value (and the degrees of freedom, but you can ignore this for now). If the p-value is below a pre-established limit, typically \\(0.05\\), we say that the association between the two questions is ‘statistically significant’. In this case, the association is not statistically significant.\nThe assumption that there is no association between the questions is called the ‘null hypothesis’.\n\n\n\n\n\n\nWarning\n\n\n\nIt takes some effort to fully understand the meaning of p-values, and of the phrase ‘statistically significant’.\nFor one thing, ‘statistical significance’ does not mean significant in the sense of ‘big’. Some ‘statistically significant’ effects are so small that they have no practical importance.\nThis is a bigger topic which we will not cover here!\n\n\nWe can see what values were expected given no association between the questions here:\n\nsurvey_test$expected\n\n                survey$Q10\nsurvey$Q8              No      Yes\n  Definitely     3.294118 3.705882\n  Definitely not 1.411765 1.588235\n  Maybe?         3.294118 3.705882\n\n\nFailure to find a statistically significant effect does not mean that there is no effect. For instance, even if there is an effect we may not have enough data points to detect it.\nIt is also important to note that an association between two questions can appear as statistically significant even if it does not exist. The p-value indicates the probability that we would see a \\(\\chi^2\\) value this big (or bigger) in the absence of a genuine association between the answers to the two questions. Any data is consistent with the absence of an association. Perhaps we were just unlucky in our observations.\nTo report a \\(\\chi^2\\) test, you can say something like: &gt; We carried out a Chi-square test on questions 8 and 10 and failed to reject the null hypothesis (Chi = 4.4, df = 2, p = 0.1).\nOr, if the association were statistically significant: &gt; We found a statistically significant association between questions x and y (Chi = ???, df = ???, p = ???).\nWith the appropriate values inserted.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#wide-and-long-data",
    "href": "chapters/qualtrics.html#wide-and-long-data",
    "title": "7  Loading Data from Qualtrics",
    "section": "7.6 Wide and Long Data",
    "text": "7.6 Wide and Long Data\nOne of the most common things to do with survey data is to convert from our current situation, where we have a column for each question to one in which we use two columns: one to identify questions and one to store responses.\nThat is, we want to move from a wider data frame to a longer data frame. A longer data frame as fewer columns and more rows.\nWe achieve this using the pivot_longer() function. It is often useful to assign the longer version of the data frame to a new variable.\nLet’s import another Qualtrics survey for which this is necessary:\n\nskills &lt;- read_survey(here('data', 'ARTS102-S2-2022.csv'))\n\n\nHave a look at this survey using the skills you have picked up above.\n\nHere is a long, but easier to explain, version of the code which makes this data frame longer:\n\nskills_longer &lt;- skills |&gt; \n  pivot_longer(\n    # First we name the columns we want to convert into longer formant.\n    cols = c(\n      Q18_1, Q18_2, Q18_3, Q18_4, Q18_5, Q18_6, Q18_7, Q18_8, Q18_9, \n      Q18_10, Q18_11, Q18_12, Q18_13, Q18_14\n    ),\n    # Where do we want the column names to go? This will be the name of our new\n    # column to identify the question.\n    names_to = \"question\",\n    # Where do we want the responses to go?\n    values_to = \"response\"\n  )\n\nLet’s look at some ways to make this code a little more convenient to write. There are lots of ways to specify the columns we are interested in rather than writing them all in by hand.\n\nWe can use a colon (:) to indicate a range of columns. This will use the order in which the columns appear in the data frame (the same as the order in the RStudio viewer or the order you get if you enter names(skills) into the console). So, in this case, we can use Q18_1:Q18_14:\n\n\nskills_longer &lt;- skills |&gt; \n  pivot_longer(\n    cols = Q18_1:Q18_14,\n    names_to = \"question\",\n    values_to = \"response\"\n  )\n\n\nYou can select columns using starts_with() or ends_with(), if all the columns you want have names starting with or ending with a given character. All the columns we want start with Q and none of the other columns start with Q, we we can use:\n\n\nsurvey_longer &lt;- survey |&gt; \n  pivot_longer(\n    cols = starts_with('Q'),\n    names_to = \"question\",\n    values_to = \"response\"\n  )\n\n\nWe can use contains() if we have a specific string which each column contains (in this case contains('Q') would work).\n\nThere are some other column selection options. If you want more detail, enter ?tidyselect::language into the console and look at the help file which appears.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#another-plot",
    "href": "chapters/qualtrics.html#another-plot",
    "title": "7  Loading Data from Qualtrics",
    "section": "7.7 Another Plot",
    "text": "7.7 Another Plot\nLet’s produce a nice violin plot, which will show the distribution of scores across multiple questions:\n\nskills_v_plot &lt;- skills_longer |&gt; \n  ggplot(\n    aes(\n      x = question, \n      y = response\n    )\n  ) +\n  geom_violin(draw_quantiles = c(0.5)) +\n  labs(\n    title = \"Response to skills questions in LING310 Survey\"\n  )\n  \nskills_v_plot\n\n\n\n\n\n\n\n\nIn the above plot, the bars give the median value for the question. So, for instance, for the first question, the median value is somewhere between \\(37.5\\) and \\(50\\), whereas the median for the fourteenth question is somewhere between \\(12.5\\) and \\(25\\).\nThe labels on the \\(x\\)-axis are not very clear. Let’s fix this! There’s no low-labour way to do this. The names are taken from the column names which we got when we read in the data. So we can modify the skills data frame and then pivot it into longer form again. I get the names by looking at the output of the sjlabelled::get_label() function again.2 We will change the names of the columns using the rename() function.\n\nskills &lt;- skills |&gt; \n  rename(\n    maths = Q18_1,\n    skipping = Q18_2,\n    knitting = Q18_3,\n    minecraft = Q18_4,\n    driving = Q18_5,\n    conversation = Q18_6,\n    handwriting = Q18_7,\n    maps = Q18_8,\n    programming = Q18_9,\n    drawing = Q18_10,\n    lego = Q18_11,\n    threading_needle = Q18_12,\n    cooking = Q18_13,\n    te_reo = Q18_14,\n  )\n\nWe pivot again:\n\nskills_longer &lt;- skills |&gt; \n  pivot_longer(\n    cols = maths:te_reo,\n    names_to = \"question\",\n    values_to = \"response\"\n  )\n\nAnd plot again. We add some lines to rotate the labels so they are readable:\n\nskills_v_plot &lt;- skills_longer |&gt; \n  ggplot(\n    aes(\n      x = question, \n      y = response\n    )\n  ) +\n  geom_violin(draw_quantiles = c(0.5)) +\n  labs(\n    title = \"Response to skills questions in LING310 Survey\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle=90, vjust = 0.8)\n  )\n  \nskills_v_plot\n\n\n\n\n\n\n\n\nThere are many ways this could be improved. Have a look for some ggplot2 tutorials online.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#continuous-associations",
    "href": "chapters/qualtrics.html#continuous-associations",
    "title": "7  Loading Data from Qualtrics",
    "section": "7.8 Continuous Associations",
    "text": "7.8 Continuous Associations\nCan we look at how these skills are related to one another? Yes!\nFirst, let’s look at a correlation plot:\n\nggcorrplot(\n  cor(skills |&gt; select(maths:te_reo))\n)\n\n\n\n\n\n\n\n\nThe red blocks indicate positive associations (when one goes up, the other does as well). There is only one negative correlation here, between Minecraft and handwriting. This suggests (weakly!) that increased Minecraft skill comes with weakened handwriting.\nOn the stronger end, it looks like there is a strong positive correlation between knitting skill and the ability to thread a needle and between reading maps and driving.\nWe can test all three associations using the cor.test() function. First: between Minecraft and handwriting:\n\ncor.test(\n  skills$minecraft,\n  skills$handwriting\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  skills$minecraft and skills$handwriting\nt = -1.9122, df = 293, p-value = 0.05683\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.222396789  0.003220217\nsample estimates:\n       cor \n-0.1110185 \n\n\nThe p-value here is just above 0.05. This is below the cutoff, so we do not say that the association is ‘statistically significant’.\nLet’s look at the association between threading a needle and knitting:\n\ncor.test(\n  skills$knitting,\n  skills$threading_needle\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  skills$knitting and skills$threading_needle\nt = 7.1109, df = 293, p-value = 8.847e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2817856 0.4769410\nsample estimates:\n      cor \n0.3836382 \n\n\nThis is a much stronger correlation (\\(0.383...\\)) and has a much lower p-value. It is below \\(0.05\\), so we can say that the association between the questions is statistically significant. For instance, by saying “the correlation between the skill of threading a needle and knitting is statistically significant at the 0.05 level (Pearson’s cor = \\(0.38\\), p-value &lt; \\(0.001\\)).”3\nWhat about the association between driving a car and reading maps:\n\ncor.test(\n  skills$driving,\n  skills$maps\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  skills$driving and skills$maps\nt = 6.1072, df = 293, p-value = 3.216e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2306929 0.4335966\nsample estimates:\n      cor \n0.3360379 \n\n\nThe story here is very similar.\n\n\n\n\n\n\nWarning\n\n\n\nThere is much more to be said about statistical testing. In particular, in actual research it is very bad practice to start with a plot of the strength of a series of correlations, pick the strongest ones, and then do a statistical test on them. This can result in many false associations. Rather, you should have a hypothesis about what associations will be significant before you look at the data.\n\n\nLet’s have a look at what the association between threading a needle and knitting looks like with one final plot:\n\nskills |&gt; \n  ggplot(\n    aes(\n      x = threading_needle,\n      y = knitting\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\nBecause our values have to be between \\(0\\) and \\(100\\), there are ‘floor’ and ‘ceiling’ effects here. That is, many of our values sit at either 0 or 100. Nonetheless, the correlation coefficient gives us a tool for expressing the association between these variables.\nThere is much more to say about how to analyse data of this sort. However, this is enough to get started with.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/qualtrics.html#footnotes",
    "href": "chapters/qualtrics.html#footnotes",
    "title": "7  Loading Data from Qualtrics",
    "section": "",
    "text": "You can see how many rows there are in a data frame by looking at the ‘environment’ pane at the top right of the R Studio window. You can also use the function nrow(). In this case, you could enter nrow(survey) into your script or the console pane in RStudio.↩︎\nYou could use any method you like to do this, including looking at the original spreadsheet in Excel.↩︎\nHere, because the p-value is so small, we simply report that it is less than a very small number \\(0.0001\\). It is good practice not to just say \\(&lt; 0.05\\).↩︎",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading Data from Qualtrics</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html",
    "href": "chapters/gamms_1.html",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "",
    "text": "8.1 Overview\nThis chapter is going to grow over four workshop sessions. The rough plan is:\nWe will be using the following libraries:\n# The usual suspects\nlibrary(tidyverse)\nlibrary(here)\n\n# GAMM-specific libraries\nlibrary(mgcv)\nlibrary(itsadug)\nlibrary(gratia)\n\n# Non-essential. Used in my `bam` to determine how many CPU cores to use.\nlibrary(parallel)\n\n# NZILBB vowel package\n# If you do not have this use the following lines of code:\n# install.packages('remotes')\n# remotes::install_github('nzilbb/nzilbb_vowels')\nlibrary(nzilbb.vowels)\n\n# Set ggplot theme\ntheme_set(theme_bw())\nThis workshop is heavily indebted to the workshops put together by Márton Sóskuthy and Martijn Wieling. Links to these are provided in Section 8.7.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html#overview",
    "href": "chapters/gamms_1.html#overview",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "",
    "text": "Introduction to the idea of GAMs and how to specify parametric and smooth terms.\nThe parenthetical ‘M’: we’ll add random effects, looking at random intercepts, slopes, and smooths.\nAuto-correlation and model comparison: we’ll consider various strategies for testing for and controlling autocorrelation and methods for comparing models.\nBy-factor smooths and reporting models: We’ll consider the varieties of by-factor smooths available and how to implement them, before turning to how to report GA(M)Ms in papers and across supplementary material.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html#introduction-to-gams",
    "href": "chapters/gamms_1.html#introduction-to-gams",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "8.2 Introduction to GAMs",
    "text": "8.2 Introduction to GAMs\n\n8.2.1 Why?\nStraight lines have a lot of advantages. They can be completely specified by two numbers: how steep they are (the slope) and where they intersect the \\(y\\)-axis (the intercept). Fitting a straight line through a collection of points is just a matter of finding the optimum slope and intercept.\nBut sometimes straight lines aren’t enough. There are plenty of effects in nature which do not follow a straight line. There are plenty of examples of trajectories with non-linear behaviour in the study of language. For instance, consider the following trajectory for the price vowel from ONZE via (Sóskuthy, Hay, and Brand 2019).1\n\n\nTo view the code click here\n# Source: https://osf.io/74mza\n\n# Load all data (we will use the full set later)\nprice &lt;- read_rds(here('data', 'price_anon.rds'))\n\n# The dataset will be explained in full below.\n# Pull out a single trajectory.\nprice &lt;- price |&gt; \n  filter(id == \"price_58\") |&gt; \n  pivot_longer(\n    cols = f1:f2,\n    names_to = \"formant_type\",\n    values_to = \"formant_value\"\n  )\n\n# Plot it\nprice_plot &lt;- price |&gt; \n  ggplot(\n    aes(\n      x = time,\n      y = formant_value,\n      colour = formant_type\n    )\n  ) +\n  geom_point() +\n  labs(\n    colour = \"Formant type\",\n    y = \"Frequency (Hz)\",\n    x = \"Time (s)\"\n  )\n\nprice_plot\n\n\n\n\n\nF1 and F2 of a PRICE vowel.\n\n\n\n\nIf we try to fit this trajectory with straight lines, we get:\n\n\nTo view the code click here\nprice_plot +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nF1 and F2 of a PRICE vowel with linear model.\n\n\n\n\nWhat we want instead, is a way to fit a non-linear relationship. GA(M)Ms provide one flexible way of doing this. A simple GAM for this trajectory looks like this:\n\n\nTo view the code click here\nprice_plot +\n  geom_smooth(method = \"gam\", se = FALSE)\n\n\n\n\n\nF1 and F2 of a PRICE vowel with GAM model.\n\n\n\n\nThe same comments apply to trajectories taken from, e.g., tongue sensors or derived from video footage. They also apply at very different time scales. Consider GAMMs fit through full monologues (Wilson Black et al. 2023), or to formant values across the history of a dialect (Brand et al. 2021).\n\n\n\n\n\n\nGAMMs and Polynomial Models\n\n\n\n\n\nYou may have used polynomial models in the past. These models add terms such as \\(x^2\\) and \\(x^3\\) to models (where previously, only \\(x\\) was included). You can do a lot with polynomial models, but they have some negative properties. For a brief and accessible explanation of the shortcomings of polynomial models compared to GAMMs see the motivating example section from the following Gavin Simpson video: https://youtu.be/Ukfvd8akfco?si=COncckzAvpqvfIfj&t=107. A key issue with polynomial models, especially when the \\(n\\) in \\(x^n\\) gets very large, is called Runge’s Phenomenon. At the edges of the range of the data, high order polynomials become very unstable.\nSóskuthy (2017) introduces polynomial models as, in effect, a simple kind of GAMM. This is different from thinking of GAMMs as an alternative to polynomial models. Both are defensible views, but you may get confused if you try to think them both at the same time!\n\n\n\n\n\n8.2.2 What?\nWe’ll start with a bit of mathematics. A linear model looks like this:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon \\] where the \\(\\beta\\)’s are the model coefficients, the \\(x\\)’s are the model predictors, and the \\(\\epsilon\\) is an error term. The \\(\\beta_0\\) term is the aforementioned intercept and the other \\(\\beta_n\\)’s are slopes. These slopes estimate the effect of the attached predictor. For any predictor, all we get is a single slope and the intercept — a straight line.\nGAMs replace each of the \\(\\beta_n x_n\\) terms with a function of \\(x_n\\). This function will usually be some kind of smooth. We’ll look at this visually in a moment. But, mathematically, the overall GAM model looks like this:\n\\[  y = \\beta_0 + f_1(x_1) + f_2(x_2) + \\ldots + f_n(x_n) + \\epsilon .\\] The parameters have been replaces by functions. Now, the nature of the relationship between a given predictor and \\(y\\) need not be a straight line.\nThe functions we choose attempt to balance smoothness and wiggliness. Wiggliness simply means deviation from a straight line. Smoothness indicates a lack of bumps. We are speaking intuitively, but this language is used by the mathematicians as well!.\nLet’s consider one class of smooth, and how it can be used to balance these two demands: splines. Here, a set of basis functions is summed together to create a smooth line through the data.\nThe basis functions for one common set of splines looks like this ( borrowing code from Gavin Simpson):\n\n\nTo view the code click here\n# Simulate 500 observations\nset.seed(1)\nN &lt;- 500\ndata &lt;- tibble(\n  x = runif(N),\n  ytrue = map_dbl(\n    x, \n    \\(x) {x^11 * (10 * (1 - x))^6 + ((10 * (10 * x)^3) * (1 - x)^10)}\n  ),\n  ycent = ytrue - mean(ytrue),\n  yobs  = ycent + rnorm(N, sd = 0.5)\n)\nk &lt;- 10\nknots &lt;- with(data, list(x = seq(min(x), max(x), length = k)))\nsm &lt;- smoothCon(s(x, k = k, bs = \"cr\"), data = data, knots = knots)[[1]]$X\ncolnames(sm) &lt;- levs &lt;- paste0(\"f\", seq_len(k))\nbasis &lt;- pivot_longer(cbind(sm, data), -(x:yobs), names_to = 'bf')\n\nbasis |&gt; \n  ggplot(\n    aes(\n      x = x, \n      y = value, \n      colour = bf\n    )\n  ) +\n  geom_line(lwd = 2, alpha = 0.5) +\n  guides(colour = FALSE) +\n  labs(x = 'x', y = 'b(x)')\n\n\n\n\n\n\n\n\nFigure 8.1: Cubic regression spline basis functions with 10 knots.\n\n\n\n\n\nEach distinct ‘basis function’ is in a different colour. We fit out actual data by multiplying the functions by appropriate coefficients (to change how high they are on the graph) and adding them together.\nThere are a number of knots in Figure 8.1. These are the points at which the functions a joined together (informally speaking) and at which we aim to ensure smoothness. For this set of basis functions, the knots are at (red points):\n\n\nTo view the code click here\n# Simulate 500 observations\nset.seed(1)\nN &lt;- 500\ndata &lt;- tibble(\n  x = runif(N),\n  ytrue = map_dbl(\n    x, \n    \\(x) {x^11 * (10 * (1 - x))^6 + ((10 * (10 * x)^3) * (1 - x)^10)}\n  ),\n  ycent = ytrue - mean(ytrue),\n  yobs  = ycent + rnorm(N, sd = 0.5)\n)\nk &lt;- 10\nknots &lt;- with(data, list(x = seq(min(x), max(x), length = k)))\nsm &lt;- smoothCon(s(x, k = k, bs = \"cr\"), data = data, knots = knots)[[1]]$X\ncolnames(sm) &lt;- levs &lt;- paste0(\"f\", seq_len(k))\nbasis &lt;- pivot_longer(cbind(sm, data), -(x:yobs), names_to = 'bf')\n\nbasis |&gt; \n  ggplot(\n    aes(\n      x = x, \n      y = value, \n      colour = bf\n    )\n  ) +\n  geom_line(lwd = 2, alpha = 0.5) +\n  geom_point(\n    inherit.aes = FALSE, \n    aes(\n      x = x\n    ),\n    y = 0,\n    colour = \"red\",\n    data = as_tibble(knots),\n    size = 5\n  ) +\n  guides(colour = FALSE) +\n  labs(x = 'x', y = 'b(x)')\n\n\n\n\n\n\n\n\nFigure 8.2: Cubic regression spline basis functions with 10 knots (red dots).\n\n\n\n\n\nAgain, borrowing code from Gavin Simpson, we can see what this looks like for our simulated data.\n\n\nTo view the code click here\nbeta &lt;- coef(lm(ycent ~ sm - 1, data = data))\nwtbasis &lt;- sweep(sm, 2L, beta, FUN = \"*\")\ncolnames(wtbasis) &lt;- colnames(sm) &lt;- paste0(\"F\", seq_len(k))\n## create stacked unweighted and weighted basis\nbasis &lt;- as_tibble(wtbasis) %&gt;%\n  mutate(\n    x = data$x,\n    spline_fit = pmap_dbl(\n      # Yikes, bad coding here by me (JWB)\n      list(F1, F2, F3, F4, F5, F6, F7, F8, F9, F10),\n      sum\n    )\n  ) \n\nbasis_long &lt;- basis |&gt; \n  pivot_longer(\n    cols = contains('F'),\n    values_to = \"value\",\n    names_to = \"bf\"\n  )\n\n\ndata |&gt; \n  ggplot(\n    aes(\n      x = x,\n      y = yobs\n    )\n  ) +\n  geom_point(alpha = 0.4) +\n  geom_line(\n    aes(\n      x = x,\n      y = value,\n      colour = bf\n    ),\n    data = basis_long,\n    inherit.aes = FALSE,\n    linewidth = 1\n  ) +\n  geom_line(\n    aes(\n      x = x,\n      y = spline_fit\n    ),\n    inherit.aes = FALSE,\n    colour = \"black\",\n    linewidth = 1,\n    data = basis\n  ) + \n  guides(colour = FALSE)\n\n\n\n\n\n\n\n\nFigure 8.3: Simulated data (black dots) with the basis functions after multiplication by their weights (colourful lines) and the sum of the basis functions (black line).\n\n\n\n\n\nSpend some time looking at Figure 8.3. Convince yourself that if you added together the colourful lines you would get the black line. The easiest way to do this is to work one point on the \\(x\\)-axis at a time. The case where \\(x=0\\) is the easiest, where the only colourful line is the red one and it is at the same point on the \\(y\\)-axis as the black line.\nWhat about this wiggliness and smoothness trade off? We’ve already seen one way in which wiggliness can be controlled: the number of knots sets an upper limit on how wiggly the resulting smooth function can be. If we only had 3 knots, this is what we would get:\n\n\nTo view the code click here\nk &lt;- 3\nknots &lt;- with(data, list(x = seq(min(x), max(x), length = k)))\nsm &lt;- smoothCon(s(x, k = k, bs = \"cr\"), data = data, knots = knots)[[1]]$X\ncolnames(sm) &lt;- levs &lt;- paste0(\"f\", seq_len(k))\nbeta &lt;- coef(lm(ycent ~ sm - 1, data = data))\nwtbasis &lt;- sweep(sm, 2L, beta, FUN = \"*\")\ncolnames(wtbasis) &lt;- colnames(sm) &lt;- paste0(\"F\", seq_len(k))\n## create stacked unweighted and weighted basis\nbasis &lt;- as_tibble(wtbasis) %&gt;%\n  mutate(\n    x = data$x,\n    spline_fit = pmap_dbl(\n      # Yikes, bad coding here by me (JWB)\n      list(F1, F2, F3),\n      sum\n    )\n  ) \n\nbasis_long &lt;- basis |&gt; \n  pivot_longer(\n    cols = contains('F'),\n    values_to = \"value\",\n    names_to = \"bf\"\n  )\n\ndata |&gt; \n  ggplot(\n    aes(\n      x = x,\n      y = yobs\n    )\n  ) +\n  geom_point(alpha = 0.4) +\n  geom_line(\n    aes(\n      x = x,\n      y = value,\n      colour = bf\n    ),\n    data = basis_long,\n    inherit.aes = FALSE,\n    linewidth = 1\n  ) +\n  geom_line(\n    aes(\n      x = x,\n      y = spline_fit\n    ),\n    inherit.aes = FALSE,\n    colour = \"black\",\n    linewidth = 1,\n    data = basis\n  ) + \n  guides(colour = FALSE)\n\n\n\n\n\n\n\n\nFigure 8.4: Simulated data (black dots) with the basis functions after multiplication by their weights (colourful lines) and the sum of the basis functions (black line).\n\n\n\n\n\nThe black line is our best possible fit to the data here, but it is no good. It needs to be wigglier.\nKnots are one determinant of wiggliness. But there is another: the smoothing parameter. This is used in order to penalise wiggliness when we fit a GAM model and is handled automatically by the mgcv package. In practice, it is determined from the data, rather than being manually specified. However, it is worth looking manually at what happens if we set the smoothing parameter too low and fail to sufficiently penalise wiggliness.\nHere’s what an excessively wiggly smooth function looks like with the New Zealand English dress vowel in ONZE:\n\n\nTo view the code click here\nonze_vowels_full |&gt; \n  lobanov_2() |&gt; \n  filter(\n    gender == \"F\",\n    vowel == \"DRESS\"\n  ) |&gt; \n  group_by(speaker) |&gt; \n  summarise(\n    F1_lob2 = mean(F1_lob2),\n    yob = first(yob)\n  ) |&gt; \n  ggplot(\n    aes(\n      x = yob,\n      y = F1_lob2\n    )\n  ) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(\n    method = \"gam\", \n    formula =  y ~ s(x, bs = \"cs\", k = 50, sp=0.01), \n    se=FALSE\n  )\n\n\n\n\n\n\n\n\nFigure 8.5: Mean normalised F1 for female speakers in ONZE by year of birth. Smoothing parameter set to 0.01.\n\n\n\n\n\nOn the other side, we can set the smoothing parameter too high. If we do, we’ll end up with a straight line.\n\n\nTo view the code click here\nmean_onze_full &lt;- onze_vowels_full |&gt; \n  lobanov_2() |&gt; \n  filter(\n    vowel == \"DRESS\"\n  ) |&gt; \n  group_by(speaker) |&gt; \n  summarise(\n    F1_lob2 = mean(F1_lob2),\n    yob = first(yob),\n    speech_rate = mean(speech_rate),\n    gender = first(gender)\n  )\n\nmean_onze_full |&gt; \n  filter(gender == \"F\") |&gt; \n  ggplot(\n    aes(\n      x = yob,\n      y = F1_lob2\n    )\n  ) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(\n    method = \"gam\", \n    formula =  y ~ s(x, bs = \"cs\", k = 50, sp=10000000), \n    se=FALSE\n  )\n\n\n\n\n\n\n\n\nFigure 8.6: Mean normalised F1 for female speakers in ONZE by year of birth. Smoothing parameter set to 10,000,000.\n\n\n\n\n\n\n\n\n\n\n\nThe Bias-Variance Trade Off\n\n\n\n\n\nWe are now in the broad area of a core data science concept: the ‘bias variance tradeoff’. That is, with any statistical learning method, we can introduce errors with the assumptions of our model (bias) and errors due to excessively following small fluctuations in our data (variance). But the less bias we include, the more we will be lead astray by noise and vice versa. It’s a tradeoff. In the case we are looking at now, reducing the smoothing parameter is equivalent to decreasing bias and increasing variance.\nSee the Wikipedia page.\n\n\n\n\n\n8.2.3 Fitting GAMs with mgcv\nHow do we specify GAMs with the mgcv package? Let’s start with model formulae.\nThe most obvious this is the construction of smooth terms. These use the s function. Let’s look at the ONZE data from Figure 8.5 and Figure 8.6. Here we want normalised first formant values to vary with year of birth in a non-linear way. We want a smooth on year of birth.\nWhat are the column names here?\n\nmean_onze_full\n\n# A tibble: 481 × 5\n   speaker  F1_lob2   yob speech_rate gender\n   &lt;fct&gt;      &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;fct&gt; \n 1 CC_f_007  -0.765  1982        5.76 F     \n 2 CC_f_010  -0.554  1947        5.06 F     \n 3 CC_f_020  -0.242  1936        5.04 F     \n 4 CC_f_024  -0.760  1973        5.61 F     \n 5 CC_f_025  -0.747  1949        4.63 F     \n 6 CC_f_027  -0.962  1981        5.22 F     \n 7 CC_f_033  -0.725  1953        4.82 F     \n 8 CC_f_040  -0.731  1955        4.24 F     \n 9 CC_f_051  -0.769  1955        4.67 F     \n10 CC_f_052  -0.838  1942        5.61 F     \n# ℹ 471 more rows\n\n\nThe variable we want to explain is F1_lob2. This contains normalised mean first formant values for each speaker. We want to explain it using `yob’, which we can see from the tibble output, is an integer. We’ll look at incorporating the other variables later.\nThe simplest version of the formula is this: F1_lob2 ~ s(yob). But this is usually bad practice — we should be more explicit! As a general principle, relying on defaults is dangerous as they can change under you, causing your code to have a different outcome.\nThe first area to be explicit is the knots. The default value for many smoothing splines is \\(10\\) and this is almost always fine. But we should think about it each time we fit a GAMM. So, an improvement: F1_lob2 ~ s(yob, k = 10).\nThe second argument to highlight is bs. This says what kind of basis functions we are using. The default, tp, or thin plate regression splines, are fine. Typically this choice won’t make a big different to you. But I will add more detail here soon. Regardless, it is good to make this explicit. So our final version of this formula: F1_lob2 ~ s(yob, k = 10, bs = \"tp\").\nWhat do we do with this formula? We will use the function bam to fit our first GAM.\n\n\n\n\n\n\nNote\n\n\n\nWe could just as easily use the gam function, but bam is optimised for large datasets.\n\n\n\n\n\n\n\n\nParallel Processing on macOS\n\n\n\n\n\nThe mgcv package requires the openmp library to take advantage of the parallel processing available through bam(). This is typically not a problem for Windows and Linux installations of R. On macos, it can be a problem.\nIf you get a warning about openmp not being available on macOS. I suggest you install the mandatory tools listed here: https://mac.r-project.org/tools/ and then follow the instructions at https://mac.r-project.org/openmp/.\nThis is not likely to be a pleasant experience, especially if you don’t like battling with the command line. UC and NZILBB researchers and students, feel free to message me on Rocket.Chat or via joshua.black@canterbury.ac.nz and I may be able to help.\n\n\n\n\nonze_fit &lt;- bam(\n  formula = F1_lob2 ~ s(yob, k = 10, bs = \"tp\"),\n  data = mean_onze_full\n)\n\nWe obtain a summary for this model using the summary function:\n\nsummary(onze_fit)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nF1_lob2 ~ s(yob, k = 10, bs = \"tp\")\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.590123   0.008102  -72.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n         edf Ref.df     F p-value    \ns(yob) 3.633  4.481 178.6  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.625   Deviance explained = 62.8%\nfREML = -138.75  Scale est. = 0.031571  n = 481\n\n\nThis summary has two primary sections. The Parametric coefficients, which indicate the non-smooth aspects of the model. In this case, the model fits an intercept term, which sets the overall height of the smooth function. The Approximate significance of smooth terms section indicates, as it says, the approximate significance of our smooths. This is the GAM equivalent of the coefficient for a variable in a linear model.\nIn this model, we only have s(yob) to look at. We see that it has an edf or ‘estimated degrees of freedom’ of 3.633. This is an indication of how wiggly the line is. If the esimated degrees of freedom are 1, it’s pretty much a straight line. We also see a p-value entry. This indicates whether the shape of the smooth is statistically significantly different from a flat line at the intercept value. In this case, unsurprisingly, it is distinct from a flat line.\nBut there are some problems here. First, we are merging male and female data together here. What if we want to fit a smooth for both male and female speakers? Here we can us the by argument to s() and add a parametric term for gender. This results in:\n\nonze_fit_gender &lt;- bam(\n  formula = F1_lob2 ~ gender + s(yob, by = gender, k = 10, bs = \"tp\"),\n  data = mean_onze_full\n)\n\nsummary(onze_fit_gender)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nF1_lob2 ~ gender + s(yob, by = gender, k = 10, bs = \"tp\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.64493    0.01146 -56.255  &lt; 2e-16 ***\ngenderM      0.10157    0.01565   6.492 2.13e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df      F p-value    \ns(yob):genderF 2.754  3.394  72.13  &lt;2e-16 ***\ns(yob):genderM 1.000  1.000 588.35  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.658   Deviance explained = 66.2%\nfREML = -156.83  Scale est. = 0.028773  n = 481\n\n\nNow we have two intercept terms, one for the female speakers and one for the male, both of which are significant. Just as in generalised linear models, the genderM parametric term gives a difference from the female intercept. This indicates that the first formant value is on average higher for male speakers. In the smooth terms section we now see s(yob):genderF and s(yob):genderM. We get independent p-values for each. What we do not get is a representation of the difference between the smooth for the female speakers and the smooth for the male speakers.2\nNote that the smooth for the male speakers in effectively a straight line. We will see this in a moment when we visualise.\nWe will add one more thing to this model before turning to diagnostics and plotting. What if we want two smooths? We know that speech rate can affect formant values. We can add this as an additional smooth term as follows:\n\nonze_fit_rate &lt;- bam(\n  formula = F1_lob2 ~ gender + \n    s(yob, by = gender, k = 10, bs = \"tp\") +\n    s(speech_rate, k = 10, bs = \"tp\"),\n  data = mean_onze_full\n)\n\nsummary(onze_fit_rate)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nF1_lob2 ~ gender + s(yob, by = gender, k = 10, bs = \"tp\") + s(speech_rate, \n    k = 10, bs = \"tp\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.64505    0.01148 -56.176  &lt; 2e-16 ***\ngenderM      0.10180    0.01568   6.492 2.14e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df       F p-value    \ns(yob):genderF 2.754  3.393  64.128  &lt;2e-16 ***\ns(yob):genderM 1.000  1.000 488.583  &lt;2e-16 ***\ns(speech_rate) 1.000  1.000   0.082   0.775    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.658   Deviance explained = 66.2%\nfREML = -153.06  Scale est. = 0.028828  n = 481\n\n\nIn this case we don’t see a significant difference as a result of speech rate. This may be because we are working with mean values for the formants. Once we can include random effects, and thus multiple values from a single speaker, this will change!\n\n\n\n\n\n\nTip\n\n\n\nWieling (2018) provides an example of building up a model from the ground up, exploring many different possible structures and including the R code. It is a good first port of call for looking at additional possible structures.\n\n\n\n\n8.2.4 Model diagnostics\nThe primary port of call for model diagnostics in mgcv is the gam.check() function. One of the outputs is a text output to help determine if \\(k\\) is too low.\n\ngam.check(onze_fit_rate)\n\n\nMethod: fREML   Optimizer: perf newton\nfull convergence after 15 iterations.\nGradient range [-1.027586e-06,1.027607e-07]\n(score -153.0627 & scale 0.02882816).\nHessian positive definite, eigenvalue range [2.671181e-07,238.0032].\nModel rank =  29 / 29 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                 k'  edf k-index p-value  \ns(yob):genderF 9.00 2.75    1.00   0.530  \ns(yob):genderM 9.00 1.00    1.00   0.460  \ns(speech_rate) 9.00 1.00    0.93   0.065 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output above has an entry for each smooth term in the model. k' indicate the knots the smooth function has available. This, as explained above, is an upper limit on wiggliness. It will usually be one less than the value of k given in the model formula. The value edf indicates the actual wiggliness of the smooth. To determine if there is evidence of insufficient k, check whether the p-value is low and the edf is close to k'. If so, consider increasing k in the model. In this case, k is plenty high enough.\nLet’s see a case where this doesn’t work well.\n\nsim_fit &lt;- bam(\n  formula = yobs ~ s(x, k = 3, bs = \"cr\"),\n  data = data\n)\n\ngam.check(sim_fit)\n\n\nMethod: fREML   Optimizer: perf newton\nfull convergence after 11 iterations.\nGradient range [-2.519727e-09,2.506653e-09]\n(score 1287.688 & scale 9.969049).\nHessian positive definite, eigenvalue range [0.4972726,249.001].\nModel rank =  3 / 3 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n     k' edf k-index p-value    \ns(x)  2   2    0.03  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere k' and edf are the same, and the p-value is very low. Something is going wrong here. In fact, the problem here is the same problem we saw in Figure 8.4.\nIf you run gam.check you will also see some diagnostic plots. I have suppressed them in this document. The gratia package provides a nice wrapper for the gam.check visualisations (via the appraise function). It has the advantage of being a single plot in ggplot format which will take on any global changes to you ‘theme’. That is, it will produce output which matches your other plots.\nLet’s look at the bad fit to our simulated data again:\n\nappraise(sim_fit)\n\n\n\n\n\n\n\n\nThese plots tell us something about the residuals. The assumption of our model is that these residuals will be normally distributed. That is, we assume that the variation which is left behind by our model looks like random observations from a normal distribution.\nThese diagnostic plots are not like this. At the top left, we should see a straight line of black points following the red line. However, we see that at the tails, at extreme values of a predictor, we are not getting what we would expect from a normal distribution. The histogram tells a similar story. This should look like a nice(ish) bell curve. But the most extreme warning signs are the two plots on the right. These show the model predictions plotted against the actual values. There should be no obvious (non-linear) pattern in these plots.\nWe already know what to do in this case, we need to increase \\(k\\)! If we do, this is what we get:\n\nsim_fit_highk &lt;- bam(\n  formula = yobs ~ s(x, k = 20, bs = \"cr\"),\n  data = data\n)\n\nappraise(sim_fit_highk)  \n\n\n\n\n\n\n\n\nMuch better! And the check of k looks OK too:\n\ngam.check(sim_fit_highk)\n\n\nMethod: fREML   Optimizer: perf newton\nfull convergence after 5 iterations.\nGradient range [-1.018439e-06,1.032479e-06]\n(score 434.2567 & scale 0.2813964).\nHessian positive definite, eigenvalue range [8.433327,249.2893].\nModel rank =  20 / 20 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n       k'  edf k-index p-value\ns(x) 19.0 17.7     1.1    0.98\n\n\nReturning to our model of the ONZE data, let’s use appraise again:\n\nappraise(onze_fit_rate)\n\n\n\n\n\n\n\n\nThis looks basically fine. But we can see heavier tails that we would usually want at either end of the QQ plot and the histogram. This is quite common in vocalic data. One way to handle this is to assume that the residuals follow a t distribution instead (these have fatter tails than normal distributions). We can do this with the family argument to bam. If we do this, the plots look a bit better:\n\nonze_fit_rate &lt;- bam(\n  formula = F1_lob2 ~ gender + \n    s(yob, by = gender, k = 10, bs = \"tp\") +\n    s(speech_rate, k = 10, bs = \"tp\"),\n  data = mean_onze_full,\n  family = scat(link=\"identity\")\n)\n\nappraise(onze_fit_rate)\n\n\n\n\n\n\n\n\nWe will see a case below where a t-distribution functions better than a standard normal distribution with formant data. It is worth checking this case-by-case though.\n\n\n8.2.5 Plotting\nPlotting smooths can be done in at least three ways:\n\nUsing a prediction function to generate predictions from the model and then plot them yourself. The advantage is high flexibility in your plots.\nUse the plot_smooth() and related functions from itsadug.\nUse the GAM plotting functions from gratia.\n\nLet’s look at the plot_smooth() function. This has been used in a lot of projects at NZILBB.\n\nplot_smooth(\n  x = # the model,\n  view = # the name of the variable you want to plot.\n  cond = # a named list containing the values of other terms in the model. \n  # if not given you will get mean values.\n  plot_all = # The name of any factors which for which you want all levels to be\n  # plotted\n  rug = # display a 'rug' at the bottom of the plot to indicate where there are\n  # actual observations.\n)\n\nNow, using these to plot the ONZE model, we get:\n\nplot_smooth(\n  x = onze_fit_rate,\n  view = \"yob\",\n  plot_all = \"gender\",\n  rug = TRUE\n)\n\nSummary:\n    * gender : factor; set to the value(s): F, M. \n    * yob : numeric predictor; with 30 values ranging from 1864.000000 to 1982.000000. \n    * speech_rate : numeric predictor; set to the value(s): 4.86001666666667. \n    * NOTE : No random effects in the model to cancel.\n \n\n\n\n\n\n\n\n\n\nNote that the values given for other predictors are given in console output.\nNow change the view and cond arguments and see what happens.\nThe draw() function from gratia is very useful for plotting the ‘partial effects’ of each smooth term. Let’s look at an example and then consider what this means in more detail.\n\ndraw(onze_fit_rate)\n\n\n\n\n\n\n\n\nThe difference between ‘summed effects’, which you get in plot_smooth(), and the ‘partial effects’ produced by draw(), is that summer effects include intercept terms and values for other predictors. We’ve seen above that plot_smooth() produces an output message which states what the level of the other predictors has been set to. The draw() function, and the mgcv function plot.gam(), instead show the effect of the smooth terms individually. These will be centred around \\(0\\).\n\n\n8.2.6 Interactions\nOften we are interested in how two predictors interact to generate a response. We have already seen one version of this: continuous by factor interactions created using the by argument to s(). We determined the change over time in ONZE independently for male and female speakers.\nThere are also methods for including continuous by continuous interactions in GAMMs. The two main functions to consider are ti() and te().\nThe ti() function produces a ‘tensor product interaction’. With the example data from ONZE we have been using there are only two predictors which can be naturally treated as continuous: year of birth and speech rate. In order to interact these predictors, we could add a ti() term. This is done by the following code block. For the purposes of this example we remove gender from the model, but we will add it in again in a moment. The use of k=5 just makes the default behaviour for this kind of smooth explicit.\n\nonze_fit_int &lt;- bam(\n  formula = F1_lob2 ~ \n    s(yob, k = 10, bs = \"tp\") +\n    s(speech_rate, k = 10, bs = \"tp\") +\n    ti(yob, speech_rate, k=5, bs = \"tp\"),\n  data = mean_onze_full,\n  family = scat(link=\"identity\")\n)\n\ndraw(onze_fit_int)\n\n\n\n\n\n\n\n\nWe now get a heat map, which represents the interaction. In order to read the heat map, find the values of the predictors on the \\(x\\) and \\(y\\) axis and then use the colour guide to determine what to add to the partial effects of the predictors. For instance, if we look in the bottom left of the heatmap we see a patch of blue. This indicates that early speakers who are slow tend to have lower F1 values for dress. This is to say, they start from a higher position in the vowel space (although still low relative to contemporary New Zealand English speakers). The heatmap helpfully greys out areas in which there are no datapoints.\nWhat does this look like in a model summary? Here is the relevant table:\n\nsummary(onze_fit_int)$s.table\n\n                         edf   Ref.df         F    p-value\ns(yob)              3.975988 4.909228 74.411993 0.00000000\ns(speech_rate)      2.696854 3.447972  1.896666 0.12778914\nti(yob,speech_rate) 5.684495 7.421081  1.863619 0.06482433\n\n\nThere is a new line for the ti() term. We see that it does not achieve statistical significance at the \\(0.05\\) level.3\nThe function te() specifies a ‘tensor product smooth’. This includes the individual effects of any included predictors and their interaction. This means the formula for the model no longer has individual terms for year of birth and speech rate. This looks very simple:\n\nonze_fit_te &lt;- gam(\n  formula = F1_lob2 ~ \n    te(yob, speech_rate, k=5, bs = \"tp\"),\n  data = mean_onze_full,\n  family = scat(link=\"identity\")\n)\n\ndraw(onze_fit_te)\n\n\n\n\n\n\n\n\nNow there’s only one smooth term, carrying all of the relationship between speech rate, year of birth, their interaction, and the normalised F1 of the dress vowel. This is, as one might imagine, less useful for testing specific hypotheses about the role of, say, speech rate, or year of birth. Indeed, if we look in the model summary, we will see one term:\n\nsummary(onze_fit_te)\n\n\nFamily: Scaled t(5.187,0.141) \nLink function: identity \n\nFormula:\nF1_lob2 ~ te(yob, speech_rate, k = 5, bs = \"tp\")\n\nParametric coefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.592324   0.007391  -80.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                      edf Ref.df Chi.sq p-value    \nte(yob,speech_rate) 10.94  13.68  974.5  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.627   Deviance explained = 51.7%\n-REML = -149.4  Scale est. = 1         n = 481\n\n\nThere is no obvious way to unpack this information.\nOne final thing to note is that we can add by arguments to ti() and te() just as we did for s(). So, for instance, we might fit a model which has an interaction for speech rate, year of birth, and gender, as follows:\n\nonze_fit_gen_int &lt;- gam(\n  formula = F1_lob2 ~ \n    s(yob, k = 10, by=gender, bs = \"tp\") +\n    s(speech_rate, k = 10, bs = \"tp\") +\n    ti(yob, speech_rate, by=gender, k=5, bs = \"tp\"),\n  data = mean_onze_full,\n  family = scat(link=\"identity\")\n)\n\ndraw(onze_fit_gen_int)\n\n\n\n\n\n\n\n\nNow we have, in effect, a three way interaction. We get two distinct heatmaps, one for the interaction between year of birth and speech rate in the men, and one for the same interaction in the women.\nLet’s look at the heat maps by themselves, as they are a little bit small.\n\ndraw(onze_fit_gen_int, select=c(4, 5))\n\n\n\n\n\n\n\n\nNote the very different colour scales for male and female speakers.\nAnd look at the smooths section of the model summary:\n\nsummary(onze_fit_gen_int)$s.table\n\n                                 edf   Ref.df       Chi.sq      p-value\ns(yob):genderF              3.447366 4.245710 191.16420928 0.0000000000\ns(yob):genderM              1.224522 1.412964 360.09732544 0.0000000000\ns(speech_rate)              2.541233 3.301794   5.69677023 0.1377318834\nti(yob,speech_rate):genderF 3.946719 5.233997  22.54341618 0.0005519912\nti(yob,speech_rate):genderM 1.000523 1.000995   0.02282337 0.8815218793\n\n\nThis version of the model claims a statistically significant interaction between year of birth and speech rate for the female speakers. The rough story, from the heatmap, when combined with the smooth for year of birth for the female speakers, is that the change over time is more extreme for faster speakers and less extreme for slower speakers.4",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html#the-parenthetical-m-mixed-effects",
    "href": "chapters/gamms_1.html#the-parenthetical-m-mixed-effects",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "8.3 The Parenthetical ‘M’: Mixed Effects",
    "text": "8.3 The Parenthetical ‘M’: Mixed Effects\nWe can include quite complex random effects structures in GAMMs. As in the case of generalised linear models, we can add random intercepts and random slopes. GAMMs introduce the additional possibility of random smooths. The purpose of random effects in this context is exactly the same as in linear mixed-effects models: we want to capture dependence between observations.\n\n8.3.1 Random Intercepts\nWhen fitting a random intercept in a linear mixed-effects model, using the lme4package, for instance, we use the syntax (1 | x_1) within the model formula to get random intercepts for each level of the variable x_1. In mgcv, we use the syntax s(x_1, bs=\"re\"). That is, we specify that we want random intercepts by changing the basis of a smooth term.\nIn corpus phonetics, we usually want a random intercept for each speaker and each word. In our previous models, we took mean values for each speaker so that we had a single value for each. This is one strategy to avoid the need for random effects, but it is better to include all the data points we have while building the dependence structure between the in to the model.\nWe create a data set containing all of the dress F1 readings, again applying Lobanov 2.0 normalisation.\n\nonze_dress &lt;- onze_vowels_full |&gt; \n  lobanov_2() |&gt; \n  filter(\n    vowel == \"DRESS\"\n  )\n\nWe now have a column with speaker identifiers (speaker) and word identifiers (word). We fit the same structure as the previous model, but with random intercepts for these two variables. Since we are significantly increasing the number of tokens (from 481 to 69925), we should also switch to the bam function.\n\nonze_ri_fit &lt;- bam(\n  formula = F1_lob2 ~ \n    s(yob, k = 10, by=gender, bs = \"tp\") +\n    s(speech_rate, k = 10, bs = \"tp\") +\n    ti(yob, speech_rate, by=gender, k=5, bs = \"tp\") +\n    s(speaker, bs = \"re\") + s(word, bs = \"re\"),\n  data = onze_dress,\n  family = scat(link=\"identity\"),\n  discrete = TRUE,\n  nthreads = detectCores() - 1\n)\n\n\n\n8.3.2 Random Smooths\nWe’ll now switch to a data set which is more useful for illustrating random smooths and which will also be useful when we turn to the question of auto-correlation later (Section 8.4).\nThe new data set comes from Sóskuthy, Hay, and Brand (2019). This paper is an exploratory study of the dynamics of New Zealand English diphthongs. It contains formant trajectories for the price and mouth vowels. We will take a series of formant trajectories from the price vowel, looking at both F1 and F2 values. The full data set is available on GitHub, here",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html#sec-auto",
    "href": "chapters/gamms_1.html#sec-auto",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "8.4 Auto-correlation",
    "text": "8.4 Auto-correlation\nAuto-correlation is an issue in time series data. Often a measurement at one time point is correlated with measurements at nearby time points.\nWhen talking about random effects in the previous section, we discussed them in terms of information. We can think of auto-correlation in the same way. Measurements from nearby time points do not provide independent information.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html#sec-hyp",
    "href": "chapters/gamms_1.html#sec-hyp",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "8.5 Hypothesis Testing",
    "text": "8.5 Hypothesis Testing\nHow do we test a GAMM model…\n\n8.5.1 Visual methods\n\n\n8.5.2 Model summary\n\n\n8.5.3 itsadug::compareML().",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html#reporting-a-gamm",
    "href": "chapters/gamms_1.html#reporting-a-gamm",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "8.6 Reporting a GAMM",
    "text": "8.6 Reporting a GAMM\nIt is important to know how to report a GAMM in a paper.\nThis section presents some (somewhat randomly selected) examples of GAMMs ‘in the wild’. All of these are good enough for publication. After the examples, I’ll give a few opinions about what ought to be included in the body of an article and in supplementary materials.\n\nRenwick et al. (2023)Brand et al. (2021)Derrick and Gick (2021)\n\n\nHow did you fit the GAMMs?:\n\nWe built a separate GAMM for each combination of allophone, gender, and formant, leading to twenty-eight separate models. The dependent variable was log-means normalized formant values, from five measurement points per token. The models’ independent variables included a nonlinear, continuous smooth term for speaker year of birth (YoB), and a smooth term for measurement point (percent), ordered 20% to 80%. Smooths used four knots. The two smooths were combined into a tensor-product interaction allowing the predicted trajectory to freely vary in shape across YoB. Vowel duration was included as a parametric effect. The random effects structure comprised linear random intercepts and slopes for speaker, word, and collection. Model specifications and summaries appear in Appendix 1. Models were fitted using the mgcv::bam() function in R (Wood, 2017b). (Renwick et al. 2023, 185)\n\nAppendix 1 contains the output of the summary() function for each GAMM model pasted in to a word document. This includes the model formulae.\nHow did you evaluate the GAMMs?\n\nWe evaluate the GAMMs in two ways: first, via visualizations of predicted measurements, which were extracted from each model. Second, we tested the significance of YoB via model comparison (Renwick & Stanley, 2020; Stanley et al., 2021). For each GAMM, we constructed a model that excluded YoB but was otherwise identical. Each “dropped” model was evaluated against its “full” equivalent via itsadug:: compareML() (van Rij, Wieling, Baayen, & van Rijn, 2017), which returns a score and p-value through chi-squared testing of log-likelihood. If the “full” model including birth year is deemed to be significantly better ( p &lt; 0.05) than the “dropped” version without birth year, we infer that the shape and/or position of that vowel’s trajectory is meaningfully predicted by speaker year of birth. (Renwick et al. 2023, 185)\n\nHow did you report the results?\n\nModel comparisons confirmed that for all models save one (F2 of PRY, for women) the inclusion of YoB provided a significant improvement in fit over an identical GAMM lacking that term. For all allophones (including women’s pry, whose F1 model is improved by YoB), there is significant variation in trajectory shape and/or vowel space position across time. (Renwick et al. 2023, 186)\n\nA footnote indicates that pry has the lowest token count and the prediction that increased tokens would lead to an improvement by adding their year of birth term.\nA series of figures are then described in detail. For instance:\n\n\n\nFigure 5 from Renwick et al. 2023, p. 189.\n\n\nThe description is as follows:\n\nYoung Georgians’ trajectories are modeled in Figure 5, including birth years 1974 (left) and 2000 (right), representing Gen X and Gen Z. The vowel systems shown here are markedly different from older speakers’. For all groups, the /aɪ/ allophones’ offset has a lower F1 than bat, indicating its trajectory lengthening. bought remains slightly backer than bot, although they have similar ingliding trajectories. The lax front vowels have shifted. bat is ingliding and retracted, while bet is lower in Gen Z compared to Gen X. bait is very peripheral, especially in Gen Z, compared to the other front vowels, with a lower F1 and higher F2 throughout its trajectory. bait and bet are very distinct, and their trajectories do not cross; there is no face/dress swapping for these speakers. These effects are stronger for Gen Z than for Gen X. The picture shown for late twentieth century Georgians is not the SVS; instead, the Gen X speakers show a retreat from the SVS, characterized by retracting front lax vowels, which is consistent with the LBMS among younger, Gen Z speakers. [Renwick et al. (2023), 187\n\n\n\nAs in the Renwick et al. (2023) example, Brand et al. (2021) reports many GAMMs fit to readings from vowels. There is significantly more detail concerning the GAMMs in supplementary material for this paper hosted on osf.io and github.com. There is less detail in the paper itself, because the GAMMs are one stage in a larger method.\nHow did you fit the GAMMs?\nIn the paper the use of GAMMs is described as follows in the methodology section:\n\nSpeaker intercepts from linear mixed effects regression models have been used to index speaker advancement in sound change (Drager & Hay, 2012; Sóskuthy et al., 2017). The data we analyse here contains some non-linear effects, and thus an appropriate modelling technique to obtain the speaker intercepts from our data is generalised additive mixed modelling (GAMMs). This is because we want to model the normalised F1 and F2 of each of the 10 monophthongs separately, whilst also reliably capturing non-linear changes across time (see Winter & Wieling, 2016; Sóskuthy, 2017; Wieling, 2018 & Chuang et al., Chuang, Fon, & Baayen, 2020 for introductory tutorials on GAMMs). We fitted separate models to normalised F1 and F2 for each of the 10 vowels in the data set, giving 20 models in total.\nAll models were fitted using the same formula via the mgcv package in R (Wood, 2017), with fixed-effects comprising separate smooths over year of birth for each gender (using an adaptive smooth basis with 10 knots), as well as a smooth over speech rate. Speech rate is calculated as syllables per second for the transcript (where each speaker’s recording is spread across several transcripts. Individual transcripts contain on average approximately 6 min of speech). Random intercepts were included for speaker and word form. We then combined all of the speaker intercepts from the separate models, providing us with a final 481 X 20 data set, where each of the 481 speakers had a separate intercept for each of the 20 vocalic variables. (Brand et al. 2021, 8)\n\nHow did you report the results?\nIn the results section, the GAMMs are plotted, again in a vowel space: \nThe GAMMs are discussed as follows:\n\nWhile our primary purpose in fitting the GAMMs is to control for speaker factors such as year of birth, the major patterns revealed by the GAMMs also facilitate an understanding of sound change occurring over this time period, which is a necessary precursor to interpreting the patterns of co-variation. Fig. 5 shows the trajectories of vowel change, based on GAMM smooths over year of birth. All vowels have undergone some change, either linear or non-linear, based on the year of birth smooth effect (all p-values &lt; 0.001), see the Supplementary Materials for the model summaries. An animated and interactive version of the changes is available in the Shiny application. (Brand et al. 2021, 9)\n\nThe above notes statistical significance of the year of birth smooths and points readers to mode detail if required. The next paragraphs of the results section describe the patterns in the plot in light of existing literature.\nWhat is in supplementary material?\nThe supplementary material for the analysis is hosted on GitHub. It contains two relevant sections. First, a section with the code for fitting the models, which includes discussion of the model formula and the R code used to fit a series of models with the same formula (here). Second, there is a section containing all of the model summaries (here). These summaries do not contain information about random effects. We have seen above that this radically reduces the amount of time required to generate a model summary.\nIn addition, an online interactive is provided for exploring the analysis in the paper (including the GAMMs). See here.\n\n\nDerrick and Gick (2021) is a shorter piece for Scientific Reports. Space is at a premium so a short explanation is given.\nHow did you fit the GAMMs?\nThe discussion of the GAMMs in the methodology sections comes after an earlier set of steps have already been described. Justification is given in terms of non-linearity in the data.\n\nNext, and in order to make sense of this highly non-linear data, we ran a generalized additive mixed-effects model (GAMM) on the data shown in Eq. (7). GAMMs are extremely effective for the analysis of non-linear data, and are therefore highly suitable for the analysis of the critical fluctuations captured in Eq. (6).\nEquation (7), written in R-code, describes a generalized additive mixed-effects model, comparing Fluctuation based on tongue-front displacement (TFd) and the fluctuation time slice position (FTS), forming a 3-dimensional tensor (te) field [te(FTS, TFd)]. The random effects factor out participant variability in a 3-dimensional tensor field [s(FTS, TFd, Participant, bs = “fs”, m = 1)], as well as random-effect smooths for syllables per second [s(SPS, Participant, bs = “re”)], and token type [s(Token type, Participant, bs = “re”)]. In order to correct for autocorrelation effects, we ran the GAMM, calculated an estimate for a start value ρ from that first run, and provided that ρ to a second run of the GAMM model, along with an indicator identifying the first position of our time slices. This removes most of the autocorrelation, maximizing the accuracy of the resulting statistical model output.\nEquation (7) produces an output that shows the relationship between critical fluctuation, token position, and tongue-front displacement range, highlighting regions of significant difference. And with these methods, we were able to identify whether tongue-front displacement range affected speech-rate range, and whether tongue-front displacement range had any influence on the timing slice positions of critical fluctuations. (Derrick and Gick 2021, 8)\n\nEquation 7 is the following R formula:\n\ngam(Fluctuation ~ te(FTS, TFd) + \n      s(FTS, TFd, Participant, bs = \"fs\", m = 1) + \n      s(SPS, Participant, bs = \"re\") + \n      s(Tokentype, Participant, bs = \"re\")\n)\n\nNote, btw, the use of all both random smooths (the second smooth term) and two smooth terms defining random slopes (the third and fourth smooth terms in the model).\nThe methods sections describes the method used to deal with autocorrelation as well.\nFinally, a paragraph is given to explain the relationship between the output of the GAMMs and the actual subject matter of the research.\nCitations are given in footnotes to R itself, the mgcv package and the itsadug package.\nHow did you report the results?\nThere results section reports both the significance of smooth terms and discusses a plot.\nThe main plot is Figure 7:\n The discussion is as follows:\n\nGeneralized additive mixed-effects model analysis of critical fluctuations during the time course of token production by tongue-front displacement range are shown in Fig. 7. The model shows that speakers producing tokens with the lowest tongue-front displacement ranges have relatively higher critical fluctuations in the early part of their token productions, spanning from the first vowel through the first flap into the middle of the second vowel. In contrast, they show much lower rates of critical fluctuation from the second half of the second vowel, through the second flap to the end of the third vowel. This constitutes evidence of end-state comfort effects for speakers producing token types with narrow tongue-front displacement ranges.\nFor speakers producing token types in the middle of the group, there are no end-state comfort effects, but instead the most effort made during the second flap. For speakers with very wide tongue-front displacement ranges, there is again statistically significant evidence for end-state comfort effects, with extra beginning-state effort for the initial vowel. These are the same speakers producing token types that demonstrate two categorically different patterns of motion—one for slow speech, and one for fast speech.\nFigure 7 shows the regions of significance for the GAMM whose model output is shown in Table 3. These results show that all of the model parameters are significant, and most importantly that the tensor field shown in Fig. 7 accounts for a significant portion of the variance of the data. This includes the fixed-effect tensor relating tongue-front displacement range and critical fluctuation along time slices, as well as the random-effects for participant, token type, and reiterant speech rate. The entire GAMM accounts for an adjusted r2 of 0.452, explaining 47% of the deviance in critical fluctuations in this dataset. (Derrick and Gick 2021, 10–11)\n\nSignificance information is given as Table 3.\n\n\n\nTable 3 from Derrick and Gick (2021).\n\n\nWhat is in supplementary materials?\nThere is a repository on OSF (here). Code is available as an Rmarkdown or as an HTML file to be downloaded and read in the browser.\nThe analysis shows that the p-values in Table 3 are generated by using itsadug::compareML() and fitting the models with maximum likelihood estimation. With more space, this might have been discussed in the paper. But there is not always more space!\n\n\n\nWhat can we take from the above examples? Here are some reflections:\n\nYou don’t need to say everything in the paper. You can use supplementary material. This should include, at a minimum, the specific code you used to fit the model and generate any reported p-values (not just the summary).\n\nThis can be important for seeing, e.g., whether the ‘difference smooth’ structure was used (see discussion above).\n\nCite the specific packages you used. The function citation() in R can be very helpful for this (e.g. look at the results of citation('mgcv')). I often use the package grateful to generate a bibliography of all of the packages I use within an analysis project.\nVisualisations are an important part of evaluating GAMMs (moreso than for linear models where a single coefficient is what you are testing).\n\nYour discussion section might consist of description of the specific trajectories you find visually.\n\nSupplementary material opens up many windows (including e.g., the interactive application in Brand et al. (2021)). This is also, however, a potential time sink. Try to balance the costs and benefits when you go beyond the requirements of open science and into the realm of web development!",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html#sec-resources",
    "href": "chapters/gamms_1.html#sec-resources",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "8.7 Further Resources",
    "text": "8.7 Further Resources\n\nMárton Sóskuthy’s GAMM tutorial: https://arxiv.org/pdf/1703.05339.pdf\nMárton Sóskuthy’s paper compared multiple significance testing strategies for error rates: https://www.sciencedirect.com/science/article/pii/S009544702030108X#s0070\nMartijn Wieling’s tutorial: https://www.sciencedirect.com/science/article/pii/S0095447017301377\n\n\n\n\n\n\n\nNote\n\n\n\nResources have also been written by non-Martins. I will add some soon!\n\n\n\n\n\n\nBrand, James, Jen Hay, Lynn Clark, Kevin Watson, and Márton Sóskuthy. 2021. “Systematic Co-Variation of Monophthongs Across Speakers of New Zealand English.” Journal of Phonetics 88: 101096. https://www.sciencedirect.com/science/article/pii/S0095447021000711.\n\n\nDerrick, Donald, and Bryan Gick. 2021. “Gait Change in Tongue Movement.” Scientific Reports 11 (1): 16565. https://doi.org/10.1038/s41598-021-96139-4.\n\n\nRenwick, Margaret E. L., Joseph A. Stanley, Jon Forrest, and Lelia Glass. 2023. “Boomer Peak or Gen X Cliff? From SVS to LBMS in Georgia English.” Language Variation and Change 35 (2): 175–97. https://doi.org/10.1017/S095439452300011X.\n\n\nSóskuthy, Márton. 2017. “Generalised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction.” arXiv.org. March 15, 2017. https://arxiv.org/abs/1703.05339v1.\n\n\nSóskuthy, Márton, J. Hay, and James Brand. 2019. “Horizontal Diphthong Shift in New Zealand English.” In. https://www.semanticscholar.org/paper/HORIZONTAL-DIPHTHONG-SHIFT-IN-NEW-ZEALAND-ENGLISH-S%C3%B3skuthy-Hay/cd1bd700686b3d1270be5536e5881e8946ba57ab.\n\n\nWieling, M. 2018. “Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: A Tutorial Focusing on Articulatory Differences Between L1 and L2 Speakers of English.” Journal of Phonetics 70: 86–116. https://doi.org/10.1016/j.wocn.2018.03.002.\n\n\nWilson Black, Joshua, Jennifer Hay, Lynn Clark, and James Brand. 2023. “The Overlooked Effect of Amplitude on Within-Speaker Vowel Variation.” Linguistics Vanguard 9 (1): 173–89. https://doi.org/10.1515/lingvan-2022-0086.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/gamms_1.html#footnotes",
    "href": "chapters/gamms_1.html#footnotes",
    "title": "8  Generalized Additive (Mixed) Models",
    "section": "",
    "text": "If you are not familiar with this bit of linguistics, just think of this a set of readings derived from a single action. It could just as easily be a set of readings from a brain scan, or a moving slider, or whatever you like. The only restriction is that we have a set of measurements across time of the same event or action.↩︎\nWe will look at how to do this later in the series.↩︎\nHypothesis testing will be covered in more detail in Section 8.5.↩︎\n You may have noticed that there is a correlation between speech rate and year of birth. This should be kept in mind when interpreting the model.↩︎",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Additive (Mixed) Models</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Brand, James, Jen Hay, Lynn Clark, Kevin Watson, and Márton Sóskuthy.\n2021. “Systematic Co-Variation of Monophthongs Across Speakers of\nNew Zealand English.” Journal of Phonetics\n88: 101096. https://www.sciencedirect.com/science/article/pii/S0095447021000711.\n\n\nDerrick, Donald, and Bryan Gick. 2021. “Gait Change in Tongue\nMovement.” Scientific Reports 11 (1): 16565. https://doi.org/10.1038/s41598-021-96139-4.\n\n\nRenwick, Margaret E. L., Joseph A. Stanley, Jon Forrest, and Lelia\nGlass. 2023. “Boomer Peak or Gen X\nCliff? From SVS to LBMS in\nGeorgia English.” Language Variation and\nChange 35 (2): 175–97. https://doi.org/10.1017/S095439452300011X.\n\n\nSóskuthy, Márton. 2017. “Generalised Additive Mixed Models for\nDynamic Analysis in Linguistics: A Practical Introduction.”\narXiv.org. March 15, 2017. https://arxiv.org/abs/1703.05339v1.\n\n\nSóskuthy, Márton, J. Hay, and James Brand. 2019. “Horizontal\nDiphthong Shift in New Zealand English.” In. https://www.semanticscholar.org/paper/HORIZONTAL-DIPHTHONG-SHIFT-IN-NEW-ZEALAND-ENGLISH-S%C3%B3skuthy-Hay/cd1bd700686b3d1270be5536e5881e8946ba57ab.\n\n\nSóskuthy, Márton, and Jennifer Hay. 2017. “Changing Word Usage\nPredicts Changing Word Durations in New Zealand\nEnglish.” Cognition 166 (September): 298–313. https://doi.org/10.1016/j.cognition.2017.05.032.\n\n\nWickham, H. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Use R! Springer International Publishing. https://books.google.co.nz/books?id=RTMFswEACAAJ.\n\n\nWieling, M. 2018. “Analyzing Dynamic Phonetic Data Using\nGeneralized Additive Mixed Modeling: A Tutorial Focusing on\nArticulatory Differences Between L1 and L2\nSpeakers of English.” Journal of Phonetics\n70: 86–116. https://doi.org/10.1016/j.wocn.2018.03.002.\n\n\nWilkinson, Leland. 1999. The Grammar of Graphics. Statistics\nand Computing. New York: Springer. http://digitool.hbz-nrw.de:1801/webclient/DeliveryManager?pid=2966363.\n\n\nWilson Black, Joshua, Jennifer Hay, Lynn Clark, and James Brand. 2023.\n“The Overlooked Effect of Amplitude on Within-Speaker Vowel\nVariation.” Linguistics Vanguard 9 (1): 173–89. https://doi.org/10.1515/lingvan-2022-0086.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An\nIntroduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "References"
    ]
  }
]